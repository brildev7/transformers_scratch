#!/usr/bin/env python3
"""
í•œêµ­ì–´ ì§€ì› í† í¬ë‚˜ì´ì €
Korean Language Supporting Tokenizer

í•œêµ­ì–´ì˜ êµì°©ì–´ íŠ¹ì„±ì„ ê³ ë ¤í•œ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤.
- í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í† í¬ë‚˜ì´ì§•
- ì¡°ì‚¬, ì–´ë¯¸ ë¶„ë¦¬
- êµ¬ë‘ì  ì²˜ë¦¬
- í•™ìŠµ-ì¶”ë¡  ì¼ê´€ì„± ë³´ì¥
"""

import re
import json
from typing import List, Dict, Optional, Tuple
from pathlib import Path


class KoreanTokenizer:
    """í•œêµ­ì–´ ì§€ì› í† í¬ë‚˜ì´ì €"""
    
    def __init__(self, vocab_file: Optional[str] = None):
        """
        Args:
            vocab_file: ì–´íœ˜ íŒŒì¼ ê²½ë¡œ (ì„ íƒì )
        """
        # íŠ¹ìˆ˜ í† í°
        self.pad_token = "<pad>"
        self.bos_token = "<s>"
        self.eos_token = "</s>"
        self.unk_token = "<unk>"
        
        # íŠ¹ìˆ˜ í† í° ID
        self.pad_token_id = 0
        self.bos_token_id = 1
        self.eos_token_id = 2
        self.unk_token_id = 3
        
        # ì–´íœ˜ í¬ê¸°
        self.vocab_size = 65536
        
        # í•œêµ­ì–´ íŒ¨í„´ ì •ì˜
        self._setup_korean_patterns()
        
        # ê¸°ë³¸ ì–´íœ˜ êµ¬ì¶•
        self._build_vocab()
        
        print(f"í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  â€¢ ì–´íœ˜ í¬ê¸°: {self.vocab_size:,}")
        print(f"  â€¢ ê¸°ë³¸ ì–´íœ˜: {len(self.word_to_id):,}ê°œ")
        
    def _setup_korean_patterns(self):
        """í•œêµ­ì–´ ì²˜ë¦¬ íŒ¨í„´ ì„¤ì •"""
        
        # ì¡°ì‚¬ íŒ¨í„´ (ì£¼ê²©, ëª©ì ê²©, ë¶€ì‚¬ê²© ë“±)
        self.josa_patterns = [
            'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 
            'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì²˜ëŸ¼', 'ê°™ì´', 'ë³´ë‹¤', 'ë§ˆë‹¤',
            'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ê»˜ì„œ', 'ë¼ì„œ', 'ë‹ˆê¹Œ', 'ë•Œë¬¸ì—', 'ìœ„í•´', 'ëŒ€í•´'
        ]
        
        # ì–´ë¯¸ íŒ¨í„´ (ì¡´ëŒ“ë§, ë°˜ë§, ì‹œì œ ë“±)
        self.eomi_patterns = [
            'ìŠµë‹ˆë‹¤', 'ìŠµë‹ˆê¹Œ', 'ì„¸ìš”', 'ì‹­ì‹œì˜¤', 'í•©ë‹ˆë‹¤', 'í–ˆìŠµë‹ˆë‹¤', 'í•˜ê² ìŠµë‹ˆë‹¤',
            'ì…ë‹ˆë‹¤', 'ì˜€ìŠµë‹ˆë‹¤', 'ì´ì—ˆìŠµë‹ˆë‹¤', 'í•´ìš”', 'í•´ì„œ', 'í•˜ê³ ', 'í•˜ë©´',
            'í•˜ì§€ë§Œ', 'í•˜ê±°ë‚˜', 'í•˜ë“ ì§€', 'í•œë‹¤', 'í–ˆë‹¤', 'í•˜ì', 'í•˜ë¼'
        ]
        
        # êµ¬ë‘ì  íŒ¨í„´
        self.punctuation = r'[.!?,:;"\'\(\)\[\]\{\}~\-_+=<>/@#$%^&*`|\\]'
        
        # ìˆ«ì íŒ¨í„´
        self.number_pattern = r'\d+'
        
        # ì˜ì–´ íŒ¨í„´
        self.english_pattern = r'[a-zA-Z]+'
        
    def _build_vocab(self):
        """ê¸°ë³¸ ì–´íœ˜ êµ¬ì¶•"""
        
        # ê¸°ë³¸ í•œêµ­ì–´ ì–´íœ˜ (í˜•íƒœì†Œ ë‹¨ìœ„)
        basic_vocab = [
            # ëŒ€ëª…ì‚¬
            "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ê·¸ë“¤", "ì´", "ê·¸", "ì €", "ê²ƒ", "ê³³",
            
            # ëª…ì‚¬ - ì‚¬ëŒ
            "ì‚¬ëŒ", "ì¹œêµ¬", "ê°€ì¡±", "ë¶€ëª¨", "ì•„ë²„ì§€", "ì–´ë¨¸ë‹ˆ", "í˜•", "ëˆ„ë‚˜", "ë™ìƒ",
            "ì„ ìƒë‹˜", "í•™ìƒ", "ì˜ì‚¬", "ê°„í˜¸ì‚¬", "íšŒì‚¬ì›", "ì•„ì´", "ì–´ë¥¸",
            
            # ëª…ì‚¬ - ì¥ì†Œ  
            "ì§‘", "í•™êµ", "íšŒì‚¬", "ë³‘ì›", "ì€í–‰", "ìƒì ", "ì‹œì¥", "ê³µì›", "ë„ì„œê´€",
            "ì¹´í˜", "ì‹ë‹¹", "í˜¸í…”", "ì—­", "ê³µí•­", "ë„ì‹œ", "ë§ˆì„",
            
            # ëª…ì‚¬ - ì‚¬ë¬¼
            "ë¬¼", "ìŒì‹", "ì±…", "ì»´í“¨í„°", "í•¸ë“œí°", "ìë™ì°¨", "ì˜·", "ì‹ ë°œ", "ê°€ë°©",
            "ëˆ", "ì‹œê°„", "ë‚ ì”¨", "ê³µê¸°", "ë‚˜ë¬´", "ê½ƒ", "ë°”ë‹¤", "ì‚°", "ê°•", "í•˜ëŠ˜",
            
            # ë™ì‚¬ ì–´ê·¼
            "ê°€", "ì˜¤", "ë³´", "ë“£", "ë§í•˜", "ì½", "ì“°", "ë¨¹", "ë§ˆì‹œ", "ì", "ì¼ì–´ë‚˜",
            "ì•‰", "ì„œ", "ëˆ•", "ê±·", "ë›°", "ë†€", "ì¼í•˜", "ê³µë¶€í•˜", "í•˜", "ë˜", "ìˆ", "ì—†",
            
            # í˜•ìš©ì‚¬ ì–´ê·¼
            "ì¢‹", "ë‚˜ì˜", "í¬", "ì‘", "ë§", "ì ", "ê¸¸", "ì§§", "ë†’", "ë‚®", "ë„“", "ì¢",
            "ê¸°ì˜", "ìŠ¬í”„", "í™”ë‚˜", "ë¬´ì„­", "í–‰ë³µí•˜", "í¸í•˜", "ë¶ˆí¸í•˜", "í”¼ê³¤í•˜",
            
            # ë¶€ì‚¬
            "ë§¤ìš°", "ì •ë§", "ì•„ì£¼", "ì¡°ê¸ˆ", "ë§ì´", "ì ê²Œ", "ë¹¨ë¦¬", "ì²œì²œíˆ",
            "ì˜", "ëª»", "ë”", "ëœ", "ê°€ì¥", "ì œì¼", "ë„ˆë¬´", "ìì£¼", "ê°€ë”",
            
            # ì—°ê²°ì–´
            "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ëŸ°ë°", "ë˜í•œ", "ë˜", "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ",
            "ë§Œì•½", "ë¹„ë¡", "ì•„ë§ˆ", "í˜¹ì‹œ", "ì •ë§ë¡œ", "í™•ì‹¤íˆ", "ë¶„ëª…íˆ",
            
            # ì¸ì‚¬
            "ì•ˆë…•í•˜", "ë°˜ê°‘", "ê°ì‚¬í•˜", "ì£„ì†¡í•˜", "ê³ ë§™", "ë¯¸ì•ˆí•˜",
            
            # ì‹œê°„
            "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼", "ì§€ê¸ˆ", "ë‚˜ì¤‘", "ì „", "í›„", "ë™ì•ˆ",
            "ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ë°¤", "ìƒˆë²½",
            
            # ê¸°ìˆ /AI ìš©ì–´
            "ëª¨ë¸", "ë°ì´í„°", "í•™ìŠµ", "ì¶”ë¡ ", "AI", "ì¸ê³µì§€ëŠ¥", "ì»´í“¨í„°", "í”„ë¡œê·¸ë¨",
            "í…ìŠ¤íŠ¸", "ë¬¸ì¥", "ë‹¨ì–´", "í† í°", "ìƒì„±", "ì˜ˆì¸¡", "ê²°ê³¼", "ì„±ëŠ¥"
        ]
        
        # ì¡°ì‚¬ì™€ ì–´ë¯¸ ì¶”ê°€
        basic_vocab.extend(self.josa_patterns)
        basic_vocab.extend(self.eomi_patterns)
        
        # ê¸°ë³¸ ì‘ë‹µì–´
        basic_vocab.extend([
            "ë„¤", "ì•„ë‹ˆì˜¤", "ì˜ˆ", "ì•„ë‹ˆ", "ë§", "í‹€ë¦¬", "ê·¸ë ‡", "ê´œì°®", "ì¢‹ê² ", "ì‹«"
        ])
        
        # ë‹¨ì–´ â†’ ID ë§¤í•‘ êµ¬ì¶•
        self.word_to_id = {}
        self.id_to_word = {}
        
        # íŠ¹ìˆ˜ í† í°ë¶€í„° í• ë‹¹
        special_tokens = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]
        for i, token in enumerate(special_tokens):
            self.word_to_id[token] = i
            self.id_to_word[i] = token
        
        # ê¸°ë³¸ ì–´íœ˜ í• ë‹¹ (ID 4ë¶€í„° ì‹œì‘)
        current_id = 4
        for word in basic_vocab:
            if word not in self.word_to_id:
                self.word_to_id[word] = current_id
                self.id_to_word[current_id] = word
                current_id += 1
        
        print(f"  â€¢ íŠ¹ìˆ˜ í† í°: {len(special_tokens)}ê°œ")
        print(f"  â€¢ ê¸°ë³¸ ì–´íœ˜: {len(basic_vocab)}ê°œ")
        
    def tokenize(self, text: str) -> List[str]:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• """
        
        # ì „ì²˜ë¦¬: ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text.strip())
        
        tokens = []
        i = 0
        
        while i < len(text):
            # ê³µë°± ê±´ë„ˆë›°ê¸°
            if text[i].isspace():
                i += 1
                continue
            
            # êµ¬ë‘ì  ì²˜ë¦¬
            if re.match(self.punctuation, text[i]):
                tokens.append(text[i])
                i += 1
                continue
            
            # ìˆ«ì ì²˜ë¦¬
            number_match = re.match(self.number_pattern, text[i:])
            if number_match:
                tokens.append(number_match.group())
                i += len(number_match.group())
                continue
            
            # ì˜ì–´ ì²˜ë¦¬
            english_match = re.match(self.english_pattern, text[i:])
            if english_match:
                tokens.append(english_match.group().lower())
                i += len(english_match.group())
                continue
            
            # í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ
            word_start = i
            while i < len(text) and not text[i].isspace() and not re.match(self.punctuation, text[i]):
                i += 1
            
            word = text[word_start:i]
            if word:
                # í˜•íƒœì†Œ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
                morphemes = self._analyze_morphemes(word)
                tokens.extend(morphemes)
        
        return tokens
    
    def _analyze_morphemes(self, word: str) -> List[str]:
        """í˜•íƒœì†Œ ë¶„ì„ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)"""
        
        # ì¡°ì‚¬ ë¶„ë¦¬
        for josa in sorted(self.josa_patterns, key=len, reverse=True):
            if word.endswith(josa) and len(word) > len(josa):
                stem = word[:-len(josa)]
                return [stem, josa]
        
        # ì–´ë¯¸ ë¶„ë¦¬ 
        for eomi in sorted(self.eomi_patterns, key=len, reverse=True):
            if word.endswith(eomi) and len(word) > len(eomi):
                stem = word[:-len(eomi)]
                return [stem, eomi]
        
        # ë¶„ë¦¬í•  ìˆ˜ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return [word]
    
    def encode(self, text: str, add_special_tokens: bool = True, max_length: Optional[int] = None, return_tensors: Optional[str] = None, **kwargs) -> Dict:
        """í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ì¸ì½”ë”©"""
        
        tokens = self.tokenize(text)
        
        # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        if add_special_tokens:
            tokens = [self.bos_token] + tokens + [self.eos_token]
        
        # í† í° â†’ ID ë³€í™˜
        input_ids = []
        for token in tokens:
            if token in self.word_to_id:
                input_ids.append(self.word_to_id[token])
            else:
                # OOV í† í° ì²˜ë¦¬: í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ì„± ìˆê²Œ í• ë‹¹
                token_id = hash(token) % (self.vocab_size - 1000) + 1000  # ìƒìœ„ ID ì˜ì—­ ì‚¬ìš©
                input_ids.append(token_id)
        
        # ê¸¸ì´ ì œí•œ
        if max_length and len(input_ids) > max_length:
            input_ids = input_ids[:max_length]
        
        # attention_mask ìƒì„±
        attention_mask = [1] * len(input_ids)
        result = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "tokens": tokens
        }
        
        # PyTorch í…ì„œ ë³€í™˜ (í˜¸í™˜ì„±ì„ ìœ„í•´)
        if return_tensors == "pt":
            try:
                import torch
                result["input_ids"] = torch.tensor(result["input_ids"], dtype=torch.long).unsqueeze(0)
                result["attention_mask"] = torch.tensor(result["attention_mask"], dtype=torch.long).unsqueeze(0)
            except ImportError:
                pass  # torchê°€ ì—†ìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë°˜í™˜
        
        return result
    
    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©"""
        
        tokens = []
        for token_id in token_ids:
            if token_id in self.id_to_word:
                token = self.id_to_word[token_id]
                if skip_special_tokens and token in [self.pad_token, self.bos_token, self.eos_token]:
                    continue
                tokens.append(token)
            else:
                # OOV í† í° ì²˜ë¦¬
                if not skip_special_tokens:
                    tokens.append(f"[{token_id}]")
        
        # í† í° ì¬ì¡°í•© (í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤)
        return self._reassemble_korean(tokens)
    
    def _reassemble_korean(self, tokens: List[str]) -> str:
        """í•œêµ­ì–´ í† í°ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¬ì¡°í•©"""
        
        if not tokens:
            return ""
        
        result = []
        i = 0
        
        while i < len(tokens):
            current_token = tokens[i]
            
            # ë‹¤ìŒ í† í°ì´ ì¡°ì‚¬ë‚˜ ì–´ë¯¸ì¸ì§€ í™•ì¸
            if i + 1 < len(tokens):
                next_token = tokens[i + 1]
                if next_token in self.josa_patterns or next_token in self.eomi_patterns:
                    # ì–´ê·¼ê³¼ ì¡°ì‚¬/ì–´ë¯¸ ê²°í•©
                    result.append(current_token + next_token)
                    i += 2
                    continue
            
            # êµ¬ë‘ì ì¸ ê²½ìš° ê³µë°± ì—†ì´ ë¶™ì„
            if re.match(self.punctuation, current_token):
                if result:
                    result[-1] += current_token
                else:
                    result.append(current_token)
            else:
                result.append(current_token)
            
            i += 1
        
        return " ".join(result)
    
    def get_vocab_size(self) -> int:
        """ì–´íœ˜ í¬ê¸° ë°˜í™˜"""
        return self.vocab_size
    
    def save_vocabulary(self, save_path: str):
        """ì–´íœ˜ ì €ì¥"""
        vocab_data = {
            "word_to_id": self.word_to_id,
            "id_to_word": {str(k): v for k, v in self.id_to_word.items()},
            "vocab_size": self.vocab_size
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        print(f"ì–´íœ˜ ì €ì¥ ì™„ë£Œ: {save_path}")


# í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    tokenizer = KoreanTokenizer()
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤
    test_sentences = [
        "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤.",
        "ë°˜ê°‘ìŠµë‹ˆë‹¤! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
        "AI ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì˜ ìƒì„±í•©ë‹ˆë‹¤.",
        "í•™ìŠµ ë°ì´í„°ëŠ” ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "í† í¬ë‚˜ì´ì €ê°€ í˜•íƒœì†Œë¥¼ ë¶„ì„í•´ìš”."
    ]
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}: {sentence}")
        
        # í† í¬ë‚˜ì´ì§•
        tokens = tokenizer.tokenize(sentence)
        print(f"  í† í°ë“¤: {tokens}")
        
        # ì¸ì½”ë”©
        encoded = tokenizer.encode(sentence)
        print(f"  í† í° IDë“¤: {encoded['input_ids'][:10]}...")  # ì²˜ìŒ 10ê°œë§Œ
        
        # ë””ì½”ë”©
        decoded = tokenizer.decode(encoded['input_ids'])
        print(f"  ë””ì½”ë”©: {decoded}")
        
        # ì¼ì¹˜ì„± í™•ì¸
        print(f"  ì›ë¬¸ ì¼ì¹˜: {'âœ…' if sentence.replace(',', ' ,').replace('.', ' .').replace('!', ' !') == decoded else 'âš ï¸'}")
    
    print(f"\nğŸ“Š ì–´íœ˜ í†µê³„:")
    print(f"  â€¢ ì´ ì–´íœ˜ í¬ê¸°: {tokenizer.get_vocab_size():,}")
    print(f"  â€¢ ë“±ë¡ëœ ì–´íœ˜: {len(tokenizer.word_to_id):,}")
    print(f"  â€¢ ì¡°ì‚¬ íŒ¨í„´: {len(tokenizer.josa_patterns)}ê°œ")
    print(f"  â€¢ ì–´ë¯¸ íŒ¨í„´: {len(tokenizer.eomi_patterns)}ê°œ") 
"""
ê°œì„ ëœ í•™ìŠµ í˜¸í™˜ í† í¬ë‚˜ì´ì €
í•´ì‹œ ê¸°ë°˜ í† í¬ë‚˜ì´ì§•ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¼ë°˜ì ì¸ í•œêµ­ì–´ ë‹¨ì–´ë“¤ì— ëŒ€í•œ ê³ ì • ë§¤í•‘ ì œê³µ
"""

import json
import os
from typing import List, Dict, Optional, Union


class ImprovedTrainingCompatibleTokenizer:
    """í•™ìŠµ í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë””ì½”ë”© ë¬¸ì œë¥¼ í•´ê²°í•œ í† í¬ë‚˜ì´ì €"""
    
    def __init__(self, vocab_file: Optional[str] = None):
        """
        Args:
            vocab_file: ì–´íœ˜ íŒŒì¼ ê²½ë¡œ (ì„ íƒì )
        """
        # íŠ¹ìˆ˜ í† í° - í•™ìŠµ ì½”ë“œì™€ ë™ì¼
        self.pad_token = "<pad>"
        self.bos_token = "<s>"  # BOS
        self.eos_token = "</s>"  # EOS
        self.unk_token = "<unk>"
        
        # íŠ¹ìˆ˜ í† í° ID - í•™ìŠµ ì½”ë“œì™€ ë™ì¼
        self.pad_token_id = 0
        self.bos_token_id = 1
        self.eos_token_id = 2
        self.unk_token_id = 3
        
        # ì–´íœ˜ í¬ê¸° - í•™ìŠµ ì½”ë“œì™€ ë™ì¼
        self.vocab_size = 65536
        
        # ì¼ë°˜ì ì¸ í•œêµ­ì–´ ë‹¨ì–´ë“¤ì— ëŒ€í•œ ê³ ì • ë§¤í•‘ (í•™ìŠµ ì‹œ ì‚¬ìš©ëœ í•´ì‹œê°’ ê¸°ë°˜)
        self._build_common_vocab()
        
        print(f"ê°œì„ ëœ í•™ìŠµ í˜¸í™˜ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  â€¢ ì–´íœ˜ í¬ê¸°: {self.vocab_size:,}")
        print(f"  â€¢ ê³ ì • ë§¤í•‘ ë‹¨ì–´ ìˆ˜: {len(self.word_to_id):,}")
        print(f"  â€¢ PAD: {self.pad_token_id}, BOS: {self.bos_token_id}, EOS: {self.eos_token_id}, UNK: {self.unk_token_id}")
        
    def _build_common_vocab(self):
        """ìì£¼ ì‚¬ìš©ë˜ëŠ” í•œêµ­ì–´ ë‹¨ì–´ë“¤ì— ëŒ€í•œ ê³ ì • ë§¤í•‘ êµ¬ì¶•"""
        common_words = [
            # ì¸ì‚¬
            "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•", "ë°˜ê°‘ìŠµë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì£„ì†¡í•©ë‹ˆë‹¤",
            # ê¸°ë³¸ ëª…ì‚¬
            "ì‚¬ëŒ", "ì§‘", "í•™êµ", "íšŒì‚¬", "ë¬¼", "ìŒì‹", "ì‹œê°„", "ë‚ ì”¨", "ëˆ",
            "í•œêµ­", "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°",
            # ê¸°ë³¸ ë™ì‚¬
            "ê°€ë‹¤", "ì˜¤ë‹¤", "ë³´ë‹¤", "ë“£ë‹¤", "ë§í•˜ë‹¤", "ë¨¹ë‹¤", "ë§ˆì‹œë‹¤", "ìë‹¤", "ì¼ì–´ë‚˜ë‹¤",
            "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "ì¢‹ë‹¤", "ë‚˜ì˜ë‹¤", "í¬ë‹¤", "ì‘ë‹¤",
            # ê¸°ë³¸ í˜•ìš©ì‚¬/ë¶€ì‚¬
            "ë§¤ìš°", "ì •ë§", "ì•„ì£¼", "ì¡°ê¸ˆ", "ë§ì´", "ë¹¨ë¦¬", "ì²œì²œíˆ", "ì˜", "ëª»",
            # ëŒ€ëª…ì‚¬
            "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ê·¸ë“¤", "ì´ê²ƒ", "ê·¸ê²ƒ", "ì €ê²ƒ", "ì—¬ê¸°", "ê±°ê¸°", "ì €ê¸°",
            # ìˆ˜ì‚¬
            "í•˜ë‚˜", "ë‘˜", "ì…‹", "ë„·", "ë‹¤ì„¯", "ì—¬ì„¯", "ì¼ê³±", "ì—¬ëŸ", "ì•„í™‰", "ì—´",
            # ê¸°íƒ€ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ë“¤
            "ë„¤", "ì•„ë‹ˆì˜¤", "ì˜ˆ", "ì•ˆ", "ëª»", "ë˜", "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜",
            "ì–´ë–¤", "ë¬´ì—‡", "ëˆ„êµ¬", "ì–¸ì œ", "ì–´ë””", "ì™œ", "ì–´ë–»ê²Œ",
            # ê¸°ìˆ /AI ê´€ë ¨ ìš©ì–´
            "ëª¨ë¸", "ë°ì´í„°", "í•™ìŠµ", "ì¶”ë¡ ", "AI", "ì¸ê³µì§€ëŠ¥", "ì»´í“¨í„°", "í”„ë¡œê·¸ë¨",
            "í…ìŠ¤íŠ¸", "ë¬¸ì¥", "ë‹¨ì–´", "í† í°", "ìƒì„±", "ì˜ˆì¸¡", "ê²°ê³¼", "ì„±ëŠ¥",
            # ì¼ë°˜ì ì¸ ë™ì‚¬ í™œìš©í˜•
            "í•©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤", "ê°‘ë‹ˆë‹¤", "ì˜µë‹ˆë‹¤", "ë´…ë‹ˆë‹¤", "ë“£ìŠµë‹ˆë‹¤", "ë§í•©ë‹ˆë‹¤",
            "ë¨¹ìŠµë‹ˆë‹¤", "ë§ˆì‹­ë‹ˆë‹¤", "ì¡ë‹ˆë‹¤", "ì¼ì–´ë‚©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ì¢‹ìŠµë‹ˆë‹¤",
            # ì¡°ì‚¬
            "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì—ì„œ", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼",
            "ì˜", "ë„", "ë§Œ", "ë¶€í„°", "ê¹Œì§€", "ì²˜ëŸ¼", "ê°™ì´", "ë³´ë‹¤", "ë§ˆë‹¤"
        ]
        
        # ë‹¨ì–´ â†’ ID ë§¤í•‘ êµ¬ì¶• (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ í•´ì‹œ ë°©ì‹)
        self.word_to_id = {}
        self.id_to_word = {}
        
        for word in common_words:
            word_id = hash(word) % 65535 + 1  # í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ í•´ì‹œ ë°©ì‹
            self.word_to_id[word] = word_id
            self.id_to_word[word_id] = word
        
        # ìì£¼ ìƒì„±ë˜ëŠ” í† í° IDë“¤ì— ëŒ€í•œ ì¶”ê°€ ë§¤í•‘ (ì˜ë¯¸ ì¶”ì •)
        frequent_tokens = {
            17356: "ê·¸ëŸ°ë°",    # ì—°ê²°ì–´ ì¶”ì •
            21857: "ì •ë§",      # ë¶€ì‚¬ ì¶”ì •  
            50820: "ê·¸ë¦¬ê³ ",    # ì—°ê²°ì–´ ì¶”ì •
            20390: "ì¢‹ë‹¤",      # í˜•ìš©ì‚¬ ì¶”ì •
            3031: "ë§ì´",       # ë¶€ì‚¬ ì¶”ì •
            61151: "í•˜ì§€ë§Œ",    # ì—°ê²°ì–´ ì¶”ì •
            1400: "ë˜",         # ì—°ê²°ì–´ ì¶”ì •
            11459: "ì•„ì£¼",      # ë¶€ì‚¬ ì¶”ì •
            19034: "ê·¸ë˜ì„œ",    # ì—°ê²°ì–´ ì¶”ì •
            30346: "ì •ë§ë¡œ",    # ë¶€ì‚¬ ì¶”ì •
            34772: "ë§¤ìš°",      # ë¶€ì‚¬ ì¶”ì •
            18171: "ê·¸ëŸ¬ë‚˜",    # ì—°ê²°ì–´ ì¶”ì •
            59820: "ë˜ë‹¤",      # ë™ì‚¬ ì¶”ì •
            41887: "ìˆë‹¤",      # ë™ì‚¬ ì¶”ì •
            14944: "ì˜",        # ë¶€ì‚¬ ì¶”ì •
            8749: "ë”",         # ë¶€ì‚¬ ì¶”ì •
            39047: "í¬ë‹¤",      # í˜•ìš©ì‚¬ ì¶”ì •
            33897: "ë˜í•œ",      # ì—°ê²°ì–´ ì¶”ì •
            8306: "ê·¸ê²ƒ",       # ëŒ€ëª…ì‚¬ ì¶”ì •
            9203: "ì´ê²ƒ",       # ëŒ€ëª…ì‚¬ ì¶”ì •
            56615: "ë”°ë¼ì„œ",    # ì—°ê²°ì–´ ì¶”ì •
            35299: "ë„ˆë¬´",      # ë¶€ì‚¬ ì¶”ì •
        }
        
        # ì¶”ì • ë§¤í•‘ ì¶”ê°€
        for token_id, estimated_word in frequent_tokens.items():
            if token_id not in self.id_to_word:  # ê¸°ì¡´ ë§¤í•‘ê³¼ ì¶©ëŒí•˜ì§€ ì•ŠëŠ” ê²½ìš°ë§Œ
                self.id_to_word[token_id] = f"{estimated_word}*"  # *ë¡œ ì¶”ì •ì„ì„ í‘œì‹œ
                
        print(f"  â€¢ ì¶”ì • ë§¤í•‘ ì¶”ê°€: {len(frequent_tokens)}ê°œ")
    
    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í•  - í•™ìŠµ ì½”ë“œì™€ ë™ì¼"""
        return text.split()
    
    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:
        """í† í°ì„ IDë¡œ ë³€í™˜ - í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ í•´ì‹œ ë°©ì‹"""
        token_ids = []
        for token in tokens:
            # íŠ¹ìˆ˜ í† í° ì²˜ë¦¬
            if token == self.pad_token:
                token_ids.append(self.pad_token_id)
            elif token == self.bos_token:
                token_ids.append(self.bos_token_id)
            elif token == self.eos_token:
                token_ids.append(self.eos_token_id)
            elif token == self.unk_token:
                token_ids.append(self.unk_token_id)
            else:
                # í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ í•´ì‹œ ë°©ì‹: hash(token) % 65535 + 1
                token_id = hash(token) % 65535 + 1
                token_ids.append(token_id)
        
        return token_ids
    
    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:
        """IDë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜ (ê°œì„ ëœ ì—­ë§¤í•‘)"""
        tokens = []
        for token_id in ids:
            if token_id == self.pad_token_id:
                tokens.append("")  # PAD í† í°ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
            elif token_id == self.bos_token_id:
                tokens.append("")  # BOS í† í°ë„ ì¶œë ¥ì—ì„œ ìˆ¨ê¹€
            elif token_id == self.eos_token_id:
                tokens.append("")  # EOS í† í°ë„ ì¶œë ¥ì—ì„œ ìˆ¨ê¹€
            elif token_id == self.unk_token_id:
                tokens.append("[UNK]")
            elif token_id in self.id_to_word:
                # ê³ ì • ë§¤í•‘ì—ì„œ ì°¾ì€ ê²½ìš° - ì‹¤ì œ í•œêµ­ì–´ ë‹¨ì–´
                tokens.append(self.id_to_word[token_id])
            else:
                # ë§¤í•‘ì— ì—†ëŠ” í† í°ì€ ê°„ê²°í•˜ê²Œ í‘œì‹œ
                tokens.append(f"[{token_id}]")
        
        return tokens
        
        return tokens
    
    def encode(self, text, add_special_tokens=True, return_tensors=None, **kwargs):
        """í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ì¸ì½”ë”©"""
        tokens = self.tokenize(text)
        
        if add_special_tokens:
            tokens = [self.bos_token] + tokens + [self.eos_token]
        
        input_ids = self.convert_tokens_to_ids(tokens)
        
        if return_tensors == "pt":
            import torch
            result = {
                "input_ids": torch.tensor([input_ids], dtype=torch.long),
                "attention_mask": torch.ones(1, len(input_ids), dtype=torch.long)
            }
            print(f"âœ… í† í¬ë‚˜ì´ì§• ì™„ë£Œ: {len(input_ids)}ê°œ í† í° â†’ shape {result['input_ids'].shape}")
            return result
        
        return input_ids
    
    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (ê°œì„ ëœ ë²„ì „)"""
        if hasattr(token_ids, 'tolist'):
            token_ids = token_ids.tolist()
        
        tokens = self.convert_ids_to_tokens(token_ids)
        
        # ë¹ˆ ë¬¸ìì—´ê³¼ ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        filtered_tokens = []
        for token in tokens:
            if token and token.strip():  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                filtered_tokens.append(token)
        
        # í† í°ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°
        return " ".join(filtered_tokens)
    def get_vocab_coverage(self, text: str) -> Dict[str, float]:
        """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì–´íœ˜ ì»¤ë²„ë¦¬ì§€ í™•ì¸"""
        tokens = self.tokenize(text)
        total_tokens = len(tokens)
        covered_tokens = sum(1 for token in tokens if token in self.word_to_id)
        
        coverage = covered_tokens / total_tokens if total_tokens > 0 else 0.0
        
        return {
            "total_tokens": total_tokens,
            "covered_tokens": covered_tokens,
            "coverage_ratio": coverage,
            "uncovered_tokens": [token for token in tokens if token not in self.word_to_id]
        }
    
    def batch_encode_plus(self, texts: List[str], 
                         add_special_tokens: bool = True,
                         max_length: Optional[int] = None,
                         padding: bool = True,
                         truncation: bool = True,
                         return_tensors: Optional[str] = None) -> Dict[str, any]:
        """ë°°ì¹˜ ì¸ì½”ë”©"""
        all_input_ids = []
        all_attention_masks = []
        
        for text in texts:
            encoded = self.encode(
                text, 
                add_special_tokens=add_special_tokens,
                return_tensors=None  # ê°œë³„ì ìœ¼ë¡œëŠ” í…ì„œë¡œ ë³€í™˜í•˜ì§€ ì•ŠìŒ
            )
            all_input_ids.append(encoded)
            all_attention_masks.append([1] * len(encoded))
        
        # íŒ¨ë”© ì²˜ë¦¬
        if padding and max_length:
            for i in range(len(all_input_ids)):
                current_length = len(all_input_ids[i])
                if current_length < max_length:
                    padding_length = max_length - current_length
                    all_input_ids[i].extend([self.pad_token_id] * padding_length)
                    all_attention_masks[i].extend([0] * padding_length)
                elif truncation and current_length > max_length:
                    all_input_ids[i] = all_input_ids[i][:max_length]
                    all_attention_masks[i] = all_attention_masks[i][:max_length]
        
        result = {
            "input_ids": all_input_ids,
            "attention_mask": all_attention_masks
        }
        
        if return_tensors == "pt":
            import torch
            result["input_ids"] = torch.tensor(result["input_ids"], dtype=torch.long)
            result["attention_mask"] = torch.tensor(result["attention_mask"], dtype=torch.long)
        
        return result


# ê¸°ì¡´ TrainingCompatibleTokenizerë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜
def create_improved_tokenizer(vocab_file: Optional[str] = None):
    """ê°œì„ ëœ í† í¬ë‚˜ì´ì € ìƒì„±"""
    return ImprovedTrainingCompatibleTokenizer(vocab_file)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    tokenizer = ImprovedTrainingCompatibleTokenizer()
    
    test_texts = [
        "ì•ˆë…•í•˜ì„¸ìš” í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤",
        "ëª¨ë¸ í•™ìŠµì´ ì˜ ë˜ê³  ìˆìŠµë‹ˆë‹¤",
        "AI ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤"
    ]
    
    print("\n" + "=" * 60)
    print("ğŸ§ª ê°œì„ ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    for text in test_texts:
        print(f"\nì›ë³¸: '{text}'")
        
        # ì–´íœ˜ ì»¤ë²„ë¦¬ì§€ í™•ì¸
        coverage = tokenizer.get_vocab_coverage(text)
        print(f"ì»¤ë²„ë¦¬ì§€: {coverage['coverage_ratio']:.1%} ({coverage['covered_tokens']}/{coverage['total_tokens']})")
        if coverage['uncovered_tokens']:
            print(f"ë§¤í•‘ë˜ì§€ ì•Šì€ í† í°: {coverage['uncovered_tokens']}")
        
        # ì¸ì½”ë”©/ë””ì½”ë”© í…ŒìŠ¤íŠ¸
        tokens = tokenizer.tokenize(text)
        token_ids = tokenizer.convert_tokens_to_ids(tokens)
        decoded = tokenizer.decode(token_ids)
        
        print(f"í† í°: {tokens}")
        print(f"í† í° ID: {token_ids}")
        print(f"ë””ì½”ë”©: '{decoded}'") 
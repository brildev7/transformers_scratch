"""
í•œêµ­ì–´ ì†Œí˜• ì–¸ì–´ëª¨ë¸ ì¶”ë¡  ì—”ì§„
"""

import torch
import time
from typing import Dict, List, Optional, Union
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))
from model import InferenceModel
from tokenizer import SimpleTokenizer
from training_compatible_model import TrainingCompatibleModel
from training_compatible_tokenizer import TrainingCompatibleTokenizer
from improved_tokenizer import ImprovedTrainingCompatibleTokenizer
from korean_tokenizer import KoreanTokenizer


class InferenceEngine:
    """ì¶”ë¡  ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 model,  # InferenceModel ë˜ëŠ” TrainingCompatibleModel
                 tokenizer,  # SimpleTokenizer ë˜ëŠ” TrainingCompatibleTokenizer
                 device: str = "auto"):
        """
        Args:
            model: ì¶”ë¡ ìš© ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €
            device: ë””ë°”ì´ìŠ¤ ("auto", "cpu", "cuda")
        """
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device
        
        # ëª¨ë¸ íƒ€ì… í™•ì¸
        self.is_training_compatible = isinstance(model, TrainingCompatibleModel)
        if self.is_training_compatible:
            print(f"ğŸ”„ í•™ìŠµ í˜¸í™˜ ëª¨ë“œë¡œ ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {device})")
        else:
            print(f"ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
    
    def generate_text(self,
                     prompt: str,
                     max_length: int = 100,
                     temperature: float = 1.0,
                     top_k: int = 50,
                     top_p: float = 0.9,
                     do_sample: bool = True,
                     num_return_sequences: int = 1,
                     return_prompt: bool = False) -> Union[str, List[str]]:
        """
        í”„ë¡¬í”„íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_k: top-k ìƒ˜í”Œë§
            top_p: nucleus ìƒ˜í”Œë§
            do_sample: ìƒ˜í”Œë§ ì—¬ë¶€
            num_return_sequences: ë°˜í™˜í•  ì‹œí€€ìŠ¤ ìˆ˜
            return_prompt: í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜í• ì§€ ì—¬ë¶€
        
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸ (ë‹¨ì¼ ì‹œí€€ìŠ¤ë©´ str, ë‹¤ì¤‘ì´ë©´ List[str])
        """
        if not prompt.strip():
            return "" if num_return_sequences == 1 else [""]
        
        print(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        
        # í•™ìŠµ í˜¸í™˜ ëª¨ë¸ì¸ ê²½ìš° ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if self.is_training_compatible:
            # í•™ìŠµ ë°ì´í„°ì™€ ìœ ì‚¬í•œ í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            formatted_prompt = f"ì‚¬ìš©ì: {prompt}\në´‡: "
            print(f"í”„ë¡¬í”„íŠ¸: '{formatted_prompt}'")
        else:
            formatted_prompt = prompt
            print(f"í”„ë¡¬í”„íŠ¸: '{prompt}'")
        
        start_time = time.time()
        
        # í† í¬ë‚˜ì´ì§•
        if self.is_training_compatible:
            # í•™ìŠµ í˜¸í™˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            encoded = self.tokenizer.encode(
                formatted_prompt, 
                add_special_tokens=True,
                return_tensors="pt"
            )
            input_tensor = encoded["input_ids"].to(self.device)
            attention_mask = encoded.get("attention_mask")
            if attention_mask is not None:
                attention_mask = attention_mask.to(self.device)
        else:
            # ê¸°ì¡´ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            input_ids = self.tokenizer.encode(formatted_prompt, add_special_tokens=True)
            input_tensor = torch.tensor([input_ids], device=self.device)
            attention_mask = None
        
        # ë°°ì¹˜ í¬ê¸° í™•ì¥ (ë‹¤ì¤‘ ì‹œí€€ìŠ¤ ìƒì„±ìš©)
        if num_return_sequences > 1:
            input_tensor = input_tensor.repeat(num_return_sequences, 1)
            if attention_mask is not None:
                attention_mask = attention_mask.repeat(num_return_sequences, 1)
        
        prompt_length = input_tensor.size(1)
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        self.model.eval()
        with torch.no_grad():
            if self.is_training_compatible:
                # TrainingCompatibleModelì˜ generate ë©”ì„œë“œ ì‚¬ìš©
                generated_ids = self.model.generate(
                    input_ids=input_tensor,
                    attention_mask=attention_mask,
                    max_length=max_length,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            else:
                # ê¸°ì¡´ InferenceModelì˜ generate ë©”ì„œë“œ ì‚¬ìš©
                generated_ids = self.model.generate(
                    input_ids=input_tensor,
                    max_length=max_length,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id
                )
        
        # í”„ë¡¬í”„íŠ¸ ì œê±° (ì„ íƒì )
        if not return_prompt:
            generated_ids = generated_ids[:, prompt_length:]
        
        # ë””ì½”ë”©
        if num_return_sequences == 1:
            generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            
            end_time = time.time()
            generation_time = end_time - start_time
            tokens_per_second = len(generated_ids[0]) / generation_time if generation_time > 0 else 0
            
            print(f"ìƒì„± ì™„ë£Œ (ì‹œê°„: {generation_time:.2f}ì´ˆ, ì†ë„: {tokens_per_second:.1f} í† í°/ì´ˆ)")
            print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: '{generated_text}'")
            
            return generated_text
        else:
            generated_texts = []
            for i in range(num_return_sequences):
                text = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)
                generated_texts.append(text)
            
            end_time = time.time()
            generation_time = end_time - start_time
            avg_tokens = sum(len(seq) for seq in generated_ids) / num_return_sequences
            tokens_per_second = avg_tokens / generation_time if generation_time > 0 else 0
            
            print(f"ìƒì„± ì™„ë£Œ (ì‹œê°„: {generation_time:.2f}ì´ˆ, í‰ê·  ì†ë„: {tokens_per_second:.1f} í† í°/ì´ˆ)")
            for i, text in enumerate(generated_texts):
                print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸ {i+1}: '{text}'")
            
            return generated_texts
    
    def _generate_raw(self,
                     prompt: str,
                     max_length: int = 100,
                     temperature: float = 1.0,
                     top_k: int = 50,
                     top_p: float = 0.9,
                     do_sample: bool = True,
                     num_return_sequences: int = 1,
                     return_prompt: bool = False) -> Union[str, List[str]]:
        """
        ì›ì‹œ í…ìŠ¤íŠ¸ ìƒì„± (ì¶”ê°€ í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… ì—†ìŒ)
        """
        
        if not prompt.strip():
            return "" if num_return_sequences == 1 else [""]
        
        print(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        print(f"í”„ë¡¬í”„íŠ¸: '{prompt}'")
        
        start_time = time.time()
        
        # í† í¬ë‚˜ì´ì§• (í¬ë§·íŒ… ì—†ì´)
        try:
            if self.is_training_compatible:
                # í•™ìŠµ í˜¸í™˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                encoded = self.tokenizer.encode(
                    prompt, 
                    add_special_tokens=True,
                    return_tensors="pt"
                )
                if isinstance(encoded, dict):
                    pass  # ë””ì½”ë”© ì„±ê³µ
                input_tensor = encoded["input_ids"].to(self.device)
                attention_mask = encoded.get("attention_mask")
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                else:
                    print(f"ğŸ” attention_mask: None")
            else:
                # ê¸°ì¡´ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                input_ids = self.tokenizer.encode(prompt, add_special_tokens=True)
                input_tensor = torch.tensor([input_ids], device=self.device)
                attention_mask = None
            
            
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì§• ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return "" if num_return_sequences == 1 else [""]
        
        # ë°°ì¹˜ í¬ê¸° í™•ì¥ (ë‹¤ì¤‘ ì‹œí€€ìŠ¤ ìƒì„±ìš©)
        if num_return_sequences > 1:
            input_tensor = input_tensor.repeat(num_return_sequences, 1)
            if attention_mask is not None:
                attention_mask = attention_mask.repeat(num_return_sequences, 1)
        
        prompt_length = input_tensor.size(1)
        
        # í…ìŠ¤íŠ¸ ìƒì„±
        self.model.eval()
        with torch.no_grad():
            if self.is_training_compatible:
                # TrainingCompatibleModelì˜ generate ë©”ì„œë“œ ì‚¬ìš©
                generated_ids = self.model.generate(
                    input_ids=input_tensor,
                    attention_mask=attention_mask,
                    max_length=max_length,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            else:
                # ê¸°ì¡´ InferenceModelì˜ generate ë©”ì„œë“œ ì‚¬ìš©
                generated_ids = self.model.generate(
                    input_ids=input_tensor,
                    max_length=max_length,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id
                )
        
        # í”„ë¡¬í”„íŠ¸ ì œê±° (ì„ íƒì )
        if not return_prompt:
            generated_ids = generated_ids[:, prompt_length:]
        
        # ë””ì½”ë”©
        if num_return_sequences == 1:
            generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            
            end_time = time.time()
            generation_time = end_time - start_time
            tokens_per_second = len(generated_ids[0]) / generation_time if generation_time > 0 else 0
            
            print(f"ìƒì„± ì™„ë£Œ (ì‹œê°„: {generation_time:.2f}ì´ˆ, ì†ë„: {tokens_per_second:.1f} í† í°/ì´ˆ)")
            print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: '{generated_text}'")
            
            return generated_text
        else:
            generated_texts = []
            for i in range(num_return_sequences):
                text = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True)
                generated_texts.append(text)
            
            end_time = time.time()
            generation_time = end_time - start_time
            avg_tokens = sum(len(seq) for seq in generated_ids) / num_return_sequences
            tokens_per_second = avg_tokens / generation_time if generation_time > 0 else 0
            
            print(f"ìƒì„± ì™„ë£Œ (ì‹œê°„: {generation_time:.2f}ì´ˆ, í‰ê·  ì†ë„: {tokens_per_second:.1f} í† í°/ì´ˆ)")
            for i, text in enumerate(generated_texts):
                print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸ {i+1}: '{text}'")
            
            return generated_texts
    
    def complete_text(self, 
                     incomplete_text: str,
                     max_completion_length: int = 50,
                     temperature: float = 0.8,
                     top_k: int = 50,
                     top_p: float = 0.9) -> str:
        """
        ë¶ˆì™„ì „í•œ í…ìŠ¤íŠ¸ ì™„ì„±
        
        Args:
            incomplete_text: ì™„ì„±í•  í…ìŠ¤íŠ¸
            max_completion_length: ìµœëŒ€ ì™„ì„± ê¸¸ì´
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_k: top-k ìƒ˜í”Œë§
            top_p: nucleus ìƒ˜í”Œë§
        
        Returns:
            ì™„ì„±ëœ í…ìŠ¤íŠ¸
        """
        print(f"í…ìŠ¤íŠ¸ ì™„ì„± ì¤‘: '{incomplete_text}'")
        
        completed = self.generate_text(
            prompt=incomplete_text,
            max_length=max_completion_length,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            do_sample=True,
            return_prompt=False
        )
        
        full_text = incomplete_text + completed
        print(f"ì™„ì„±ëœ í…ìŠ¤íŠ¸: '{full_text}'")
        
        return full_text
    
    def chat_generate(self,
                     message: str,
                     chat_history: Optional[List[Dict[str, str]]] = None,
                     max_length: int = 150,
                     temperature: float = 0.9) -> str:
        """
        ëŒ€í™”í˜• í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            chat_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        print(f"ğŸ” chat_generate í˜¸ì¶œ: is_training_compatible={self.is_training_compatible}")
        
        if self.is_training_compatible:
            # í•™ìŠµ í˜¸í™˜ ëª¨ë“œ: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            print(f"âœ… í•™ìŠµ í˜¸í™˜ ëª¨ë“œë¡œ ì§„ì…")
            context = ""
            
            if chat_history:
                for turn in chat_history[-5:]:  # ìµœê·¼ 5í„´ë§Œ ì‚¬ìš©
                    if turn.get("role") == "user":
                        context += f"ì‚¬ìš©ì: {turn['content']}\n"
                    elif turn.get("role") == "assistant":
                        context += f"ë´‡: {turn['content']}\n"
            
            # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€ (generate_textì—ì„œ ì¶”ê°€ í¬ë§·íŒ…í•˜ì§€ ì•ŠìŒ)
            context += f"ì‚¬ìš©ì: {message}\në´‡: "
            
            # ì‘ë‹µ ìƒì„± (ì´ë¯¸ í¬ë§·íŒ…ëœ context ì‚¬ìš©, bypass_formatting=True)
            response = self._generate_raw(
                prompt=context,
                max_length=max_length,
                temperature=temperature,
                top_k=40,
                top_p=0.9,
                do_sample=True,
                return_prompt=False
            )
        else:
            # ê¸°ì¡´ ëª¨ë“œ: ë‹¨ìˆœ ë©”ì‹œì§€ ì „ë‹¬
            print(f"âŒ ê¸°ì¡´ ëª¨ë“œë¡œ ì§„ì… (í•™ìŠµ í˜¸í™˜ ì•„ë‹˜)")
            response = self.generate_text(
                prompt=message,
                max_length=max_length,
                temperature=temperature,
                top_k=40,
                top_p=0.9,
                do_sample=True,
                return_prompt=False
            )
        
        # ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
        response = response.split('\n')[0].strip()  # ì²« ë²ˆì§¸ ì¤„ë§Œ ì‚¬ìš©
        
        return response
    
    def get_model_info(self) -> Dict[str, Union[str, int]]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° ë°©ì‹ ê²°ì •
        if hasattr(self.model, 'get_num_params'):
            num_params = self.model.get_num_params()
        else:
            num_params = sum(p.numel() for p in self.model.parameters())
        
        # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        if self.is_training_compatible and hasattr(self.model, 'config'):
            config = self.model.config
            model_name = config.model_name
            vocab_size = config.vocab_size
            max_pos = config.max_position_embeddings
            hidden_size = config.hidden_size
            num_layers = config.num_layers
            num_heads = config.num_heads
        else:
            model_name = "Korean SLLM"
            vocab_size = getattr(self.tokenizer, 'vocab_size', len(self.tokenizer))
            max_pos = getattr(self.model.config, 'max_position_embeddings', 4096) if hasattr(self.model, 'config') else 4096
            hidden_size = getattr(self.model.config, 'hidden_size', 2048) if hasattr(self.model, 'config') else 2048
            num_layers = getattr(self.model.config, 'num_layers', 24) if hasattr(self.model, 'config') else 24
            num_heads = getattr(self.model.config, 'num_heads', 32) if hasattr(self.model, 'config') else 32
        
        return {
            "model_name": model_name,
            "vocab_size": vocab_size,
            "model_parameters": num_params,
            "device": self.device,
            "max_position_embeddings": max_pos,
            "hidden_size": hidden_size,
            "num_layers": num_layers,
            "num_heads": num_heads
        }
    
    def benchmark(self, prompt: str = "ì•ˆë…•í•˜ì„¸ìš”", num_runs: int = 5) -> Dict[str, float]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        print(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ({num_runs}íšŒ)...")
        
        times = []
        token_counts = []
        
        for i in range(num_runs):
            print(f"  ì‹¤í–‰ {i+1}/{num_runs}")
            
            start_time = time.time()
            result = self.generate_text(
                prompt=prompt,
                max_length=50,
                temperature=1.0,
                do_sample=True
            )
            end_time = time.time()
            
            generation_time = end_time - start_time
            token_count = len(self.tokenizer.encode(result))
            
            times.append(generation_time)
            token_counts.append(token_count)
        
        avg_time = sum(times) / len(times)
        avg_tokens = sum(token_counts) / len(token_counts)
        avg_tokens_per_second = avg_tokens / avg_time if avg_time > 0 else 0
        
        benchmark_results = {
            "average_time_seconds": avg_time,
            "average_tokens_generated": avg_tokens,
            "average_tokens_per_second": avg_tokens_per_second,
            "min_time_seconds": min(times),
            "max_time_seconds": max(times)
        }
        
        print("ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        for key, value in benchmark_results.items():
            print(f"  {key}: {value:.3f}")
        
        return benchmark_results
    
    @classmethod
    def from_checkpoint(cls,
                       checkpoint_path: str,
                       tokenizer_path: Optional[str] = None,
                       device: str = "auto",
                       use_training_compatible: bool = True,
                       use_improved_tokenizer: bool = True,
                       use_korean_tokenizer: bool = False) -> "InferenceEngine":
        """
        ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¶”ë¡  ì—”ì§„ ë¡œë“œ
        
        Args:
            checkpoint_path: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            tokenizer_path: í† í¬ë‚˜ì´ì € ì–´íœ˜ íŒŒì¼ ê²½ë¡œ (ì„ íƒì )
            device: ë””ë°”ì´ìŠ¤
            use_training_compatible: í•™ìŠµ í˜¸í™˜ ëª¨ë¸/í† í¬ë‚˜ì´ì € ì‚¬ìš© ì—¬ë¶€
            use_improved_tokenizer: ê°œì„ ëœ í† í¬ë‚˜ì´ì € ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì¶”ë¡  ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
        """
        print("ì¶”ë¡  ì—”ì§„ ë¡œë“œ ì¤‘...")
        
        if use_training_compatible:
            print("ğŸ”„ í•™ìŠµ í˜¸í™˜ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            
            # í•™ìŠµ í˜¸í™˜ ëª¨ë¸ ë¡œë“œ
            try:
                model = TrainingCompatibleModel.from_pretrained(checkpoint_path, device)
                print("âœ… í•™ìŠµ í˜¸í™˜ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ í•™ìŠµ í˜¸í™˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ê¸°ë³¸ InferenceModelë¡œ í´ë°±")
                model = InferenceModel.from_pretrained(checkpoint_path, device)
                use_training_compatible = False
            
            # í† í¬ë‚˜ì´ì € ì„ íƒ
            if use_training_compatible:
                if use_korean_tokenizer:
                    print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
                    tokenizer = KoreanTokenizer(tokenizer_path)
                    print("âœ… í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
                elif use_improved_tokenizer:
                    print("ğŸš€ ê°œì„ ëœ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
                    tokenizer = ImprovedTrainingCompatibleTokenizer(tokenizer_path)
                    print("âœ… ê°œì„ ëœ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
                else:
                    print("ğŸ”„ ê¸°ë³¸ í•™ìŠµ í˜¸í™˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
                    tokenizer = TrainingCompatibleTokenizer(tokenizer_path)
                    print("âœ… í•™ìŠµ í˜¸í™˜ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
            else:
                tokenizer = SimpleTokenizer(tokenizer_path)
        else:
            print("ê¸°ë³¸ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            model = InferenceModel.from_pretrained(checkpoint_path, device)
            
            # ê¸°ì¡´ í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = SimpleTokenizer(tokenizer_path)
        
        return cls(model, tokenizer, device) 
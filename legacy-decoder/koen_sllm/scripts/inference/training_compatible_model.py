"""
í•™ìŠµ ì½”ë“œì™€ í˜¸í™˜ë˜ëŠ” ì¶”ë¡ ìš© ëª¨ë¸
train_h100_dual.pyì˜ SimpleTransformerModelê³¼ ë™ì¼í•œ êµ¬ì¡°
"""

import torch
import torch.nn as nn
import json
import os
from typing import Dict, Optional, Union
from dataclasses import dataclass
from pathlib import Path


@dataclass
class TrainingCompatibleConfig:
    """í•™ìŠµ í˜¸í™˜ ì„¤ì • (train_h100_dual.pyì™€ ë™ì¼)"""
    # ëª¨ë¸ ì„¤ì • - í•™ìŠµ ì½”ë“œì™€ ë™ì¼
    model_name: str = "korean-sllm-h100"
    vocab_size: int = 65536
    hidden_size: int = 2048
    num_layers: int = 24
    num_heads: int = 32
    intermediate_size: int = 8192
    max_position_embeddings: int = 4096
    
    # ì¶”ë¡  ì„¤ì •
    dropout: float = 0.1
    layer_norm_eps: float = 1e-5
    
    @classmethod
    def from_json(cls, json_path: str):
        """JSON íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # í•™ìŠµ ì„¤ì •ì„ ì¶”ë¡  ì„¤ì •ìœ¼ë¡œ ë§¤í•‘
        config = cls()
        if 'vocab_size' in data:
            config.vocab_size = data['vocab_size']
        if 'hidden_size' in data:
            config.hidden_size = data['hidden_size']
        if 'num_layers' in data:
            config.num_layers = data['num_layers']
        if 'num_heads' in data:
            config.num_heads = data['num_heads']
        if 'intermediate_size' in data:
            config.intermediate_size = data['intermediate_size']
        if 'max_position_embeddings' in data:
            config.max_position_embeddings = data['max_position_embeddings']
            
        return config


class TrainingCompatibleModel(nn.Module):
    """í•™ìŠµìš© SimpleTransformerModelê³¼ ë™ì¼í•œ êµ¬ì¡°"""
    
    def __init__(self, config: TrainingCompatibleConfig):
        super().__init__()
        self.config = config
        
        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        
        # Transformer layers - í•™ìŠµ ì½”ë“œì™€ ë™ì¼
        layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_size,
            nhead=config.num_heads,
            dim_feedforward=config.intermediate_size,
            dropout=config.dropout,
            activation="gelu",
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(layer, num_layers=config.num_layers)
        
        self.ln_f = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        
        # weight tying - í•™ìŠµ ì½”ë“œì™€ ë™ì¼
        self.head.weight = self.embeddings.weight
        
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” - í•™ìŠµ ì½”ë“œì™€ ë™ì¼"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
            
    def forward(self, input_ids, attention_mask=None, labels=None):
        """Forward pass - í•™ìŠµ ì½”ë“œì™€ ë™ì¼"""
        batch_size, seq_len = input_ids.shape
        
        # Position ids
        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # Embeddings
        inputs_embeds = self.embeddings(input_ids)
        position_embeds = self.position_embeddings(position_ids)
        hidden_states = inputs_embeds + position_embeds
        
        # Attention mask for transformer
        if attention_mask is not None:
            attention_mask = attention_mask.bool()
            attention_mask = ~attention_mask  # invert for transformer
        
        # Transformer
        hidden_states = self.transformer(hidden_states, src_key_padding_mask=attention_mask)
        hidden_states = self.ln_f(hidden_states)
        
        # Language model head
        logits = self.head(hidden_states)
        
        loss = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            
        return {"loss": loss, "logits": logits}
    
    def generate(self, input_ids, attention_mask=None, max_length=100, 
                 temperature=1.0, top_k=50, top_p=0.9, do_sample=True,
                 pad_token_id=0, eos_token_id=2):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        print(f"ğŸš€ Generate í˜¸ì¶œ: max_length={max_length}, input shape={input_ids.shape}")
        
        self.eval()
        
        batch_size = input_ids.size(0)
        device = input_ids.device
        
        # ìƒì„± ê³¼ì •ì—ì„œ ì‚¬ìš©í•  ë³€ìˆ˜ë“¤
        generated = input_ids.clone()
        past_length = input_ids.size(1)
        
        print(f"ğŸ” ì‹œì‘: batch_size={batch_size}, past_length={past_length}, max_length={max_length}")
        
        # ìƒì„±í•  í† í° ìˆ˜ ê³„ì‚°
        tokens_to_generate = max_length - past_length
        if tokens_to_generate <= 0:
            print(f"âš ï¸ ìƒì„±í•  í† í°ì´ ì—†ìŒ: í˜„ì¬ ê¸¸ì´ {past_length} >= max_length {max_length}")
            return generated
        
        with torch.no_grad():
            for step in range(tokens_to_generate):
                print(f"ğŸ”„ Step {step}: í˜„ì¬ ê¸¸ì´={generated.size(1)}")
                
                if step >= tokens_to_generate:
                    print(f"ğŸ”š ìµœëŒ€ ê¸¸ì´ ë„ë‹¬: {step}/{tokens_to_generate}")
                    break
                    
                try:
                    # Forward pass
                    outputs = self.forward(generated, attention_mask=attention_mask)
                    logits = outputs["logits"]
                    
                    # ë§ˆì§€ë§‰ í† í°ì˜ ë¡œì§“
                    next_token_logits = logits[:, -1, :] / temperature
                    
                    # Top-k í•„í„°ë§ (ì•ˆì „í•œ ë°©ì‹)
                    if top_k > 0 and top_k < next_token_logits.size(-1):
                        try:
                            values, indices = torch.topk(next_token_logits, k=top_k, dim=-1)
                            min_values = values[:, -1:].expand_as(next_token_logits)
                            indices_to_remove = next_token_logits < min_values
                            next_token_logits[indices_to_remove] = float('-inf')
                        except Exception as e:
                            print(f"âŒ Top-k í•„í„°ë§ ì˜¤ë¥˜: {e}")
                    
                    # Top-p í•„í„°ë§ (ì•ˆì „í•œ ë°©ì‹)
                    if top_p < 1.0:
                        try:
                            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)
                            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                            sorted_indices_to_remove = cumulative_probs > top_p
                            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                            sorted_indices_to_remove[:, 0] = False
                            indices_to_remove = torch.zeros_like(next_token_logits, dtype=torch.bool)
                            indices_to_remove.scatter_(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)
                            next_token_logits[indices_to_remove] = float('-inf')
                        except Exception as e:
                            print(f"âŒ Top-p í•„í„°ë§ ì˜¤ë¥˜: {e}")
                    
                    # ìƒ˜í”Œë§
                    try:
                        if do_sample:
                            probs = torch.softmax(next_token_logits, dim=-1)
                            next_token = torch.multinomial(probs, num_samples=1)
                        else:
                            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                        
                        # ìƒì„±ëœ í† í° ì¶”ê°€
                        generated = torch.cat([generated, next_token], dim=1)
                        token_value = next_token[0, 0].item()
                        
                        if step % 10 == 0:  # 10ìŠ¤í…ë§ˆë‹¤ë§Œ ì¶œë ¥
                            print(f"ğŸ”„ Step {step}: í† í° {token_value} ìƒì„±, ê¸¸ì´ {generated.size(1)}")
                        
                        # EOS í† í°ì´ ìƒì„±ë˜ë©´ ì¤‘ë‹¨
                        if token_value == eos_token_id:
                            print(f"ğŸ”š EOS í† í° ê°ì§€, ìƒì„± ì¤‘ë‹¨ (ê¸¸ì´: {generated.size(1)})")
                            break
                            
                        # attention_mask ì—…ë°ì´íŠ¸
                        if attention_mask is not None:
                            attention_mask = torch.cat([
                                attention_mask, 
                                torch.ones(batch_size, 1, device=device, dtype=attention_mask.dtype)
                            ], dim=1)
                            
                    except Exception as e:
                        print(f"âŒ ìƒ˜í”Œë§ ì˜¤ë¥˜: {e}")
                        break
                        
                        # EOS í† í°ì´ ìƒì„±ë˜ë©´ ì¤‘ë‹¨
                        if token_value == eos_token_id:
                            print(f"ğŸ”š EOS í† í° ê°ì§€, ìƒì„± ì¤‘ë‹¨ (ê¸¸ì´: {generated.size(1)})")
                            break
                            
                        # attention_mask ì—…ë°ì´íŠ¸
                        if attention_mask is not None:
                            attention_mask = torch.cat([
                                attention_mask, 
                                torch.ones(batch_size, 1, device=device, dtype=attention_mask.dtype)
                            ], dim=1)
                            
                    except Exception as e:
                        print(f"âŒ ìƒ˜í”Œë§ ì˜¤ë¥˜: {e}")
                        break
                        
                except Exception as e:
                    print(f"âŒ Generation step {step} ì˜¤ë¥˜: {e}")
                    print(f"   generated shape: {generated.shape}")
                    print(f"   attention_mask: {attention_mask.shape if attention_mask is not None else None}")
                    import traceback
                    traceback.print_exc()
                    break
        
        return generated
    
    @classmethod
    def from_pretrained(cls, model_path: str, device: str = "auto"):
        """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        model_path = Path(model_path)
        
        # ì„¤ì • íŒŒì¼ ë¡œë“œ
        config_file = model_path / "config.json"
        if config_file.exists():
            config = TrainingCompatibleConfig.from_json(str(config_file))
        else:
            print("âš ï¸ config.jsonì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            config = TrainingCompatibleConfig()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        model = cls(config)
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model_file = model_path / "pytorch_model.bin"
        if model_file.exists():
            print(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
            print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
            
            checkpoint = torch.load(model_file, map_location=device)
            
            # ì²´í¬í¬ì¸íŠ¸ê°€ ë˜í•‘ëœ ê²½ìš° (ì˜ˆ: DDP)
            if "model_state_dict" in checkpoint:
                state_dict = checkpoint["model_state_dict"]
            else:
                state_dict = checkpoint
            
            # í‚¤ ì •ë¦¬: _orig_mod. ì ‘ë‘ì‚¬ ì œê±° (torch.compile ì‚¬ìš©ì‹œ)
            cleaned_state_dict = {}
            for key, value in state_dict.items():
                # _orig_mod. ì ‘ë‘ì‚¬ ì œê±°
                if key.startswith("_orig_mod."):
                    clean_key = key.replace("_orig_mod.", "")
                    cleaned_state_dict[clean_key] = value
                # module. ì ‘ë‘ì‚¬ ì œê±° (DDP ì‚¬ìš©ì‹œ)
                elif key.startswith("module."):
                    clean_key = key.replace("module.", "")
                    cleaned_state_dict[clean_key] = value
                else:
                    cleaned_state_dict[key] = value
            
            print(f"ğŸ”§ í‚¤ ì •ë¦¬ ì™„ë£Œ: {len(state_dict)} -> {len(cleaned_state_dict)}")
            print(f"ì •ë¦¬ëœ í‚¤ ìƒ˜í”Œ: {list(cleaned_state_dict.keys())[:3]}")
            
            model.load_state_dict(cleaned_state_dict, strict=False)
            print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            total_params = sum(p.numel() for p in model.parameters())
            print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ - íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
            
        else:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_file}")
            
        return model.to(device)
    
    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.parameters())
        
        return {
            "model_name": self.config.model_name,
            "model_parameters": total_params,
            "vocab_size": self.config.vocab_size,
            "hidden_size": self.config.hidden_size,
            "num_layers": self.config.num_layers,
            "num_heads": self.config.num_heads,
            "max_position_embeddings": self.config.max_position_embeddings,
            "device": str(next(self.parameters()).device),
        } 
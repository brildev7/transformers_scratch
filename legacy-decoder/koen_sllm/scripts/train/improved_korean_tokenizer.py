#!/usr/bin/env python3
"""
ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
Improved Korean Tokenizer

ê³µë°± ê¸°ì¤€ í† í°í™”ì™€ í•™ìŠµ ë°ì´í„° ê¸°ë°˜ ì–´íœ˜ êµ¬ì¶•ì„ ì§€ì›í•©ë‹ˆë‹¤.
- ë‹¨ì–´ ë‹¨ìœ„ ë§¥ë½ ë³´ì¡´
- í•™ìŠµ ë°ì´í„° ê¸°ë°˜ ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•
- Subword í† í°í™” ì§€ì›
- í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤
"""

import re
import json
import os
from typing import List, Dict, Optional, Tuple, Set
from pathlib import Path
from collections import Counter
import pickle


class ImprovedKoreanTokenizer:
    """ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €"""
    
    def __init__(self, vocab_file: Optional[str] = None, vocab_size: int = 32000):
        """
        Args:
            vocab_file: ì‚¬ì „ êµ¬ì¶•ëœ ì–´íœ˜ íŒŒì¼ ê²½ë¡œ
            vocab_size: ì–´íœ˜ í¬ê¸°
        """
        # íŠ¹ìˆ˜ í† í°
        self.pad_token = "<pad>"
        self.bos_token = "<s>"
        self.eos_token = "</s>"
        self.unk_token = "<unk>"
        
        # íŠ¹ìˆ˜ í† í° ID
        self.pad_token_id = 0
        self.bos_token_id = 1
        self.eos_token_id = 2
        self.unk_token_id = 3
        
        # ì–´íœ˜ í¬ê¸°
        self.vocab_size = vocab_size
        
        # ì–´íœ˜ ë§¤í•‘
        self.word_to_id = {}
        self.id_to_word = {}
        
        # í•œêµ­ì–´ ì „ì²˜ë¦¬ íŒ¨í„´
        self._setup_korean_patterns()
        
        # ì–´íœ˜ ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
        if vocab_file and os.path.exists(vocab_file):
            self._load_vocabulary(vocab_file)
        else:
            self._initialize_basic_vocab()
            
        print(f"ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  â€¢ ì–´íœ˜ í¬ê¸°: {len(self.word_to_id):,} / {self.vocab_size:,}")
        print(f"  â€¢ í† í°í™” ë°©ì‹: ê³µë°± ê¸°ì¤€ + Subword")
        
    def _setup_korean_patterns(self):
        """í•œêµ­ì–´ ì „ì²˜ë¦¬ íŒ¨í„´ ì„¤ì •"""
        
        # í•œêµ­ì–´ ë¬¸ì íŒ¨í„´
        self.korean_pattern = re.compile(r'[ê°€-í£]+')
        
        # ì˜ì–´ íŒ¨í„´  
        self.english_pattern = re.compile(r'[a-zA-Z]+')
        
        # ìˆ«ì íŒ¨í„´
        self.number_pattern = re.compile(r'\d+')
        
        # êµ¬ë‘ì  íŒ¨í„´
        self.punctuation_pattern = re.compile(r'[.!?,:;"\'\(\)\[\]\{\}~\-_+=<>/@#$%^&*`|\\]')
        
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” í•œêµ­ì–´ ì¡°ì‚¬/ì–´ë¯¸ (ë¶„ë¦¬í•˜ì§€ ì•Šê³  ë‹¨ì–´ì™€ í•¨ê»˜ ìœ ì§€)
        self.common_endings = {
            'ì´ë‹¤', 'ì´ì•¼', 'ì—ìš”', 'ì–´ìš”', 'ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'í–ˆë‹¤', 'í•œë‹¤', 'ëœë‹¤',
            'ì´ëŠ”', 'ì—ëŠ”', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œì¨', 'ì—ê²Œ', 'í•œí…Œ', 'ì—ê²Œì„œ'
        }
        
    def _initialize_basic_vocab(self):
        """ê¸°ë³¸ ì–´íœ˜ ì´ˆê¸°í™”"""
        
        # íŠ¹ìˆ˜ í† í°ë¶€í„° í• ë‹¹
        special_tokens = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]
        for i, token in enumerate(special_tokens):
            self.word_to_id[token] = i
            self.id_to_word[i] = token
            
        # ê¸°ë³¸ í•œêµ­ì–´ ì–´íœ˜ (ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ë“¤)
        basic_korean_words = [
            # ì¸ì‚¬/ê¸°ë³¸
            "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•íˆ", "ê°€ì„¸ìš”", "ë°˜ê°‘ìŠµë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì£„ì†¡í•©ë‹ˆë‹¤",
            "ê´œì°®ìŠµë‹ˆë‹¤", "ë„¤", "ì•„ë‹ˆì˜¤", "ì˜ˆ", "ì•„ë‹ˆ", "ë§ìŠµë‹ˆë‹¤", "í‹€ë ¸ìŠµë‹ˆë‹¤",
            
            # ëŒ€ëª…ì‚¬
            "ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ê·¸ë“¤", "ì´ê²ƒ", "ê·¸ê²ƒ", "ì €ê²ƒ", "ì—¬ê¸°", "ê±°ê¸°", "ì €ê¸°",
            
            # ë™ì‚¬ (ê¸°ë³¸í˜•)
            "í•˜ë‹¤", "ë˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "ê°€ë‹¤", "ì˜¤ë‹¤", "ë³´ë‹¤", "ë“£ë‹¤", "ë§í•˜ë‹¤",
            "ì½ë‹¤", "ì“°ë‹¤", "ë¨¹ë‹¤", "ë§ˆì‹œë‹¤", "ìë‹¤", "ì¼ì–´ë‚˜ë‹¤", "ì•‰ë‹¤", "ì„œë‹¤",
            
            # í˜•ìš©ì‚¬ (ê¸°ë³¸í˜•)
            "ì¢‹ë‹¤", "ë‚˜ì˜ë‹¤", "í¬ë‹¤", "ì‘ë‹¤", "ë§ë‹¤", "ì ë‹¤", "ê¸¸ë‹¤", "ì§§ë‹¤",
            "ë†’ë‹¤", "ë‚®ë‹¤", "ë„“ë‹¤", "ì¢ë‹¤", "ê¸°ì˜ë‹¤", "ìŠ¬í”„ë‹¤", "í™”ë‚˜ë‹¤",
            
            # ëª…ì‚¬ - ê¸°ë³¸
            "ì‚¬ëŒ", "ì‹œê°„", "ë‚ ", "ë…„", "ì›”", "ì¼", "ì‹œ", "ë¶„", "ì´ˆ",
            "ì§‘", "í•™êµ", "íšŒì‚¬", "ë³‘ì›", "ê°€ê²Œ", "ì‹ë‹¹", "ì¹´í˜",
            
            # AI/ê¸°ìˆ  ìš©ì–´
            "ëª¨ë¸", "ë°ì´í„°", "í•™ìŠµ", "ì¶”ë¡ ", "í…ìŠ¤íŠ¸", "í† í°", "ë¬¸ì¥", "ë‹¨ì–´",
            "AI", "ì¸ê³µì§€ëŠ¥", "ì»´í“¨í„°", "í”„ë¡œê·¸ë¨", "ì†Œí”„íŠ¸ì›¨ì–´"
        ]
        
        # ê¸°ë³¸ ì–´íœ˜ í• ë‹¹
        current_id = len(special_tokens)
        for word in basic_korean_words:
            if word not in self.word_to_id:
                self.word_to_id[word] = current_id
                self.id_to_word[current_id] = word
                current_id += 1
                
        print(f"  â€¢ ê¸°ë³¸ ì–´íœ˜ í• ë‹¹: {len(basic_korean_words)}ê°œ")
        
    def build_vocabulary_from_data(self, data_paths: List[str], save_path: Optional[str] = None):
        """í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° ì–´íœ˜ êµ¬ì¶•"""
        
        print("ğŸ“š í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° ì–´íœ˜ êµ¬ì¶• ì¤‘...")
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        all_texts = []
        for data_path in data_paths:
            if os.path.exists(data_path):
                if data_path.endswith('.jsonl'):
                    with open(data_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                data = json.loads(line)
                                all_texts.append(data.get('text', ''))
                elif data_path.endswith('.txt'):
                    with open(data_path, 'r', encoding='utf-8') as f:
                        all_texts.extend(f.readlines())
                        
        print(f"  â€¢ ì´ {len(all_texts)}ê°œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘")
        
        # í† í° ë¹ˆë„ ê³„ì‚°
        token_counter = Counter()
        for text in all_texts:
            tokens = self._tokenize_raw(text)
            token_counter.update(tokens)
            
        print(f"  â€¢ ìœ ë‹ˆí¬ í† í° ìˆ˜: {len(token_counter):,}")
        
        # ë¹ˆë„ ìˆœìœ¼ë¡œ ì–´íœ˜ êµ¬ì¶•
        current_id = len(self.word_to_id)
        vocab_limit = self.vocab_size - 100  # ì—¬ìœ  ê³µê°„
        
        for token, freq in token_counter.most_common():
            if current_id >= vocab_limit:
                break
            if token not in self.word_to_id and len(token.strip()) > 0:
                self.word_to_id[token] = current_id
                self.id_to_word[current_id] = token
                current_id += 1
                
        print(f"  â€¢ ìµœì¢… ì–´íœ˜ í¬ê¸°: {len(self.word_to_id):,}")
        
        # ì–´íœ˜ ì €ì¥
        if save_path:
            self.save_vocabulary(save_path)
            
    def _tokenize_raw(self, text: str) -> List[str]:
        """ì›ì‹œ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í•  (ì–´íœ˜ êµ¬ì¶•ìš©)"""
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        text = text.strip()
        if not text:
            return []
            
        # ê³µë°± ê¸°ì¤€ ë¶„í• 
        words = text.split()
        
        tokens = []
        for word in words:
            # êµ¬ë‘ì ì´ ë¶™ì–´ìˆëŠ” ê²½ìš° ë¶„ë¦¬
            word = word.strip()
            if not word:
                continue
                
            # êµ¬ë‘ì  ë¶„ë¦¬
            if self.punctuation_pattern.search(word):
                # ê°„ë‹¨í•œ êµ¬ë‘ì  ë¶„ë¦¬
                parts = re.split(r'([.!?,:;"\'\(\)\[\]\{\}])', word)
                for part in parts:
                    if part.strip():
                        tokens.append(part.strip())
            else:
                tokens.append(word)
                
        return tokens
        
    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í•  (ì¶”ë¡ ìš©)"""
        
        if not text.strip():
            return []
            
        # ê³µë°± ê¸°ì¤€ í† í°í™”
        tokens = self._tokenize_raw(text)
        
        # OOV ì²˜ë¦¬: ì„œë¸Œì›Œë“œ ë¶„í• 
        final_tokens = []
        for token in tokens:
            if token in self.word_to_id:
                final_tokens.append(token)
            else:
                # OOV í† í°ì„ ì„œë¸Œì›Œë“œë¡œ ë¶„í• 
                subwords = self._split_to_subwords(token)
                final_tokens.extend(subwords)
                
        return final_tokens
        
    def _split_to_subwords(self, word: str) -> List[str]:
        """OOV ë‹¨ì–´ë¥¼ ì„œë¸Œì›Œë“œë¡œ ë¶„í• """
        
        # ê¸¸ì´ê°€ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(word) <= 2:
            return [word]
            
        # í•œêµ­ì–´ì¸ ê²½ìš° ìŒì ˆ ë‹¨ìœ„ë¡œ ë¶„í• 
        if self.korean_pattern.match(word):
            # 2-3ìŒì ˆì”© ë¶„í• 
            subwords = []
            i = 0
            while i < len(word):
                if i + 3 <= len(word):
                    subword = word[i:i+3]
                    if subword in self.word_to_id:
                        subwords.append(subword)
                        i += 3
                        continue
                        
                if i + 2 <= len(word):
                    subword = word[i:i+2]
                    if subword in self.word_to_id:
                        subwords.append(subword)
                        i += 2
                        continue
                        
                # ë‹¨ì¼ ìŒì ˆ
                subwords.append(word[i])
                i += 1
                
            return subwords
        else:
            # ì˜ì–´/ê¸°íƒ€ì˜ ê²½ìš° prefix ë°©ì‹
            subwords = []
            remaining = word
            while remaining:
                found = False
                for length in range(min(len(remaining), 6), 0, -1):
                    prefix = remaining[:length]
                    if prefix in self.word_to_id:
                        subwords.append(prefix)
                        remaining = remaining[length:]
                        found = True
                        break
                        
                if not found:
                    # ë‹¨ì¼ ë¬¸ìë¡œ ë¶„í• 
                    subwords.append(remaining[0])
                    remaining = remaining[1:]
                    
            return subwords
            
    def encode(self, text: str, add_special_tokens: bool = True, max_length: Optional[int] = None, return_tensors: Optional[str] = None, **kwargs) -> Dict:
        """í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ì¸ì½”ë”©"""
        
        tokens = self.tokenize(text)
        
        # íŠ¹ìˆ˜ í† í° ì¶”ê°€
        if add_special_tokens:
            tokens = [self.bos_token] + tokens + [self.eos_token]
            
        # í† í° â†’ ID ë³€í™˜
        input_ids = []
        for token in tokens:
            if token in self.word_to_id:
                input_ids.append(self.word_to_id[token])
            else:
                # ìµœí›„ì˜ ìˆ˜ë‹¨: í•´ì‹œ ê¸°ë°˜ ID
                token_id = hash(token) % (self.vocab_size - 1000) + 1000
                input_ids.append(token_id)
                
        # ê¸¸ì´ ì œí•œ
        if max_length and len(input_ids) > max_length:
            input_ids = input_ids[:max_length]
            
        # attention_mask ìƒì„±
        attention_mask = [1] * len(input_ids)
        
        result = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "tokens": tokens
        }
        
        # PyTorch í…ì„œ ë³€í™˜
        if return_tensors == "pt":
            try:
                import torch
                result["input_ids"] = torch.tensor(result["input_ids"], dtype=torch.long).unsqueeze(0)
                result["attention_mask"] = torch.tensor(result["attention_mask"], dtype=torch.long).unsqueeze(0)
            except ImportError:
                pass
                
        return result
        
    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©"""
        
        tokens = []
        for token_id in token_ids:
            if token_id in self.id_to_word:
                token = self.id_to_word[token_id]
                if skip_special_tokens and token in [self.pad_token, self.bos_token, self.eos_token]:
                    continue
                tokens.append(token)
            else:
                if not skip_special_tokens:
                    tokens.append(f"[{token_id}]")
                    
        # ê³µë°±ìœ¼ë¡œ ì—°ê²° (ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ ìƒì„±)
        return " ".join(tokens)
        
    def get_vocab_size(self) -> int:
        """ì–´íœ˜ í¬ê¸° ë°˜í™˜"""
        return len(self.word_to_id)
        
    def save_vocabulary(self, save_path: str):
        """ì–´íœ˜ ì €ì¥"""
        vocab_data = {
            "word_to_id": self.word_to_id,
            "id_to_word": {str(k): v for k, v in self.id_to_word.items()},
            "vocab_size": self.vocab_size,
            "tokenizer_type": "improved_korean"
        }
        
        # JSON ì €ì¥
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
            
        # Pickle ì €ì¥ (ë¹ ë¥¸ ë¡œë”©ìš©)
        pickle_path = save_path.replace('.json', '.pkl')
        with open(pickle_path, 'wb') as f:
            pickle.dump(vocab_data, f)
            
        print(f"ì–´íœ˜ ì €ì¥ ì™„ë£Œ: {save_path}")
        
    def _load_vocabulary(self, vocab_file: str):
        """ì €ì¥ëœ ì–´íœ˜ ë¡œë“œ"""
        
        try:
            # Pickle íŒŒì¼ ìš°ì„  ì‹œë„
            pickle_file = vocab_file.replace('.json', '.pkl')
            if os.path.exists(pickle_file):
                with open(pickle_file, 'rb') as f:
                    vocab_data = pickle.load(f)
            else:
                with open(vocab_file, 'r', encoding='utf-8') as f:
                    vocab_data = json.load(f)
                    
            self.word_to_id = vocab_data["word_to_id"]
            self.id_to_word = {int(k): v for k, v in vocab_data["id_to_word"].items()}
            self.vocab_size = vocab_data.get("vocab_size", 32000)
            
            print(f"ì–´íœ˜ ë¡œë“œ ì™„ë£Œ: {len(self.word_to_id):,}ê°œ")
            
        except Exception as e:
            print(f"âš ï¸ ì–´íœ˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self._initialize_basic_vocab()


# í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ‡°ğŸ‡· ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    tokenizer = ImprovedKoreanTokenizer()
    
    test_sentences = [
        "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤.",
        "ê³µë°± ê¸°ì¤€ìœ¼ë¡œ í† í°í™”í•˜ì—¬ ë§¥ë½ì„ ë³´ì¡´í•©ë‹ˆë‹¤.",
        "AI ëª¨ë¸ì´ í•œêµ­ì–´ë¥¼ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆì–´ìš”!",
        "ë°ì´í„° ê³¼í•™ê³¼ ìì—°ì–´ ì²˜ë¦¬ëŠ” í¥ë¯¸ë¡œìš´ ë¶„ì•¼ì…ë‹ˆë‹¤."
    ]
    
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ {i}: {sentence}")
        
        # í† í¬ë‚˜ì´ì§•
        tokens = tokenizer.tokenize(sentence)
        print(f"  í† í°ë“¤: {tokens}")
        print(f"  í† í° ìˆ˜: {len(tokens)}")
        
        # ì¸ì½”ë”©
        encoded = tokenizer.encode(sentence)
        print(f"  í† í° IDë“¤: {encoded['input_ids'][:10]}...")
        
        # ë””ì½”ë”©
        decoded = tokenizer.decode(encoded['input_ids'])
        print(f"  ë””ì½”ë”©: {decoded}")
        
    print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
    print(f"  â€¢ ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì–´íœ˜: {tokenizer.get_vocab_size():,}")
    print(f"  â€¢ ë§¥ë½ ë³´ì¡´í˜• í† í¬ë‚˜ì´ì§•ìœ¼ë¡œ í•œêµ­ì–´ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€!") 
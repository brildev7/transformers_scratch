#!/usr/bin/env python3
"""
í•œêµ­ì–´ sLLM H100 ë“€ì–¼ GPU í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Korean sLLM Training Script for Dual H100 GPUs

H100 GPU 2ì¥ì„ í™œìš©í•œ ë¶„ì‚° í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤.
í…ŒìŠ¤íŠ¸ ëª¨ë“œ(10ìŠ¤í…)ì™€ ì‹¤ì œ í•™ìŠµ ëª¨ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import json
import logging
import argparse
from pathlib import Path
from typing import Dict, Optional, Tuple, List
from dataclasses import dataclass
import math

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import torch.nn.functional as F

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
script_dir = Path(__file__).parent.absolute()
project_root = script_dir.parent.parent.parent.parent  # legacy-decoder/koen_sllm/scripts/train -> transformers_scratch
sys.path.append(str(project_root))

# í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì¶”ê°€
from korean_tokenizer import KoreanTokenizer
from improved_korean_tokenizer import ImprovedKoreanTokenizer
from gemma3_tokenizer import Gemma3TokenizerWrapper

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • í´ë˜ìŠ¤"""
    # ëª¨ë¸ ì„¤ì •
    model_name: str = "korean-sllm-h100"
    vocab_size: int = 65536
    hidden_size: int = 2048
    num_layers: int = 24
    num_heads: int = 32
    intermediate_size: int = 8192
    max_position_embeddings: int = 4096
    
    # í•™ìŠµ ì„¤ì •
    batch_size: int = 4
    gradient_accumulation_steps: int = 8
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    warmup_ratio: float = 0.1
    max_grad_norm: float = 1.0
    
    # ë°ì´í„° ì„¤ì •
    dataset_path: str = "../../../../datasets"
    max_seq_length: int = 2048
    
    # í† í¬ë‚˜ì´ì € ì„¤ì •
    tokenizer_type: str = "gemma3"  # 'korean', 'improved_korean', 'gemma3'
    
    # H100 ìµœì í™” ì„¤ì •
    use_flash_attention: bool = True
    compile_model: bool = True
    mixed_precision: str = "bf16"  # H100ì€ bf16 ìµœì í™”
    
    # í…ŒìŠ¤íŠ¸/ì‹¤ì œ ëª¨ë“œ
    test_mode: bool = False
    max_steps: int = 10  # í…ŒìŠ¤íŠ¸ ëª¨ë“œìš©
    
    # ì €ì¥ ì„¤ì •
    output_dir: str = "./../../outputs"
    save_steps: int = 500
    eval_steps: int = 100
    logging_steps: int = 10
    
    # ë¶„ì‚° í•™ìŠµ
    world_size: int = 2  # H100 2ì¥

class SimpleTransformerModel(nn.Module):
    """ê°„ë‹¨í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ (í…ŒìŠ¤íŠ¸ìš©)"""
    
    def __init__(self, config: TrainingConfig):
        super().__init__()
        self.config = config
        
        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        
        # Transformer layers
        layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_size,
            nhead=config.num_heads,
            dim_feedforward=config.intermediate_size,
            dropout=0.1,
            activation="gelu",
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(layer, num_layers=config.num_layers)
        
        self.ln_f = nn.LayerNorm(config.hidden_size)
        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        
        # weight tying
        self.head.weight = self.embeddings.weight
        
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
            
    def forward(self, input_ids, attention_mask=None, labels=None):
        batch_size, seq_len = input_ids.shape
        
        # Position ids
        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # Embeddings
        inputs_embeds = self.embeddings(input_ids)
        position_embeds = self.position_embeddings(position_ids)
        hidden_states = inputs_embeds + position_embeds
        
        # Attention mask for transformer
        if attention_mask is not None:
            attention_mask = attention_mask.bool()
            attention_mask = ~attention_mask  # invert for transformer
        
        # Transformer
        hidden_states = self.transformer(hidden_states, src_key_padding_mask=attention_mask)
        hidden_states = self.ln_f(hidden_states)
        
        # Language model head
        logits = self.head(hidden_states)
        
        loss = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
            
        return {"loss": loss, "logits": logits}

class TextDataset(Dataset):
    """í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ì§€ì›)"""
    
    def __init__(self, data_path: str, max_length: int = 2048, tokenizer_type: str = "gemma3"):
        """
        Args:
            data_path: ë°ì´í„° ê²½ë¡œ
            max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            tokenizer_type: í† í¬ë‚˜ì´ì € íƒ€ì… ('korean', 'improved_korean', 'gemma3')
        """
        self.max_length = max_length
        self.data = []
        self.tokenizer_type = tokenizer_type
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self._initialize_tokenizer()
        
        # JSONL íŒŒì¼ ë¡œë“œ
        data_file = Path(data_path) / "mixed_pretraining.jsonl"
        if not data_file.exists():
            logger.warning(f"ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_file}")
            # ë”ë¯¸ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©) - í•œêµ­ì–´ ë¬¸ì¥ë“¤
            korean_samples = [
                "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” í•œêµ­ì–´ ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤.",
                "ë°˜ê°‘ìŠµë‹ˆë‹¤! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
                "AI ëª¨ë¸ì´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ ì´í•´í•©ë‹ˆë‹¤.",
                "ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° í¥ë¯¸ë¡œìš´ ë¶„ì•¼ì…ë‹ˆë‹¤.",
                "ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ ì–¸ì–´ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆì–´ìš”.",
                "í† í¬ë‚˜ì´ì €ê°€ ë¬¸ì¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.",
                "í˜•íƒœì†Œ ë¶„ì„ì€ í•œêµ­ì–´ì— ì¤‘ìš”í•´ìš”.",
                "ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.",
                "í•œêµ­ì–´ëŠ” êµì°©ì–´ íŠ¹ì„±ì„ ê°€ì ¸ìš”.",
                "ì»´í“¨í„°ê°€ ì‚¬ëŒì˜ ì–¸ì–´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.",
                "ê³µë°± ê¸°ì¤€ í† í°í™”ëŠ” ë§¥ë½ì„ ë³´ì¡´í•©ë‹ˆë‹¤.",
                "Gemma3 í† í¬ë‚˜ì´ì €ëŠ” ê²€ì¦ëœ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
                "ë°ì´í„° ê³¼í•™ê³¼ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "ì–¸ì–´ëª¨ë¸ì˜ ì„±ëŠ¥ì€ í† í¬ë‚˜ì´ì €ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤."
            ]
            self.data = []
            for i in range(1000):
                sample_text = korean_samples[i % len(korean_samples)]
                self.data.append({"text": f"{sample_text} ìƒ˜í”Œ {i+1}ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤."})
        else:
            with open(data_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.data.append(json.loads(line))
        
        logger.info(f"í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.data)} ìƒ˜í”Œ (í† í¬ë‚˜ì´ì €: {tokenizer_type})")
        
    def _initialize_tokenizer(self):
        """í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""
        
        if self.tokenizer_type == "korean":
            self.tokenizer = KoreanTokenizer()
            logger.info("ğŸ‡°ğŸ‡· ê¸°ë³¸ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            
        elif self.tokenizer_type == "improved_korean":
            self.tokenizer = ImprovedKoreanTokenizer()
            logger.info("ğŸš€ ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš© (ê³µë°± ê¸°ì¤€)")
            
        elif self.tokenizer_type == "gemma3":
            try:
                self.tokenizer = Gemma3TokenizerWrapper()
                logger.info("ğŸ¤– Gemma3 í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            except Exception as e:
                logger.warning(f"Gemma3 í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
                logger.info("ê¸°ë³¸ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €ë¡œ fallback")
                self.tokenizer = KoreanTokenizer()
                self.tokenizer_type = "korean"
                
        else:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” í† í¬ë‚˜ì´ì € íƒ€ì…: {self.tokenizer_type}")
            logger.info("ê¸°ë³¸ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            self.tokenizer = KoreanTokenizer()
            self.tokenizer_type = "korean"
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        text = self.data[idx]["text"]
        
        # í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©
        try:
            encoded = self.tokenizer.encode(
                text, 
                add_special_tokens=True, 
                max_length=self.max_length
            )
            
            input_ids = encoded["input_ids"]
            attention_mask = encoded["attention_mask"]
            
        except Exception as e:
            # í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì²˜ë¦¬
            logger.warning(f"í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì²˜ë¦¬ë¡œ ì „í™˜")
            
            # ê¸°ë³¸ í† í°í™” (fallback)
            tokens = text.split()[:self.max_length-2]
            input_ids = [1] + [hash(token) % 65535 + 1 for token in tokens] + [2]
            attention_mask = [1] * len(input_ids)
        
        # íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
        if len(input_ids) < self.max_length:
            # íŒ¨ë”©
            pad_length = self.max_length - len(input_ids)
            input_ids.extend([0] * pad_length)
            attention_mask.extend([0] * pad_length)
        else:
            # ìë¥´ê¸°
            input_ids = input_ids[:self.max_length]
            attention_mask = attention_mask[:self.max_length]
            
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(input_ids, dtype=torch.long)  # ì–¸ì–´ ëª¨ë¸ë§ìš©
        }

class H100Trainer:
    """H100 GPU 2ì¥ì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.local_rank = int(os.environ.get("LOCAL_RANK", 0))
        self.world_size = int(os.environ.get("WORLD_SIZE", 1))
        
        # ë¶„ì‚° í•™ìŠµ ì´ˆê¸°í™”
        self._setup_distributed()
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device(f"cuda:{self.local_rank}")
        torch.cuda.set_device(self.device)
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self._setup_model()
        
        # ë°ì´í„° ë¡œë” ì´ˆê¸°í™”
        self._setup_dataloader()
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        self._setup_optimizer()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # í•™ìŠµ ìƒíƒœ
        self.global_step = 0
        self.epoch = 0
        
    def _setup_distributed(self):
        """ë¶„ì‚° í•™ìŠµ ì´ˆê¸°í™”"""
        if self.world_size > 1:
            dist.init_process_group(
                backend="nccl",
                init_method="env://",
                world_size=self.world_size,
                rank=self.local_rank
            )
            logger.info(f"ë¶„ì‚° í•™ìŠµ ì´ˆê¸°í™” ì™„ë£Œ: rank {self.local_rank}/{self.world_size}")
        
    def _setup_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        logger.info("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        self.model = SimpleTransformerModel(self.config).to(self.device)
        
        # H100 ìµœì í™”
        if self.config.compile_model:
            self.model = torch.compile(self.model)
            logger.info("ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ (torch.compile)")
        
        # DDP ì„¤ì •
        if self.world_size > 1:
            self.model = DDP(
                self.model,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=False
            )
            
        # ëª¨ë¸ í¬ê¸° ì¶œë ¥
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logger.info(f"ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        logger.info(f"í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
        
    def _setup_dataloader(self):
        """ë°ì´í„° ë¡œë” ì´ˆê¸°í™”"""
        logger.info("ë°ì´í„° ë¡œë” ì´ˆê¸°í™” ì¤‘...")
        
        dataset = TextDataset(self.config.dataset_path, self.config.max_seq_length, self.config.tokenizer_type)
        
        # ë¶„ì‚° ìƒ˜í”ŒëŸ¬
        sampler = None
        if self.world_size > 1:
            sampler = DistributedSampler(
                dataset,
                num_replicas=self.world_size,
                rank=self.local_rank,
                shuffle=True
            )
        
        self.dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            sampler=sampler,
            shuffle=(sampler is None),
            num_workers=4,
            pin_memory=True,
            drop_last=True
        )
        
        logger.info(f"ë°ì´í„° ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ: {len(dataset)} ìƒ˜í”Œ, {len(self.dataloader)} ë°°ì¹˜")
        
    def _setup_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
        logger.info("ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì¤‘...")
        
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹
        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.model.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                "weight_decay": self.config.weight_decay,
            },
            {
                "params": [p for n, p in self.model.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        
        self.optimizer = AdamW(
            optimizer_grouped_parameters,
            lr=self.config.learning_rate,
            betas=(0.9, 0.95),
            eps=1e-8
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        if self.config.test_mode:
            total_steps = self.config.max_steps
        else:
            total_steps = len(self.dataloader) * 3  # 3 ì—í¬í¬ ê¸°ë³¸ê°’
            
        warmup_steps = int(total_steps * self.config.warmup_ratio)
        
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=total_steps - warmup_steps,
            eta_min=self.config.learning_rate * 0.1
        )
        
        # Mixed precision scaler
        self.scaler = torch.cuda.amp.GradScaler() if self.config.mixed_precision == "fp16" else None
        
        logger.info(f"ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ì™„ë£Œ: total_steps={total_steps}, warmup_steps={warmup_steps}")
        
    def train(self):
        """í•™ìŠµ ì‹¤í–‰"""
        logger.info("í•™ìŠµ ì‹œì‘!")
        logger.info(f"í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {self.config.test_mode}")
        if self.config.test_mode:
            logger.info(f"ìµœëŒ€ ìŠ¤í…: {self.config.max_steps}")
        
        self.model.train()
        start_time = time.time()
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(100):  # ìµœëŒ€ 100 ì—í¬í¬
            if self.world_size > 1:
                self.dataloader.sampler.set_epoch(epoch)
                
            epoch_loss = 0.0
            num_batches = 0
            
            for step, batch in enumerate(self.dataloader):
                # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                batch = {k: v.to(self.device) for k, v in batch.items()}
                
                # Forward pass
                with torch.cuda.amp.autocast(enabled=(self.config.mixed_precision != "fp32")):
                    outputs = self.model(**batch)
                    loss = outputs["loss"]
                    
                    # Gradient accumulation
                    loss = loss / self.config.gradient_accumulation_steps
                
                # Backward pass
                if self.scaler:
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                # Optimizer step
                if (step + 1) % self.config.gradient_accumulation_steps == 0:
                    if self.scaler:
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)
                        self.optimizer.step()
                    
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    self.global_step += 1
                    
                    # ë¡œê¹…
                    if self.global_step % self.config.logging_steps == 0 and self.local_rank == 0:
                        elapsed_time = time.time() - start_time
                        lr = self.scheduler.get_last_lr()[0]
                        
                        logger.info(
                            f"Step {self.global_step:5d} | "
                            f"Loss: {loss.item() * self.config.gradient_accumulation_steps:.4f} | "
                            f"LR: {lr:.2e} | "
                            f"Time: {elapsed_time:.1f}s"
                        )
                    
                    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì¢…ë£Œ ì¡°ê±´
                    if self.config.test_mode and self.global_step >= self.config.max_steps:
                        logger.info("í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì™„ë£Œ!")
                        return
                    
                    # ëª¨ë¸ ì €ì¥
                    if self.global_step % self.config.save_steps == 0 and self.local_rank == 0:
                        self._save_checkpoint()
                        
            self.epoch += 1
            avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0
            
            if self.local_rank == 0:
                logger.info(f"Epoch {self.epoch} ì™„ë£Œ | í‰ê·  Loss: {avg_loss:.4f}")
                
        logger.info("í•™ìŠµ ì™„ë£Œ!")
        
    def _save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        if self.local_rank != 0:
            return
            
        checkpoint_dir = self.output_dir / f"checkpoint-{self.global_step}"
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ìƒíƒœ
        model_state = self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict()
        
        checkpoint = {
            "model_state_dict": model_state,
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.state_dict(),
            "global_step": self.global_step,
            "epoch": self.epoch,
            "config": self.config.__dict__
        }
        
        if self.scaler:
            checkpoint["scaler_state_dict"] = self.scaler.state_dict()
        
        torch.save(checkpoint, checkpoint_dir / "pytorch_model.bin")
        
        # ì„¤ì • ì €ì¥
        with open(checkpoint_dir / "config.json", "w") as f:
            json.dump(self.config.__dict__, f, indent=2)
            
        logger.info(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {checkpoint_dir}")

def main():
    parser = argparse.ArgumentParser(description="í•œêµ­ì–´ sLLM H100 ë“€ì–¼ GPU ì§€ëŠ¥í˜• í•™ìŠµ")
    
    # ëª¨ë“œ ì„¤ì •
    parser.add_argument("--test-mode", action="store_true", help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ìŠ¤í…)")
    parser.add_argument("--max-steps", type=int, default=10, help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ ìµœëŒ€ ìŠ¤í…")
    parser.add_argument("--max-epochs", type=int, default=5, help="ìµœëŒ€ epoch ìˆ˜")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--dataset-path", type=str, default="../../../../datasets", help="ë°ì´í„°ì…‹ ê²½ë¡œ")
    parser.add_argument("--output-dir", type=str, default="./outputs", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--logs-dir", type=str, default="./logs", help="ë¡œê·¸ ë””ë ‰í† ë¦¬")
    parser.add_argument("--validation-split", type=float, default=0.05, help="Validation ë°ì´í„° ë¹„ìœ¨")
    
    # í† í¬ë‚˜ì´ì € ì„¤ì •
    parser.add_argument("--tokenizer-type", type=str, default="korean", 
                       choices=["korean", "improved_korean", "gemma3"],
                       help="í† í¬ë‚˜ì´ì € íƒ€ì… ì„ íƒ")
    
    # Early Stopping ì„¤ì •
    parser.add_argument("--early-stopping", action="store_true", help="Early stopping í™œì„±í™”")
    parser.add_argument("--patience", type=int, default=3, help="Validation loss ì¦ê°€ í—ˆìš© íšŸìˆ˜")
    parser.add_argument("--min-delta", type=float, default=0.001, help="ìµœì†Œ ê°œì„  ì„ê³„ê°’")
    parser.add_argument("--validation-steps", type=int, default=100, help="Validation ì²´í¬ ê°„ê²©")
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--batch-size", type=int, default=4, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning-rate", type=float, default=1e-4, help="í•™ìŠµë¥ ")
    parser.add_argument("--max-seq-length", type=int, default=2048, help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--save-steps", type=int, default=500, help="ì €ì¥ ìŠ¤í… ê°„ê²©")
    
    # H100 ìµœì í™”
    parser.add_argument("--no-compile", action="store_true", help="torch.compile ë¹„í™œì„±í™”")
    parser.add_argument("--mixed-precision", choices=["fp32", "fp16", "bf16"], default="bf16", help="Mixed precision")
    
    # ì§€ëŠ¥í˜• ê¸°ëŠ¥
    parser.add_argument("--no-save-initial", action="store_true", help="ì´ˆê¸° ëª¨ë¸ ì €ì¥ ë¹„í™œì„±í™”")
    parser.add_argument("--smart-monitoring", action="store_true", help="ì§€ëŠ¥í˜• ëª¨ë‹ˆí„°ë§ í™œì„±í™”")
    parser.add_argument("--no-token-tracking", action="store_true", help="í† í° ì¹´ìš´íŒ… ë¹„í™œì„±í™”")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = TrainingConfig(
        test_mode=args.test_mode,
        max_steps=args.max_steps,
        dataset_path=args.dataset_path,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        max_seq_length=args.max_seq_length,
        save_steps=args.save_steps,
        compile_model=not args.no_compile,
        mixed_precision=args.mixed_precision,
        tokenizer_type=args.tokenizer_type
    )
    
    # ì¶”ê°€ëœ ì¸ìˆ˜ë“¤ ì²˜ë¦¬ (ë¡œê¹…)
    if hasattr(args, 'logs_dir'):
        logger.info(f"ë¡œê·¸ ë””ë ‰í† ë¦¬: {args.logs_dir}")
    if hasattr(args, 'early_stopping') and args.early_stopping:
        logger.info(f"Early Stopping í™œì„±í™”: patience={args.patience}, min_delta={args.min_delta}")
    if hasattr(args, 'smart_monitoring') and args.smart_monitoring:
        logger.info("ì§€ëŠ¥í˜• ëª¨ë‹ˆí„°ë§ í™œì„±í™”")
    
    # í† í¬ë‚˜ì´ì € íƒ€ì… ë¡œê¹…
    tokenizer_names = {
        "korean": "ğŸ‡°ğŸ‡· ê¸°ë³¸ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €",
        "improved_korean": "ğŸš€ ê°œì„ ëœ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € (ê³µë°± ê¸°ì¤€)",
        "gemma3": "ğŸ¤– Gemma3 í† í¬ë‚˜ì´ì €"
    }
    logger.info(f"ì„ íƒëœ í† í¬ë‚˜ì´ì €: {tokenizer_names.get(args.tokenizer_type, args.tokenizer_type)}")
    
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if local_rank == 0:
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}ê°œ")
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                logger.info(f"GPU {i}: {gpu_name}")
                
    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œì‘
    trainer = H100Trainer(config)
    trainer.train()

if __name__ == "__main__":
    main() 
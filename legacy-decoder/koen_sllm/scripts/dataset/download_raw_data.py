#!/usr/bin/env python3
"""
í•œêµ­ì–´ sLLM ì›ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ëª¨ë“ˆ
Raw data download module for Korean sLLM

ì´ ëª¨ë“ˆì€ ìˆœìˆ˜í•˜ê²Œ ì›ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.
ì „ì²˜ë¦¬ëŠ” ë³„ë„ ëª¨ë“ˆì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
script_dir = Path(__file__).parent.absolute()
project_root = script_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

from datasets import load_dataset
import requests
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download_raw_data.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RawDataDownloader:
    """ì›ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "raw_datasets"):
        """
        ì´ˆê¸°í™”
        
        Args:
            output_dir: ì›ì‹œ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë‹¤ìš´ë¡œë“œ ë©”íƒ€ë°ì´í„° ì €ì¥
        self.metadata = {
            "download_time": datetime.now().isoformat(),
            "datasets": {}
        }
    
    def download_korean_datasets(self, small_sample: bool = False) -> Dict[str, str]:
        """
        í•œêµ­ì–´ ì›ì‹œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
        
        Args:
            small_sample: í…ŒìŠ¤íŠ¸ìš© ì†ŒëŸ‰ ìƒ˜í”Œë§Œ ë‹¤ìš´ë¡œë“œí• ì§€ ì—¬ë¶€
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        logger.info("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì›ì‹œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        
        korean_sources = [
            {
                "name": "kowiki",
                "dataset_id": "wikipedia",
                "config": "20231101.ko",
                "split": "train",
                "description": "í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„"
            },
            {
                "name": "korean_news",
                "dataset_id": "klue",
                "config": "ynat",
                "split": "train", 
                "description": "í•œêµ­ì–´ ë‰´ìŠ¤ ë¶„ë¥˜ ë°ì´í„°"
            },
            {
                "name": "korean_petitions",
                "dataset_id": "heegyu/petitions",
                "config": None,
                "split": "train",
                "description": "ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë°ì´í„°"
            },
            {
                "name": "korean_chat",
                "dataset_id": "heegyu/korquad-chat-v1",
                "config": None,
                "split": "train",
                "description": "í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°"
            },
            {
                "name": "korean_common_crawl",
                "dataset_id": "oscar-corpus/OSCAR-2301",
                "config": "ko",
                "split": "train",
                "description": "í•œêµ­ì–´ Common Crawl ë°ì´í„°"
            }
        ]
        
        downloaded_files = {}
        
        for source in korean_sources:
            try:
                logger.info(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {source['description']}")
                
                # ë°ì´í„°ì…‹ ë¡œë“œ
                if source['config']:
                    dataset = load_dataset(
                        source['dataset_id'], 
                        source['config'], 
                        split=source['split'],
                        streaming=True if not small_sample else False
                    )
                else:
                    dataset = load_dataset(
                        source['dataset_id'], 
                        split=source['split'],
                        streaming=True if not small_sample else False
                    )
                
                # ìƒ˜í”Œë§ (í…ŒìŠ¤íŠ¸ìš©)
                if small_sample:
                    if hasattr(dataset, 'select'):
                        dataset = dataset.select(range(min(1000, len(dataset))))
                    else:
                        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ì˜ ê²½ìš°
                        dataset = dataset.take(1000)
                
                # ì›ì‹œ ë°ì´í„° ì €ì¥
                output_file = self.output_dir / f"korean_raw_{source['name']}.jsonl"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    for i, item in enumerate(tqdm(dataset, desc=f"ì €ì¥ ì¤‘: {source['name']}")):
                        json.dump(item, f, ensure_ascii=False)
                        f.write('\n')
                        
                        # ì†ŒëŸ‰ ìƒ˜í”Œì˜ ê²½ìš° ì œí•œ
                        if small_sample and i >= 999:
                            break
                
                downloaded_files[source['name']] = str(output_file)
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                self.metadata["datasets"][source['name']] = {
                    "source": source,
                    "file_path": str(output_file),
                    "file_size": output_file.stat().st_size,
                    "small_sample": small_sample
                }
                
                logger.info(f"âœ… {source['name']} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_file}")
                
            except Exception as e:
                logger.error(f"âŒ {source['name']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
        
        return downloaded_files
    
    def download_english_datasets(self, small_sample: bool = False) -> Dict[str, str]:
        """
        ì˜ì–´ ì›ì‹œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
        
        Args:
            small_sample: í…ŒìŠ¤íŠ¸ìš© ì†ŒëŸ‰ ìƒ˜í”Œë§Œ ë‹¤ìš´ë¡œë“œí• ì§€ ì—¬ë¶€
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        logger.info("ğŸ‡ºğŸ‡¸ ì˜ì–´ ì›ì‹œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        
        english_sources = [
            {
                "name": "enwiki",
                "dataset_id": "wikipedia",
                "config": "20231101.en", 
                "split": "train",
                "description": "ì˜ì–´ ìœ„í‚¤í”¼ë””ì•„"
            },
            {
                "name": "openwebtext",
                "dataset_id": "openwebtext",
                "config": None,
                "split": "train",
                "description": "OpenWebText ë°ì´í„°"
            },
            {
                "name": "c4_en",
                "dataset_id": "c4",
                "config": "en",
                "split": "train",
                "description": "C4 ì˜ì–´ ë°ì´í„°"
            },
            {
                "name": "pile_subset",
                "dataset_id": "EleutherAI/pile",
                "config": None,
                "split": "train",
                "description": "The Pile ë°ì´í„°ì…‹"
            },
            {
                "name": "reddit_comments",
                "dataset_id": "reddit",
                "config": None,
                "split": "train", 
                "description": "Reddit ëŒ“ê¸€ ë°ì´í„°"
            }
        ]
        
        downloaded_files = {}
        
        for source in english_sources:
            try:
                logger.info(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {source['description']}")
                
                # ë°ì´í„°ì…‹ ë¡œë“œ
                if source['config']:
                    dataset = load_dataset(
                        source['dataset_id'],
                        source['config'],
                        split=source['split'],
                        streaming=True if not small_sample else False
                    )
                else:
                    dataset = load_dataset(
                        source['dataset_id'],
                        split=source['split'], 
                        streaming=True if not small_sample else False
                    )
                
                # ìƒ˜í”Œë§ (í…ŒìŠ¤íŠ¸ìš©)
                if small_sample:
                    if hasattr(dataset, 'select'):
                        dataset = dataset.select(range(min(1000, len(dataset))))
                    else:
                        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ì˜ ê²½ìš°
                        dataset = dataset.take(1000)
                
                # ì›ì‹œ ë°ì´í„° ì €ì¥
                output_file = self.output_dir / f"english_raw_{source['name']}.jsonl"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    for i, item in enumerate(tqdm(dataset, desc=f"ì €ì¥ ì¤‘: {source['name']}")):
                        json.dump(item, f, ensure_ascii=False)
                        f.write('\n')
                        
                        # ì†ŒëŸ‰ ìƒ˜í”Œì˜ ê²½ìš° ì œí•œ
                        if small_sample and i >= 999:
                            break
                
                downloaded_files[source['name']] = str(output_file)
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                self.metadata["datasets"][source['name']] = {
                    "source": source,
                    "file_path": str(output_file),
                    "file_size": output_file.stat().st_size,
                    "small_sample": small_sample
                }
                
                logger.info(f"âœ… {source['name']} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_file}")
                
            except Exception as e:
                logger.error(f"âŒ {source['name']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
        
        return downloaded_files
    
    def download_instruction_datasets(self, small_sample: bool = False) -> Dict[str, str]:
        """
        ë¯¸ì„¸ì¡°ì •ìš© ëª…ë ¹ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
        
        Args:
            small_sample: í…ŒìŠ¤íŠ¸ìš© ì†ŒëŸ‰ ìƒ˜í”Œë§Œ ë‹¤ìš´ë¡œë“œí• ì§€ ì—¬ë¶€
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œë“¤
        """
        logger.info("ğŸ¯ ë¯¸ì„¸ì¡°ì •ìš© ëª…ë ¹ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        
        instruction_sources = [
            # í•œêµ­ì–´ ëª…ë ¹ì–´ ë°ì´í„°
            {
                "name": "korean_alpaca",
                "dataset_id": "beomi/KoAlpaca-v1.1a",
                "config": None,
                "split": "train",
                "description": "í•œêµ­ì–´ Alpaca ë°ì´í„°",
                "language": "ko"
            },
            {
                "name": "korean_chatgpt",
                "dataset_id": "FreedomIntelligence/evol-instruct-korean",
                "config": None,
                "split": "train", 
                "description": "í•œêµ­ì–´ ChatGPT ìŠ¤íƒ€ì¼ ë°ì´í„°",
                "language": "ko"
            },
            {
                "name": "korean_sharegpt",
                "dataset_id": "maywell/ko_Ultrachat_200k",
                "config": None,
                "split": "train",
                "description": "í•œêµ­ì–´ ShareGPT ë°ì´í„°",
                "language": "ko"
            },
            # ì˜ì–´ ëª…ë ¹ì–´ ë°ì´í„°
            {
                "name": "alpaca_english",
                "dataset_id": "tatsu-lab/alpaca",
                "config": None,
                "split": "train",
                "description": "ì˜ì–´ Alpaca ë°ì´í„°",
                "language": "en"
            },
            {
                "name": "dolly_english", 
                "dataset_id": "databricks/databricks-dolly-15k",
                "config": None,
                "split": "train",
                "description": "Dolly 15k ë°ì´í„°",
                "language": "en"
            },
            {
                "name": "oasst1_english",
                "dataset_id": "OpenAssistant/oasst1",
                "config": None,
                "split": "train",
                "description": "OpenAssistant ë°ì´í„°",
                "language": "en"
            },
            {
                "name": "ultrachat_english",
                "dataset_id": "stingning/ultrachat",
                "config": None,
                "split": "train",
                "description": "UltraChat ë°ì´í„°",
                "language": "en"
            }
        ]
        
        downloaded_files = {}
        
        for source in instruction_sources:
            try:
                logger.info(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {source['description']}")
                
                # ë°ì´í„°ì…‹ ë¡œë“œ
                if source['config']:
                    dataset = load_dataset(
                        source['dataset_id'],
                        source['config'],
                        split=source['split'],
                        streaming=True if not small_sample else False
                    )
                else:
                    dataset = load_dataset(
                        source['dataset_id'],
                        split=source['split'],
                        streaming=True if not small_sample else False
                    )
                
                # ìƒ˜í”Œë§ (í…ŒìŠ¤íŠ¸ìš©)
                if small_sample:
                    if hasattr(dataset, 'select'):
                        dataset = dataset.select(range(min(100, len(dataset))))
                    else:
                        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ì˜ ê²½ìš°
                        dataset = dataset.take(100)
                
                # ì›ì‹œ ë°ì´í„° ì €ì¥
                output_file = self.output_dir / f"instruction_raw_{source['name']}.jsonl"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    for i, item in enumerate(tqdm(dataset, desc=f"ì €ì¥ ì¤‘: {source['name']}")):
                        # ì–¸ì–´ íƒœê·¸ ì¶”ê°€
                        item['language'] = source['language']
                        json.dump(item, f, ensure_ascii=False)
                        f.write('\n')
                        
                        # ì†ŒëŸ‰ ìƒ˜í”Œì˜ ê²½ìš° ì œí•œ
                        if small_sample and i >= 99:
                            break
                
                downloaded_files[source['name']] = str(output_file)
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                self.metadata["datasets"][source['name']] = {
                    "source": source,
                    "file_path": str(output_file),
                    "file_size": output_file.stat().st_size,
                    "small_sample": small_sample,
                    "language": source['language']
                }
                
                logger.info(f"âœ… {source['name']} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_file}")
                
            except Exception as e:
                logger.error(f"âŒ {source['name']} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
        
        return downloaded_files
    
    def save_metadata(self):
        """ë‹¤ìš´ë¡œë“œ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata_file = self.output_dir / "download_metadata.json"
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_file}")
    
    def get_download_summary(self) -> str:
        """ë‹¤ìš´ë¡œë“œ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        total_files = len(self.metadata["datasets"])
        total_size = sum(
            dataset_info["file_size"] 
            for dataset_info in self.metadata["datasets"].values()
        )
        
        summary = f"""
ğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ìš”ì•½
==================
ì´ íŒŒì¼ ìˆ˜: {total_files}ê°œ
ì´ ìš©ëŸ‰: {total_size / (1024**3):.2f} GB
ì €ì¥ ìœ„ì¹˜: {self.output_dir}

íŒŒì¼ ëª©ë¡:
"""
        
        for name, info in self.metadata["datasets"].items():
            file_size_mb = info["file_size"] / (1024**2)
            summary += f"  - {name}: {file_size_mb:.1f} MB\n"
        
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í•œêµ­ì–´ sLLM ì›ì‹œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    
    parser.add_argument(
        "--output-dir", 
        default="raw_datasets",
        help="ì›ì‹œ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: raw_datasets)"
    )
    parser.add_argument(
        "--korean", 
        action="store_true",
        help="í•œêµ­ì–´ ë°ì´í„°ì…‹ë§Œ ë‹¤ìš´ë¡œë“œ"
    )
    parser.add_argument(
        "--english", 
        action="store_true",
        help="ì˜ì–´ ë°ì´í„°ì…‹ë§Œ ë‹¤ìš´ë¡œë“œ"
    )
    parser.add_argument(
        "--instructions", 
        action="store_true",
        help="ëª…ë ¹ì–´ ë°ì´í„°ì…‹ë§Œ ë‹¤ìš´ë¡œë“œ"
    )
    parser.add_argument(
        "--small", 
        action="store_true",
        help="í…ŒìŠ¤íŠ¸ìš© ì†ŒëŸ‰ ìƒ˜í”Œë§Œ ë‹¤ìš´ë¡œë“œ"
    )
    parser.add_argument(
        "--all", 
        action="store_true",
        help="ëª¨ë“  ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ"
    )
    
    args = parser.parse_args()
    
    # ë‹¤ìš´ë¡œë” ì´ˆê¸°í™”
    downloader = RawDataDownloader(output_dir=args.output_dir)
    
    try:
        # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
        if args.all or (not args.korean and not args.english and not args.instructions):
            logger.info("ğŸš€ ëª¨ë“  ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
            downloader.download_korean_datasets(small_sample=args.small)
            downloader.download_english_datasets(small_sample=args.small)
            downloader.download_instruction_datasets(small_sample=args.small)
        else:
            if args.korean:
                downloader.download_korean_datasets(small_sample=args.small)
            if args.english:
                downloader.download_english_datasets(small_sample=args.small)
            if args.instructions:
                downloader.download_instruction_datasets(small_sample=args.small)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        downloader.save_metadata()
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print(downloader.get_download_summary())
        
        logger.info("âœ… ëª¨ë“  ë‹¤ìš´ë¡œë“œ ì‘ì—… ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 
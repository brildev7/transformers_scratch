#!/usr/bin/env python3
"""
Finetuning Dataset Download Script
íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (configs/dataset/finetuning.json ê¸°ë°˜)
"""
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FinetuningDatasetDownloader:
    """íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë” (configs/dataset ê¸°ë°˜)"""
    
    def __init__(self, config_path: str = None):
        if config_path is None:
            config_path = script_dir.parent.parent / "configs" / "dataset" / "finetuning.json"
        
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê³„ì‚°
        self.project_root = script_dir.parent.parent.parent.parent
        self.output_dir = self.project_root / "datasets"
        
        logger.info(f"ğŸ“‹ ì„¤ì • íŒŒì¼: {self.config_path}")
        logger.info(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        
    def _load_config(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _resolve_dataset_path(self, relative_path: str) -> Path:
        """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
        if relative_path.startswith("/"):
            return Path(relative_path)
        
        # configs/dataset/ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ í•´ì„
        base_dir = self.config_path.parent
        return (base_dir / relative_path).resolve()
    
    def get_dataset_info(self):
        """ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ"""
        logger.info(f"ğŸ“Š íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ì„¤ì •:")
        logger.info(f"   ì„¤ëª…: {self.config.get('description', 'N/A')}")
        logger.info(f"   íƒœìŠ¤í¬ ìœ í˜•: {self.config.get('task_type', 'N/A')}")
        
        datasets = self.config.get('datasets', [])
        if datasets:
            logger.info(f"\nğŸ“š ë°ì´í„°ì…‹ ëª©ë¡:")
            for i, ds in enumerate(datasets, 1):
                name = ds.get('name', 'Unknown')
                format_type = ds.get('format', 'Unknown')
                weight = ds.get('weight', 1.0)
                path = ds.get('path', 'N/A')
                logger.info(f"   {i}. {name}")
                logger.info(f"      ğŸ“ ê²½ë¡œ: {path}")
                logger.info(f"      ğŸ¯ í˜•ì‹: {format_type}")
                logger.info(f"      âš–ï¸  ê°€ì¤‘ì¹˜: {weight}")
        
        # ì „ì²˜ë¦¬ ì„¤ì •
        preprocessing = self.config.get('preprocessing', {})
        if preprocessing:
            logger.info(f"\nğŸ”§ ì „ì²˜ë¦¬ ì„¤ì •:")
            max_len = preprocessing.get('max_sequence_length', 'N/A')
            template = preprocessing.get('prompt_template', 'N/A')
            logger.info(f"   ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {max_len}")
            logger.info(f"   í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: {template[:50]}..." if len(str(template)) > 50 else f"   í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: {template}")
    
    def validate_datasets(self) -> bool:
        """ë°ì´í„°ì…‹ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        logger.info("ğŸ” ë°ì´í„°ì…‹ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬...")
        
        datasets = self.config.get('datasets', [])
        all_valid = True
        
        for ds in datasets:
            name = ds.get('name', 'Unknown')
            path = ds.get('path', '')
            
            # ê²½ë¡œ í•´ì„
            if path.startswith('../../../../datasets/'):
                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ datasets ë””ë ‰í† ë¦¬ ì°¸ì¡°
                dataset_path = self.project_root / "datasets" / Path(path).name
            else:
                dataset_path = self._resolve_dataset_path(path)
            
            if dataset_path.exists():
                file_size = dataset_path.stat().st_size / (1024 * 1024)  # MB
                logger.info(f"   âœ… {name}: {dataset_path} ({file_size:.1f} MB)")
            else:
                logger.warning(f"   âŒ {name}: {dataset_path} (íŒŒì¼ ì—†ìŒ)")
                all_valid = False
        
        return all_valid
    
    def create_sample_datasets(self):
        """ìƒ˜í”Œ íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„±"""
        logger.info("ğŸ“ ìƒ˜í”Œ íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # í•œêµ­ì–´ ëª…ë ¹ì–´ ë°ì´í„° ìƒ˜í”Œ
        korean_instructions = [
            {
                "instruction": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.",
                "input": "ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ì—¬ í•™ìŠµ, ì¶”ë¡ , íŒë‹¨ ë“±ì˜ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ìµœê·¼ ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ë°œì „ìœ¼ë¡œ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "output": "ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•œ ê¸°ìˆ ë¡œ, ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ë°œì „ì— ë”°ë¼ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤."
            },
            {
                "instruction": "íŒŒì´ì¬ìœ¼ë¡œ ê°„ë‹¨í•œ ê³„ì‚°ê¸° í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.",
                "input": "ë§ì…ˆ, ëº„ì…ˆ, ê³±ì…ˆ, ë‚˜ëˆ—ì…ˆì´ ê°€ëŠ¥í•œ ê³„ì‚°ê¸°",
                "output": "```python\ndef calculator(a, b, operation):\n    if operation == '+':\n        return a + b\n    elif operation == '-':\n        return a - b\n    elif operation == '*':\n        return a * b\n    elif operation == '/':\n        return a / b if b != 0 else '0ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'\n    else:\n        return 'ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—°ì‚°ì…ë‹ˆë‹¤'\n```"
            },
            {
                "instruction": "ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”.",
                "input": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
                "output": "The weather is really nice today."
            }
        ]
        
        # ì˜ì–´ ëª…ë ¹ì–´ ë°ì´í„° ìƒ˜í”Œ
        english_instructions = [
            {
                "instruction": "Explain what machine learning is in simple terms.",
                "input": "",
                "output": "Machine learning is a type of artificial intelligence that allows computers to learn and make decisions from data without being explicitly programmed for every task."
            },
            {
                "instruction": "Write a Python function to check if a number is prime.",
                "input": "",
                "output": "```python\ndef is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n```"
            },
            {
                "instruction": "Translate the following sentence to Korean.",
                "input": "I love learning new languages.",
                "output": "ì €ëŠ” ìƒˆë¡œìš´ ì–¸ì–´ë¥¼ ë°°ìš°ëŠ” ê²ƒì„ ì¢‹ì•„í•©ë‹ˆë‹¤."
            }
        ]
        
        # íŒŒì¼ ì €ì¥
        korean_file = self.output_dir / "korean_instructions.json"
        english_file = self.output_dir / "english_instructions.json"
        
        with open(korean_file, 'w', encoding='utf-8') as f:
            json.dump(korean_instructions, f, ensure_ascii=False, indent=2)
        
        with open(english_file, 'w', encoding='utf-8') as f:
            json.dump(english_instructions, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… í•œêµ­ì–´ ëª…ë ¹ì–´ ë°ì´í„°: {korean_file} ({len(korean_instructions)}ê°œ)")
        logger.info(f"âœ… ì˜ì–´ ëª…ë ¹ì–´ ë°ì´í„°: {english_file} ({len(english_instructions)}ê°œ)")
        
        return korean_file, english_file
    
    def process_datasets(self):
        """íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        logger.info("ğŸ”„ íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì¤‘...")
        
        datasets = self.config.get('datasets', [])
        preprocessing = self.config.get('preprocessing', {})
        
        all_data = []
        
        for ds in datasets:
            name = ds.get('name', 'Unknown')
            path = ds.get('path', '')
            format_type = ds.get('format', 'alpaca')
            weight = ds.get('weight', 1.0)
            
            # ê²½ë¡œ í•´ì„
            if path.startswith('../../../../datasets/'):
                dataset_path = self.project_root / "datasets" / Path(path).name
            else:
                dataset_path = self._resolve_dataset_path(path)
            
            if not dataset_path.exists():
                logger.warning(f"âš ï¸  {name} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
                continue
            
            try:
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ê°€ì¤‘ì¹˜ ì ìš© (ë°ì´í„° ë³µì œ)
                weighted_data = data * int(weight)
                all_data.extend(weighted_data)
                
                logger.info(f"âœ… {name}: {len(data)}ê°œ â†’ {len(weighted_data)}ê°œ (ê°€ì¤‘ì¹˜ {weight})")
                
            except Exception as e:
                logger.error(f"âŒ {name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        if all_data:
            output_file = self.output_dir / "processed_finetuning_data.json"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ¯ ì²˜ë¦¬ëœ íŒŒì¸íŠœë‹ ë°ì´í„°: {output_file}")
            logger.info(f"   ì´ ì˜ˆì‹œ ìˆ˜: {len(all_data):,}ê°œ")
            
            return output_file
        
        return None


def main():
    parser = argparse.ArgumentParser(description="íŒŒì¸íŠœë‹ìš© ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (configs/dataset ê¸°ë°˜)")
    parser.add_argument("--config", type=str, help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: configs/dataset/finetuning.json)")
    parser.add_argument("--info", action="store_true", help="ë°ì´í„°ì…‹ ì •ë³´ë§Œ í‘œì‹œ")
    parser.add_argument("--validate", action="store_true", help="ë°ì´í„°ì…‹ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬")
    parser.add_argument("--create_samples", action="store_true", help="ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±")
    parser.add_argument("--process", action="store_true", help="ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ë³‘í•©")
    
    args = parser.parse_args()
    
    try:
        downloader = FinetuningDatasetDownloader(args.config)
        
        if args.info:
            downloader.get_dataset_info()
            return 0
        
        if args.validate:
            is_valid = downloader.validate_datasets()
            if is_valid:
                logger.info("âœ… ëª¨ë“  ë°ì´í„°ì…‹ íŒŒì¼ì´ ìœ íš¨í•©ë‹ˆë‹¤.")
            else:
                logger.warning("âš ï¸  ì¼ë¶€ ë°ì´í„°ì…‹ íŒŒì¼ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            return 0 if is_valid else 1
        
        if args.create_samples:
            korean_file, english_file = downloader.create_sample_datasets()
            logger.info(f"\nğŸ‰ ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
            logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {downloader.output_dir}")
            return 0
        
        if args.process:
            output_file = downloader.process_datasets()
            if output_file:
                logger.info(f"\nğŸ‰ íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ë£Œ!")
                logger.info(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
            else:
                logger.warning("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        # ê¸°ë³¸ ë™ì‘: ì •ë³´ í‘œì‹œ
        downloader.get_dataset_info()
        return 0
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1


if __name__ == "__main__":
    exit(main()) 
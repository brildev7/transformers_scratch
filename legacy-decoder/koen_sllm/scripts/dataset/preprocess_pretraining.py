#!/usr/bin/env python3
"""
í•œêµ­ì–´ sLLM ì‚¬ì „í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
Pretraining data preprocessing module for Korean sLLM

ì´ ëª¨ë“ˆì€ ë‹¤ìš´ë¡œë“œëœ ì›ì‹œ ë°ì´í„°ë¥¼ ì‚¬ì „í•™ìŠµì— ì í•©í•˜ë„ë¡ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
í•œì˜ í˜¼í•© ë¹„ìœ¨, í’ˆì§ˆ í•„í„°ë§, ì¤‘ë³µ ì œê±° ë“±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import argparse
import json
import re
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging
from datetime import datetime
import random
from collections import defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
script_dir = Path(__file__).parent.absolute()
project_root = script_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

import pandas as pd
from tqdm import tqdm
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('preprocess_pretraining.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PretrainingDataProcessor:
    """ì‚¬ì „í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, raw_data_dir: str = "raw_datasets", output_dir: str = "datasets"):
        """
        ì´ˆê¸°í™”
        
        Args:
            raw_data_dir: ì›ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬
            output_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.raw_data_dir = Path(raw_data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # í•œì˜ í˜¼í•© ë¹„ìœ¨ ì„¤ì •
        self.language_ratios = {
            "ko": 0.7,  # í•œêµ­ì–´ 70%
            "en": 0.3   # ì˜ì–´ 30%
        }
        
        # í…ìŠ¤íŠ¸ í’ˆì§ˆ í•„í„°ë§ ì„¤ì •
        self.min_text_length = 50      # ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´
        self.max_text_length = 5000    # ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
        self.min_words_korean = 5      # í•œêµ­ì–´ ìµœì†Œ ë‹¨ì–´ ìˆ˜
        self.min_words_english = 8     # ì˜ì–´ ìµœì†Œ ë‹¨ì–´ ìˆ˜
        
        # ì¤‘ë³µ ì œê±°ìš© í•´ì‹œì…‹
        self.seen_hashes = set()
        
        # ì²˜ë¦¬ í†µê³„
        self.stats = {
            "total_processed": 0,
            "korean_texts": 0,
            "english_texts": 0,
            "filtered_out": 0,
            "duplicates_removed": 0
        }
    
    def _extract_text_from_item(self, item: Dict, source_name: str) -> Optional[str]:
        """
        ë°ì´í„° ì•„ì´í…œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            item: ë°ì´í„° ì•„ì´í…œ
            source_name: ë°ì´í„° ì†ŒìŠ¤ ì´ë¦„
            
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        text = None
        
        # ì†ŒìŠ¤ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¡œì§
        if "wiki" in source_name:
            text = item.get("text", "")
        elif "news" in source_name:
            text = item.get("text", "") or item.get("content", "")
        elif "petitions" in source_name:
            title = item.get("title", "")
            content = item.get("content", "")
            text = f"{title}\n{content}" if title and content else (title or content)
        elif "chat" in source_name:
            text = item.get("text", "") or item.get("context", "")
        elif "common_crawl" in source_name or "oscar" in source_name:
            text = item.get("text", "")
        elif "openwebtext" in source_name:
            text = item.get("text", "")
        elif "c4" in source_name:
            text = item.get("text", "")
        elif "pile" in source_name:
            text = item.get("text", "")
        elif "reddit" in source_name:
            text = item.get("body", "") or item.get("text", "")
        else:
            # ì¼ë°˜ì ì¸ í•„ë“œë“¤ ì‹œë„
            text = (item.get("text", "") or 
                   item.get("content", "") or 
                   item.get("body", "") or
                   item.get("article", ""))
        
        return text.strip() if text else None
    
    def _detect_language(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            ì–¸ì–´ ì½”ë“œ ('ko' ë˜ëŠ” 'en')
        """
        # í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return "en"
        
        korean_ratio = korean_chars / total_chars
        
        # í•œê¸€ ë¹„ìœ¨ì´ 20% ì´ìƒì´ë©´ í•œêµ­ì–´ë¡œ íŒë‹¨
        return "ko" if korean_ratio >= 0.2 else "en"
    
    def _is_high_quality_text(self, text: str, language: str) -> bool:
        """
        í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì‚¬
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            language: ì–¸ì–´ ì½”ë“œ
            
        Returns:
            ê³ í’ˆì§ˆ ì—¬ë¶€
        """
        # ê¸¸ì´ ê²€ì‚¬
        if len(text) < self.min_text_length or len(text) > self.max_text_length:
            return False
        
        # ë‹¨ì–´ ìˆ˜ ê²€ì‚¬
        if language == "ko":
            # í•œêµ­ì–´: ê³µë°±ê³¼ êµ¬ë‘ì ìœ¼ë¡œ ë¶„ë¦¬
            words = re.findall(r'[ê°€-í£]+', text)
            if len(words) < self.min_words_korean:
                return False
        else:
            # ì˜ì–´: ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
            words = text.split()
            if len(words) < self.min_words_english:
                return False
        
        # ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬
        if self._has_repetitive_patterns(text):
            return False
        
        # URLì´ ë„ˆë¬´ ë§ì€ ê²½ìš° ì œì™¸
        url_count = len(re.findall(r'http[s]?://\S+', text))
        if url_count > 3:
            return False
        
        # ìˆ«ìê°€ ë„ˆë¬´ ë§ì€ ê²½ìš° ì œì™¸
        digit_ratio = len(re.findall(r'\d', text)) / len(text)
        if digit_ratio > 0.3:
            return False
        
        return True
    
    def _has_repetitive_patterns(self, text: str) -> bool:
        """
        ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            ë°˜ë³µ íŒ¨í„´ ì¡´ì¬ ì—¬ë¶€
        """
        # ê°™ì€ ë¬¸ì¥ì´ 3ë²ˆ ì´ìƒ ë°˜ë³µë˜ëŠ” ê²½ìš°
        sentences = re.split(r'[.!?]\s+', text)
        if len(sentences) >= 3:
            for sentence in sentences:
                if len(sentence) > 10 and text.count(sentence) >= 3:
                    return True
        
        # ê°™ì€ ë‹¨ì–´ê°€ ì—°ì†ìœ¼ë¡œ 5ë²ˆ ì´ìƒ ë‚˜ì˜¤ëŠ” ê²½ìš°
        words = text.split()
        for i in range(len(words) - 4):
            if all(words[i] == words[i+j] for j in range(5)):
                return True
        
        return False
    
    def _get_text_hash(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì œê±°ìš©)
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            MD5 í•´ì‹œ
        """
        # ê³µë°±ê³¼ êµ¬ë‘ì  ì •ê·œí™” í›„ í•´ì‹œ ìƒì„±
        normalized = re.sub(r'\s+', ' ', text.lower().strip())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def _is_duplicate(self, text: str) -> bool:
        """
        ì¤‘ë³µ í…ìŠ¤íŠ¸ ê²€ì‚¬
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            ì¤‘ë³µ ì—¬ë¶€
        """
        text_hash = self._get_text_hash(text)
        if text_hash in self.seen_hashes:
            return True
        
        self.seen_hashes.add(text_hash)
        return False
    
    def load_raw_data(self) -> Dict[str, List[Dict]]:
        """
        ì›ì‹œ ë°ì´í„° ë¡œë“œ
        
        Returns:
            ì–¸ì–´ë³„ë¡œ ê·¸ë£¹í™”ëœ ë°ì´í„°
        """
        logger.info("ğŸ“‚ ì›ì‹œ ë°ì´í„° ë¡œë“œ ì‹œì‘")
        
        grouped_data = {"ko": [], "en": []}
        
        # ì›ì‹œ ë°ì´í„° íŒŒì¼ë“¤ ì°¾ê¸°
        raw_files = list(self.raw_data_dir.glob("*.jsonl"))
        
        if not raw_files:
            logger.warning("âŒ ì›ì‹œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return grouped_data
        
        for file_path in raw_files:
            logger.info(f"ì²˜ë¦¬ ì¤‘: {file_path.name}")
            
            # íŒŒì¼ëª…ì—ì„œ ì†ŒìŠ¤ ì´ë¦„ ì¶”ì¶œ
            source_name = file_path.stem
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(tqdm(f, desc=f"ë¡œë”©: {file_path.name}")):
                        try:
                            item = json.loads(line.strip())
                            
                            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            text = self._extract_text_from_item(item, source_name)
                            if not text:
                                continue
                            
                            # ì–¸ì–´ ê°ì§€
                            language = item.get('language') or self._detect_language(text)
                            
                            # í’ˆì§ˆ ê²€ì‚¬
                            if not self._is_high_quality_text(text, language):
                                self.stats["filtered_out"] += 1
                                continue
                            
                            # ì¤‘ë³µ ê²€ì‚¬
                            if self._is_duplicate(text):
                                self.stats["duplicates_removed"] += 1
                                continue
                            
                            # ë°ì´í„° ì¶”ê°€
                            processed_item = {
                                "text": text,
                                "language": language,
                                "source": source_name,
                                "length": len(text)
                            }
                            
                            grouped_data[language].append(processed_item)
                            self.stats["total_processed"] += 1
                            
                            if language == "ko":
                                self.stats["korean_texts"] += 1
                            else:
                                self.stats["english_texts"] += 1
                                
                        except json.JSONDecodeError:
                            logger.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {file_path.name}:{line_num}")
                            continue
                        except Exception as e:
                            logger.warning(f"ì²˜ë¦¬ ì˜¤ë¥˜: {file_path.name}:{line_num} - {e}")
                            continue
                            
            except Exception as e:
                logger.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path.name} - {e}")
                continue
        
        logger.info(f"âœ… ì›ì‹œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        logger.info(f"í•œêµ­ì–´: {len(grouped_data['ko'])}ê°œ, ì˜ì–´: {len(grouped_data['en'])}ê°œ")
        
        return grouped_data
    
    def create_mixed_dataset(self, grouped_data: Dict[str, List[Dict]], 
                           target_total_size: Optional[int] = None) -> List[Dict]:
        """
        í•œì˜ í˜¼í•© ë°ì´í„°ì…‹ ìƒì„±
        
        Args:
            grouped_data: ì–¸ì–´ë³„ë¡œ ê·¸ë£¹í™”ëœ ë°ì´í„°
            target_total_size: ëª©í‘œ ë°ì´í„° í¬ê¸° (Noneì´ë©´ ì „ì²´ ì‚¬ìš©)
            
        Returns:
            í˜¼í•©ëœ ë°ì´í„°ì…‹
        """
        logger.info("ğŸ”€ í•œì˜ í˜¼í•© ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
        
        korean_data = grouped_data["ko"]
        english_data = grouped_data["en"]
        
        # ëª©í‘œ í¬ê¸° ì„¤ì •
        if target_total_size is None:
            target_total_size = len(korean_data) + len(english_data)
        
        # ì–¸ì–´ë³„ ëª©í‘œ í¬ê¸° ê³„ì‚°
        target_korean = int(target_total_size * self.language_ratios["ko"])
        target_english = int(target_total_size * self.language_ratios["en"])
        
        logger.info(f"ëª©í‘œ í¬ê¸° - í•œêµ­ì–´: {target_korean}, ì˜ì–´: {target_english}")
        
        # ë°ì´í„° ìƒ˜í”Œë§
        if len(korean_data) > target_korean:
            korean_sample = random.sample(korean_data, target_korean)
        else:
            korean_sample = korean_data
            logger.warning(f"í•œêµ­ì–´ ë°ì´í„° ë¶€ì¡±: {len(korean_data)} < {target_korean}")
        
        if len(english_data) > target_english:
            english_sample = random.sample(english_data, target_english)
        else:
            english_sample = english_data
            logger.warning(f"ì˜ì–´ ë°ì´í„° ë¶€ì¡±: {len(english_data)} < {target_english}")
        
        # í˜¼í•© ë° ì…”í”Œ
        mixed_data = korean_sample + english_sample
        random.shuffle(mixed_data)
        
        logger.info(f"âœ… í˜¼í•© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(mixed_data)}ê°œ")
        
        return mixed_data
    
    def save_pretraining_dataset(self, dataset: List[Dict], 
                                dataset_type: str = "mixed") -> str:
        """
        ì‚¬ì „í•™ìŠµ ë°ì´í„°ì…‹ ì €ì¥
        
        Args:
            dataset: ë°ì´í„°ì…‹
            dataset_type: ë°ì´í„°ì…‹ íƒ€ì… (mixed, korean, english)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        output_file = self.output_dir / f"{dataset_type}_pretraining_corpus.json"
        
        logger.info(f"ğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘: {output_file}")
        
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥ (ì‚¬ì „í•™ìŠµìš© í˜•ì‹)
        corpus = []
        for item in tqdm(dataset, desc="ë°ì´í„°ì…‹ ë³€í™˜"):
            corpus.append({
                "text": item["text"],
                "language": item["language"],
                "source": item["source"],
                "length": item["length"]
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(corpus, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
        return str(output_file)
    
    def save_statistics(self):
        """ì²˜ë¦¬ í†µê³„ ì €ì¥"""
        stats_file = self.output_dir / "pretraining_preprocessing_stats.json"
        
        # ì–¸ì–´ë³„ ë¹„ìœ¨ ê³„ì‚°
        if self.stats["total_processed"] > 0:
            korean_ratio = self.stats["korean_texts"] / self.stats["total_processed"]
            english_ratio = self.stats["english_texts"] / self.stats["total_processed"]
        else:
            korean_ratio = english_ratio = 0
        
        detailed_stats = {
            **self.stats,
            "language_ratios": {
                "korean": round(korean_ratio, 3),
                "english": round(english_ratio, 3)
            },
            "processing_time": datetime.now().isoformat(),
            "quality_filters": {
                "min_text_length": self.min_text_length,
                "max_text_length": self.max_text_length,
                "min_words_korean": self.min_words_korean,
                "min_words_english": self.min_words_english
            }
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š í†µê³„ ì €ì¥ ì™„ë£Œ: {stats_file}")
    
    def get_processing_summary(self) -> str:
        """ì²˜ë¦¬ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        korean_ratio = (self.stats["korean_texts"] / self.stats["total_processed"] 
                       if self.stats["total_processed"] > 0 else 0)
        english_ratio = (self.stats["english_texts"] / self.stats["total_processed"] 
                        if self.stats["total_processed"] > 0 else 0)
        
        summary = f"""
ğŸ“Š ì‚¬ì „í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½
==================================
ì´ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸: {self.stats['total_processed']:,}ê°œ
â”œâ”€ í•œêµ­ì–´: {self.stats['korean_texts']:,}ê°œ ({korean_ratio:.1%})
â””â”€ ì˜ì–´: {self.stats['english_texts']:,}ê°œ ({english_ratio:.1%})

í•„í„°ë§ ê²°ê³¼:
â”œâ”€ í’ˆì§ˆ í•„í„°ë¡œ ì œì™¸: {self.stats['filtered_out']:,}ê°œ
â””â”€ ì¤‘ë³µ ì œê±°: {self.stats['duplicates_removed']:,}ê°œ

ëª©í‘œ ì–¸ì–´ ë¹„ìœ¨:
â”œâ”€ í•œêµ­ì–´: {self.language_ratios['ko']:.1%}
â””â”€ ì˜ì–´: {self.language_ratios['en']:.1%}

ì €ì¥ ìœ„ì¹˜: {self.output_dir}
"""
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì‚¬ì „í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬")
    
    parser.add_argument(
        "--raw-data-dir",
        default="raw_datasets", 
        help="ì›ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: raw_datasets)"
    )
    parser.add_argument(
        "--output-dir",
        default="datasets",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: datasets)"
    )
    parser.add_argument(
        "--korean-ratio",
        type=float,
        default=0.7,
        help="í•œêµ­ì–´ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.7)"
    )
    parser.add_argument(
        "--english-ratio", 
        type=float,
        default=0.3,
        help="ì˜ì–´ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.3)"
    )
    parser.add_argument(
        "--target-size",
        type=int,
        help="ëª©í‘œ ë°ì´í„°ì…‹ í¬ê¸° (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--korean-only",
        action="store_true",
        help="í•œêµ­ì–´ ì „ìš© ë°ì´í„°ì…‹ë§Œ ìƒì„±"
    )
    parser.add_argument(
        "--english-only", 
        action="store_true",
        help="ì˜ì–´ ì „ìš© ë°ì´í„°ì…‹ë§Œ ìƒì„±"
    )
    parser.add_argument(
        "--mixed-only",
        action="store_true", 
        help="í˜¼í•© ë°ì´í„°ì…‹ë§Œ ìƒì„±"
    )
    
    args = parser.parse_args()
    
    # ì–¸ì–´ ë¹„ìœ¨ ê²€ì¦
    if abs((args.korean_ratio + args.english_ratio) - 1.0) > 0.01:
        logger.error("âŒ í•œêµ­ì–´ì™€ ì˜ì–´ ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        sys.exit(1)
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = PretrainingDataProcessor(
        raw_data_dir=args.raw_data_dir,
        output_dir=args.output_dir
    )
    
    # ì–¸ì–´ ë¹„ìœ¨ ì„¤ì •
    processor.language_ratios = {
        "ko": args.korean_ratio,
        "en": args.english_ratio
    }
    
    try:
        # ì›ì‹œ ë°ì´í„° ë¡œë“œ
        grouped_data = processor.load_raw_data()
        
        if not grouped_data["ko"] and not grouped_data["en"]:
            logger.error("âŒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        # ë°ì´í„°ì…‹ ìƒì„± ë° ì €ì¥
        if args.korean_only or (not args.english_only and not args.mixed_only):
            if grouped_data["ko"]:
                logger.info("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì „ìš© ë°ì´í„°ì…‹ ìƒì„±")
                processor.save_pretraining_dataset(grouped_data["ko"], "korean")
        
        if args.english_only or (not args.korean_only and not args.mixed_only):
            if grouped_data["en"]:
                logger.info("ğŸ‡ºğŸ‡¸ ì˜ì–´ ì „ìš© ë°ì´í„°ì…‹ ìƒì„±")
                processor.save_pretraining_dataset(grouped_data["en"], "english")
        
        if args.mixed_only or (not args.korean_only and not args.english_only):
            if grouped_data["ko"] and grouped_data["en"]:
                logger.info("ğŸ”€ í˜¼í•© ë°ì´í„°ì…‹ ìƒì„±")
                mixed_dataset = processor.create_mixed_dataset(
                    grouped_data, 
                    target_total_size=args.target_size
                )
                processor.save_pretraining_dataset(mixed_dataset, "mixed")
        
        # í†µê³„ ì €ì¥
        processor.save_statistics()
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print(processor.get_processing_summary())
        
        logger.info("âœ… ì‚¬ì „í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 
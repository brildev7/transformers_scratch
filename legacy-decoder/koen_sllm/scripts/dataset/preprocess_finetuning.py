#!/usr/bin/env python3
"""
í•œêµ­ì–´ sLLM ë¯¸ì„¸ì¡°ì • ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
Finetuning data preprocessing module for Korean sLLM

ì´ ëª¨ë“ˆì€ ë‹¤ìš´ë¡œë“œëœ ì›ì‹œ ëª…ë ¹ì–´ ë°ì´í„°ë¥¼ ë¯¸ì„¸ì¡°ì •ì— ì í•©í•˜ë„ë¡ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
ìµœì†Œ 5ë§Œê°œ ì´ìƒì˜ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìƒì„±í•˜ë©°, í•œì˜ í˜¼í•©ì„ ê³ ë ¤í•©ë‹ˆë‹¤.
"""

import argparse
import json
import re
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import logging
from datetime import datetime
import random
from collections import defaultdict
import copy

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
script_dir = Path(__file__).parent.absolute()
project_root = script_dir.parent.parent.parent.parent
sys.path.append(str(project_root))

from tqdm import tqdm
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('preprocess_finetuning.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FinetuningDataProcessor:
    """ë¯¸ì„¸ì¡°ì • ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, raw_data_dir: str = "raw_datasets", output_dir: str = "datasets"):
        """
        ì´ˆê¸°í™”
        
        Args:
            raw_data_dir: ì›ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬
            output_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.raw_data_dir = Path(raw_data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª©í‘œ ë°ì´í„° ê°œìˆ˜ ì„¤ì •
        self.min_target_count = 50000  # ìµœì†Œ 5ë§Œê°œ
        self.target_count = 100000     # ëª©í‘œ 10ë§Œê°œ
        
        # í•œì˜ í˜¼í•© ë¹„ìœ¨ ì„¤ì •
        self.language_ratios = {
            "ko": 0.6,  # í•œêµ­ì–´ 60%
            "en": 0.4   # ì˜ì–´ 40%
        }
        
        # í’ˆì§ˆ í•„í„°ë§ ì„¤ì •
        self.min_instruction_length = 10
        self.max_instruction_length = 500
        self.min_output_length = 10
        self.max_output_length = 2000
        
        # ì¤‘ë³µ ì œê±°ìš© í•´ì‹œì…‹
        self.seen_hashes = set()
        
        # ì²˜ë¦¬ í†µê³„
        self.stats = {
            "total_processed": 0,
            "korean_instructions": 0,
            "english_instructions": 0,
            "filtered_out": 0,
            "duplicates_removed": 0,
            "augmented_data": 0
        }
        
        # íƒœìŠ¤í¬ ë¶„ë¥˜
        self.task_categories = {
            "qa": "ì§ˆë¬¸ë‹µë³€",
            "summarization": "ìš”ì•½",
            "translation": "ë²ˆì—­",
            "classification": "ë¶„ë¥˜",
            "generation": "ìƒì„±",
            "conversation": "ëŒ€í™”",
            "reasoning": "ì¶”ë¡ ",
            "math": "ìˆ˜í•™",
            "coding": "ì½”ë”©",
            "general": "ì¼ë°˜"
        }
    
    def _extract_instruction_data(self, item: Dict, source_name: str) -> Optional[Dict]:
        """
        ë°ì´í„° ì•„ì´í…œì—ì„œ ëª…ë ¹ì–´ ë°ì´í„° ì¶”ì¶œ
        
        Args:
            item: ë°ì´í„° ì•„ì´í…œ
            source_name: ë°ì´í„° ì†ŒìŠ¤ ì´ë¦„
            
        Returns:
            í‘œì¤€ í˜•ì‹ì˜ ëª…ë ¹ì–´ ë°ì´í„° ë˜ëŠ” None
        """
        instruction = None
        input_text = ""
        output = None
        language = item.get('language', 'ko')
        
        try:
            # ì†ŒìŠ¤ë³„ ë°ì´í„° ì¶”ì¶œ ë¡œì§
            if "alpaca" in source_name:
                instruction = item.get("instruction", "")
                input_text = item.get("input", "")
                output = item.get("output", "")
                
            elif "chatgpt" in source_name or "evol" in source_name:
                if "conversations" in item:
                    conversations = item["conversations"]
                    if len(conversations) >= 2:
                        instruction = conversations[0].get("value", "")
                        output = conversations[1].get("value", "")
                else:
                    instruction = item.get("instruction", "")
                    output = item.get("output", "")
                    
            elif "sharegpt" in source_name or "ultrachat" in source_name:
                if "messages" in item:
                    messages = item["messages"]
                    if len(messages) >= 2:
                        instruction = messages[0].get("content", "")
                        output = messages[1].get("content", "")
                elif "conversations" in item:
                    conversations = item["conversations"]
                    if len(conversations) >= 2:
                        instruction = conversations[0].get("value", "")
                        output = conversations[1].get("value", "")
                        
            elif "dolly" in source_name:
                instruction = item.get("instruction", "")
                context = item.get("context", "")
                if context:
                    input_text = context
                output = item.get("response", "")
                
            elif "oasst" in source_name:
                instruction = item.get("text", "")
                if item.get("role") == "assistant":
                    output = instruction
                    instruction = item.get("parent_text", "")
                else:
                    output = item.get("response", "")
                    
            else:
                # ì¼ë°˜ì ì¸ í•„ë“œë“¤ ì‹œë„
                instruction = (item.get("instruction", "") or 
                             item.get("question", "") or
                             item.get("input", "") or
                             item.get("prompt", ""))
                output = (item.get("output", "") or
                         item.get("answer", "") or
                         item.get("response", "") or
                         item.get("completion", ""))
                input_text = item.get("input", "") or item.get("context", "")
            
            # ë°ì´í„° ê²€ì¦
            if not instruction or not output:
                return None
            
            # í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            return {
                "instruction": instruction.strip(),
                "input": input_text.strip(),
                "output": output.strip(),
                "language": language,
                "source": source_name,
                "task_category": self._classify_task(instruction, output)
            }
            
        except Exception as e:
            logger.warning(f"ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {source_name} - {e}")
            return None
    
    def _classify_task(self, instruction: str, output: str) -> str:
        """
        íƒœìŠ¤í¬ ë¶„ë¥˜
        
        Args:
            instruction: ëª…ë ¹ì–´
            output: ì‘ë‹µ
            
        Returns:
            íƒœìŠ¤í¬ ì¹´í…Œê³ ë¦¬
        """
        instruction_lower = instruction.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        if any(keyword in instruction_lower for keyword in ["what", "who", "where", "when", "why", "how", "ë¬´ì—‡", "ëˆ„êµ¬", "ì–´ë””", "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ"]):
            return "qa"
        elif any(keyword in instruction_lower for keyword in ["summarize", "summary", "ìš”ì•½"]):
            return "summarization"
        elif any(keyword in instruction_lower for keyword in ["translate", "ë²ˆì—­"]):
            return "translation"
        elif any(keyword in instruction_lower for keyword in ["classify", "category", "ë¶„ë¥˜"]):
            return "classification"
        elif any(keyword in instruction_lower for keyword in ["write", "create", "generate", "ì‘ì„±", "ìƒì„±"]):
            return "generation"
        elif any(keyword in instruction_lower for keyword in ["chat", "talk", "conversation", "ëŒ€í™”"]):
            return "conversation"
        elif any(keyword in instruction_lower for keyword in ["solve", "calculate", "math", "í’€ì–´", "ê³„ì‚°"]):
            return "math"
        elif any(keyword in instruction_lower for keyword in ["code", "program", "coding", "ì½”ë“œ", "í”„ë¡œê·¸ë˜ë°"]):
            return "coding"
        elif any(keyword in instruction_lower for keyword in ["reason", "think", "explain", "ì¶”ë¡ ", "ì„¤ëª…"]):
            return "reasoning"
        else:
            return "general"
    
    def _is_high_quality_instruction(self, data: Dict) -> bool:
        """
        ëª…ë ¹ì–´ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
        
        Args:
            data: ëª…ë ¹ì–´ ë°ì´í„°
            
        Returns:
            ê³ í’ˆì§ˆ ì—¬ë¶€
        """
        instruction = data["instruction"]
        output = data["output"]
        
        # ê¸¸ì´ ê²€ì‚¬
        if (len(instruction) < self.min_instruction_length or 
            len(instruction) > self.max_instruction_length or
            len(output) < self.min_output_length or
            len(output) > self.max_output_length):
            return False
        
        # ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´ ê²€ì‚¬
        if self._has_meaningless_patterns(instruction) or self._has_meaningless_patterns(output):
            return False
        
        # ë„ˆë¬´ ë°˜ë³µì ì¸ ì‘ë‹µ ê²€ì‚¬
        if self._is_too_repetitive(output):
            return False
        
        # ë¶ˆì™„ì „í•œ ì‘ë‹µ ê²€ì‚¬
        if self._is_incomplete_response(output):
            return False
        
        return True
    
    def _has_meaningless_patterns(self, text: str) -> bool:
        """ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´ ê²€ì‚¬"""
        # ë„ˆë¬´ ë§ì€ íŠ¹ìˆ˜ë¬¸ì
        special_char_ratio = len(re.findall(r'[^\w\sê°€-í£]', text)) / len(text)
        if special_char_ratio > 0.3:
            return True
        
        # ë„ˆë¬´ ë§ì€ ìˆ«ì
        digit_ratio = len(re.findall(r'\d', text)) / len(text)
        if digit_ratio > 0.5:
            return True
        
        # ì˜ë¯¸ ì—†ëŠ” ë°˜ë³µ
        words = text.split()
        if len(words) > 3:
            unique_words = set(words)
            if len(unique_words) / len(words) < 0.3:
                return True
        
        return False
    
    def _is_too_repetitive(self, text: str) -> bool:
        """ë„ˆë¬´ ë°˜ë³µì ì¸ í…ìŠ¤íŠ¸ ê²€ì‚¬"""
        sentences = re.split(r'[.!?]\s+', text)
        if len(sentences) > 2:
            for sentence in sentences:
                if len(sentence) > 10 and text.count(sentence) > 2:
                    return True
        return False
    
    def _is_incomplete_response(self, text: str) -> bool:
        """ë¶ˆì™„ì „í•œ ì‘ë‹µ ê²€ì‚¬"""
        # ë„ˆë¬´ ì§§ì€ ì‘ë‹µ
        if len(text.split()) < 3:
            return True
        
        # ëì´ ì˜ë¦° ì‘ë‹µ
        if text.endswith(("...", "â€¦", "ë“±ë“±", "ë“±", "ê·¸ë¦¬ê³ ")):
            return True
        
        # ì˜ë¯¸ ì—†ëŠ” ì‘ë‹µ
        meaningless_responses = [
            "ì£„ì†¡í•©ë‹ˆë‹¤", "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤", "í™•ì¸ì´ ì–´ë µìŠµë‹ˆë‹¤",
            "sorry", "i don't know", "i'm not sure"
        ]
        if any(response in text.lower() for response in meaningless_responses):
            if len(text.split()) < 10:  # ì§§ìœ¼ë©´ì„œ ì˜ë¯¸ ì—†ëŠ” ì‘ë‹µ
                return True
        
        return False
    
    def _get_instruction_hash(self, data: Dict) -> str:
        """
        ëª…ë ¹ì–´ ë°ì´í„° í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì œê±°ìš©)
        
        Args:
            data: ëª…ë ¹ì–´ ë°ì´í„°
            
        Returns:
            MD5 í•´ì‹œ
        """
        combined_text = f"{data['instruction']} {data['input']} {data['output']}"
        normalized = re.sub(r'\s+', ' ', combined_text.lower().strip())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def _is_duplicate(self, data: Dict) -> bool:
        """
        ì¤‘ë³µ ë°ì´í„° ê²€ì‚¬
        
        Args:
            data: ëª…ë ¹ì–´ ë°ì´í„°
            
        Returns:
            ì¤‘ë³µ ì—¬ë¶€
        """
        data_hash = self._get_instruction_hash(data)
        if data_hash in self.seen_hashes:
            return True
        
        self.seen_hashes.add(data_hash)
        return False
    
    def load_instruction_data(self) -> Dict[str, List[Dict]]:
        """
        ì›ì‹œ ëª…ë ¹ì–´ ë°ì´í„° ë¡œë“œ
        
        Returns:
            ì–¸ì–´ë³„ë¡œ ê·¸ë£¹í™”ëœ ëª…ë ¹ì–´ ë°ì´í„°
        """
        logger.info("ğŸ“‚ ì›ì‹œ ëª…ë ¹ì–´ ë°ì´í„° ë¡œë“œ ì‹œì‘")
        
        grouped_data = {"ko": [], "en": []}
        
        # ëª…ë ¹ì–´ ë°ì´í„° íŒŒì¼ë“¤ ì°¾ê¸°
        instruction_files = list(self.raw_data_dir.glob("instruction_raw_*.jsonl"))
        
        if not instruction_files:
            logger.warning("âŒ ëª…ë ¹ì–´ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return grouped_data
        
        for file_path in instruction_files:
            logger.info(f"ì²˜ë¦¬ ì¤‘: {file_path.name}")
            
            # íŒŒì¼ëª…ì—ì„œ ì†ŒìŠ¤ ì´ë¦„ ì¶”ì¶œ
            source_name = file_path.stem.replace("instruction_raw_", "")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(tqdm(f, desc=f"ë¡œë”©: {file_path.name}")):
                        try:
                            item = json.loads(line.strip())
                            
                            # ëª…ë ¹ì–´ ë°ì´í„° ì¶”ì¶œ
                            instruction_data = self._extract_instruction_data(item, source_name)
                            if not instruction_data:
                                continue
                            
                            # í’ˆì§ˆ ê²€ì‚¬
                            if not self._is_high_quality_instruction(instruction_data):
                                self.stats["filtered_out"] += 1
                                continue
                            
                            # ì¤‘ë³µ ê²€ì‚¬
                            if self._is_duplicate(instruction_data):
                                self.stats["duplicates_removed"] += 1
                                continue
                            
                            # ì–¸ì–´ë³„ ë¶„ë¥˜
                            language = instruction_data["language"]
                            grouped_data[language].append(instruction_data)
                            self.stats["total_processed"] += 1
                            
                            if language == "ko":
                                self.stats["korean_instructions"] += 1
                            else:
                                self.stats["english_instructions"] += 1
                                
                        except json.JSONDecodeError:
                            logger.warning(f"JSON íŒŒì‹± ì˜¤ë¥˜: {file_path.name}:{line_num}")
                            continue
                        except Exception as e:
                            logger.warning(f"ì²˜ë¦¬ ì˜¤ë¥˜: {file_path.name}:{line_num} - {e}")
                            continue
                            
            except Exception as e:
                logger.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path.name} - {e}")
                continue
        
        logger.info(f"âœ… ì›ì‹œ ëª…ë ¹ì–´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        logger.info(f"í•œêµ­ì–´: {len(grouped_data['ko'])}ê°œ, ì˜ì–´: {len(grouped_data['en'])}ê°œ")
        
        return grouped_data
    
    def augment_data(self, grouped_data: Dict[str, List[Dict]]) -> Dict[str, List[Dict]]:
        """
        ë°ì´í„° ì¦ê°•
        
        Args:
            grouped_data: ì–¸ì–´ë³„ë¡œ ê·¸ë£¹í™”ëœ ë°ì´í„°
            
        Returns:
            ì¦ê°•ëœ ë°ì´í„°
        """
        logger.info("ğŸš€ ë°ì´í„° ì¦ê°• ì‹œì‘")
        
        augmented_data = {"ko": [], "en": []}
        
        # ê¸°ì¡´ ë°ì´í„° ë³µì‚¬
        for lang in ["ko", "en"]:
            augmented_data[lang] = copy.deepcopy(grouped_data[lang])
        
        # ë°ì´í„° ë¶€ì¡±í•œ ì–¸ì–´ì— ëŒ€í•´ ì¦ê°• ìˆ˜í–‰
        korean_count = len(grouped_data["ko"])
        english_count = len(grouped_data["en"])
        
        target_korean = int(self.target_count * self.language_ratios["ko"])
        target_english = int(self.target_count * self.language_ratios["en"])
        
        # í•œêµ­ì–´ ë°ì´í„° ì¦ê°•
        if korean_count < target_korean:
            needed = target_korean - korean_count
            logger.info(f"í•œêµ­ì–´ ë°ì´í„° ì¦ê°• í•„ìš”: {needed}ê°œ")
            
            # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ë³€í˜• ìƒì„±
            korean_augmented = self._create_variations(grouped_data["ko"], needed, "ko")
            augmented_data["ko"].extend(korean_augmented)
            self.stats["augmented_data"] += len(korean_augmented)
        
        # ì˜ì–´ ë°ì´í„° ì¦ê°•
        if english_count < target_english:
            needed = target_english - english_count
            logger.info(f"ì˜ì–´ ë°ì´í„° ì¦ê°• í•„ìš”: {needed}ê°œ")
            
            # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ë³€í˜• ìƒì„±
            english_augmented = self._create_variations(grouped_data["en"], needed, "en")
            augmented_data["en"].extend(english_augmented)
            self.stats["augmented_data"] += len(english_augmented)
        
        logger.info(f"âœ… ë°ì´í„° ì¦ê°• ì™„ë£Œ")
        logger.info(f"í•œêµ­ì–´: {len(augmented_data['ko'])}ê°œ, ì˜ì–´: {len(augmented_data['en'])}ê°œ")
        
        return augmented_data
    
    def _create_variations(self, data: List[Dict], needed_count: int, language: str) -> List[Dict]:
        """
        ë°ì´í„° ë³€í˜• ìƒì„±
        
        Args:
            data: ì›ë³¸ ë°ì´í„°
            needed_count: í•„ìš”í•œ ë°ì´í„° ê°œìˆ˜
            language: ì–¸ì–´ ì½”ë“œ
            
        Returns:
            ë³€í˜•ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        variations = []
        
        if not data or needed_count <= 0:
            return variations
        
        # ë³€í˜• ìƒì„± ë°©ë²•ë“¤
        variation_methods = [
            self._paraphrase_instruction,
            self._add_context_variation,
            self._change_formality,
            self._add_examples
        ]
        
        created = 0
        max_attempts = needed_count * 3  # ë¬´í•œ ë£¨í”„ ë°©ì§€
        attempts = 0
        
        while created < needed_count and attempts < max_attempts:
            # ëœë¤í•˜ê²Œ ì›ë³¸ ë°ì´í„° ì„ íƒ
            original = random.choice(data)
            
            # ëœë¤í•˜ê²Œ ë³€í˜• ë°©ë²• ì„ íƒ
            method = random.choice(variation_methods)
            
            try:
                varied = method(original, language)
                if varied and not self._is_duplicate(varied):
                    variations.append(varied)
                    created += 1
                    
            except Exception as e:
                logger.warning(f"ë³€í˜• ìƒì„± ì˜¤ë¥˜: {e}")
            
            attempts += 1
        
        logger.info(f"{language} ë³€í˜• ë°ì´í„° {len(variations)}ê°œ ìƒì„±")
        return variations
    
    def _paraphrase_instruction(self, data: Dict, language: str) -> Optional[Dict]:
        """ëª…ë ¹ì–´ íŒ¨ëŸ¬í”„ë ˆì´ì§•"""
        varied = copy.deepcopy(data)
        
        instruction = data["instruction"]
        
        if language == "ko":
            # í•œêµ­ì–´ íŒ¨ëŸ¬í”„ë ˆì´ì§• íŒ¨í„´
            patterns = [
                (r"~í•˜ì„¸ìš”", "~í•´ì£¼ì„¸ìš”"),
                (r"~í•˜ì‹­ì‹œì˜¤", "~í•´ì£¼ì„¸ìš”"),
                (r"ì„¤ëª…í•˜ì„¸ìš”", "ì„¤ëª…í•´ì£¼ì„¸ìš”"),
                (r"ì•Œë ¤ì£¼ì„¸ìš”", "ì„¤ëª…í•´ì£¼ì„¸ìš”"),
                (r"ì‘ì„±í•˜ì„¸ìš”", "ì‘ì„±í•´ì£¼ì„¸ìš”"),
                (r"ë¬´ì—‡", "ë­"),
                (r"ì–´ë–»ê²Œ", "ì–´ë–»ê²Œ"),
            ]
        else:
            # ì˜ì–´ íŒ¨ëŸ¬í”„ë ˆì´ì§• íŒ¨í„´
            patterns = [
                (r"Please ", ""),
                (r"Can you ", ""),
                (r"What is", "What's"),
                (r"How do", "How to"),
                (r"Explain", "Describe"),
                (r"Tell me", "Explain"),
                (r"Write", "Create"),
            ]
        
        # íŒ¨í„´ ì ìš©
        for pattern, replacement in patterns:
            if re.search(pattern, instruction):
                varied["instruction"] = re.sub(pattern, replacement, instruction)
                varied["source"] = data["source"] + "_paraphrased"
                return varied
        
        return None
    
    def _add_context_variation(self, data: Dict, language: str) -> Optional[Dict]:
        """ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ ë³€í˜•"""
        varied = copy.deepcopy(data)
        
        if language == "ko":
            context_prefixes = [
                "ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: ",
                "ì•„ë˜ ë‚´ìš©ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”: ",
                "ë‹¤ìŒì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”: ",
                "ì´ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”: "
            ]
        else:
            context_prefixes = [
                "Please answer the following question: ",
                "Please explain the following: ",
                "Please provide information about: ",
                "Please solve this problem: "
            ]
        
        prefix = random.choice(context_prefixes)
        varied["instruction"] = prefix + data["instruction"]
        varied["source"] = data["source"] + "_contextualized"
        
        return varied
    
    def _change_formality(self, data: Dict, language: str) -> Optional[Dict]:
        """ê²©ì‹ ìˆ˜ì¤€ ë³€ê²½"""
        varied = copy.deepcopy(data)
        
        instruction = data["instruction"]
        
        if language == "ko":
            # ê²©ì‹ì²´ â†” ë¹„ê²©ì‹ì²´ ë³€í™˜
            if "ìš”" in instruction or "ìŠµë‹ˆë‹¤" in instruction:
                # ê²©ì‹ì²´ â†’ ë¹„ê²©ì‹ì²´
                patterns = [
                    (r"~ìŠµë‹ˆë‹¤", "~ë‹¤"),
                    (r"~ì„¸ìš”", "~ì–´"),
                    (r"í•´ì£¼ì„¸ìš”", "í•´ì¤˜"),
                    (r"~ì…ë‹ˆë‹¤", "~ì´ë‹¤"),
                ]
            else:
                # ë¹„ê²©ì‹ì²´ â†’ ê²©ì‹ì²´
                patterns = [
                    (r"~ë‹¤$", "~ìŠµë‹ˆë‹¤"),
                    (r"~ì–´$", "~ì„¸ìš”"),
                    (r"í•´ì¤˜", "í•´ì£¼ì„¸ìš”"),
                    (r"~ì´ë‹¤", "~ì…ë‹ˆë‹¤"),
                ]
        else:
            # ì˜ì–´ëŠ” ê°„ë‹¨í•œ ë³€í˜•ë§Œ
            patterns = [
                (r"can you", "could you"),
                (r"will you", "would you"),
                (r"please", "kindly"),
            ]
        
        # íŒ¨í„´ ì ìš©
        for pattern, replacement in patterns:
            if re.search(pattern, instruction):
                varied["instruction"] = re.sub(pattern, replacement, instruction)
                varied["source"] = data["source"] + "_formality_changed"
                return varied
        
        return None
    
    def _add_examples(self, data: Dict, language: str) -> Optional[Dict]:
        """ì˜ˆì‹œ ì¶”ê°€ ë³€í˜•"""
        varied = copy.deepcopy(data)
        
        if language == "ko":
            example_phrases = [
                "ì˜ˆë¥¼ ë“¤ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ë‹µí•´ì£¼ì„¸ìš”",
                "ì‹¤ì œ ì‚¬ë¡€ë¥¼ í¬í•¨í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ]
        else:
            example_phrases = [
                "Please explain with examples",
                "Please provide specific examples",
                "Please include real-world examples"
            ]
        
        example_phrase = random.choice(example_phrases)
        varied["instruction"] = data["instruction"] + ". " + example_phrase
        varied["source"] = data["source"] + "_with_examples"
        
        return varied
    
    def create_mixed_finetuning_dataset(self, grouped_data: Dict[str, List[Dict]]) -> List[Dict]:
        """
        í•œì˜ í˜¼í•© ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ìƒì„±
        
        Args:
            grouped_data: ì–¸ì–´ë³„ë¡œ ê·¸ë£¹í™”ëœ ë°ì´í„°
            
        Returns:
            í˜¼í•©ëœ ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹
        """
        logger.info("ğŸ”€ í•œì˜ í˜¼í•© ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
        
        korean_data = grouped_data["ko"]
        english_data = grouped_data["en"]
        
        # ì–¸ì–´ë³„ ëª©í‘œ í¬ê¸° ê³„ì‚°
        target_korean = int(self.target_count * self.language_ratios["ko"])
        target_english = int(self.target_count * self.language_ratios["en"])
        
        logger.info(f"ëª©í‘œ í¬ê¸° - í•œêµ­ì–´: {target_korean}, ì˜ì–´: {target_english}")
        
        # ë°ì´í„° ìƒ˜í”Œë§
        if len(korean_data) > target_korean:
            korean_sample = random.sample(korean_data, target_korean)
        else:
            korean_sample = korean_data
            logger.warning(f"í•œêµ­ì–´ ë°ì´í„° ë¶€ì¡±: {len(korean_data)} < {target_korean}")
        
        if len(english_data) > target_english:
            english_sample = random.sample(english_data, target_english)
        else:
            english_sample = english_data
            logger.warning(f"ì˜ì–´ ë°ì´í„° ë¶€ì¡±: {len(english_data)} < {target_english}")
        
        # í˜¼í•© ë° ì…”í”Œ
        mixed_data = korean_sample + english_sample
        random.shuffle(mixed_data)
        
        # ìµœì†Œ ëª©í‘œ ê°œìˆ˜ í™•ì¸
        if len(mixed_data) < self.min_target_count:
            logger.error(f"âŒ ë°ì´í„° ë¶€ì¡±: {len(mixed_data)} < {self.min_target_count}")
            logger.error("ë°ì´í„° ì¦ê°• ë˜ëŠ” ì¶”ê°€ ì†ŒìŠ¤ í™•ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            logger.info(f"âœ… ëª©í‘œ ê°œìˆ˜ ë‹¬ì„±: {len(mixed_data)} >= {self.min_target_count}")
        
        logger.info(f"âœ… í˜¼í•© ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(mixed_data)}ê°œ")
        
        return mixed_data
    
    def save_finetuning_dataset(self, dataset: List[Dict], 
                               dataset_type: str = "mixed") -> str:
        """
        ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ì €ì¥
        
        Args:
            dataset: ë°ì´í„°ì…‹
            dataset_type: ë°ì´í„°ì…‹ íƒ€ì… (mixed, korean, english)
            
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        output_file = self.output_dir / f"{dataset_type}_instructions.json"
        
        logger.info(f"ğŸ’¾ ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ì €ì¥ ì¤‘: {output_file}")
        
        # í‘œì¤€ ë¯¸ì„¸ì¡°ì • í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        finetuning_data = []
        task_distribution = defaultdict(int)
        
        for item in tqdm(dataset, desc="ë°ì´í„°ì…‹ ë³€í™˜"):
            # íƒœìŠ¤í¬ ë¶„í¬ ê³„ì‚°
            task_distribution[item["task_category"]] += 1
            
            # í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_item = {
                "instruction": item["instruction"],
                "input": item["input"],
                "output": item["output"],
                "language": item["language"],
                "task_category": item["task_category"],
                "source": item["source"]
            }
            finetuning_data.append(formatted_item)
        
        # ë°ì´í„°ì…‹ ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(finetuning_data, f, ensure_ascii=False, indent=2)
        
        # íƒœìŠ¤í¬ ë¶„í¬ ì •ë³´ ì €ì¥
        distribution_file = self.output_dir / f"{dataset_type}_task_distribution.json"
        with open(distribution_file, 'w', encoding='utf-8') as f:
            json.dump(dict(task_distribution), f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
        logger.info(f"ğŸ“Š íƒœìŠ¤í¬ ë¶„í¬ ì €ì¥: {distribution_file}")
        
        return str(output_file)
    
    def save_statistics(self):
        """ì²˜ë¦¬ í†µê³„ ì €ì¥"""
        stats_file = self.output_dir / "finetuning_preprocessing_stats.json"
        
        # ì–¸ì–´ë³„ ë¹„ìœ¨ ê³„ì‚°
        if self.stats["total_processed"] > 0:
            korean_ratio = self.stats["korean_instructions"] / self.stats["total_processed"]
            english_ratio = self.stats["english_instructions"] / self.stats["total_processed"]
        else:
            korean_ratio = english_ratio = 0
        
        detailed_stats = {
            **self.stats,
            "language_ratios": {
                "korean": round(korean_ratio, 3),
                "english": round(english_ratio, 3)
            },
            "processing_time": datetime.now().isoformat(),
            "quality_filters": {
                "min_instruction_length": self.min_instruction_length,
                "max_instruction_length": self.max_instruction_length,
                "min_output_length": self.min_output_length,
                "max_output_length": self.max_output_length
            },
            "targets": {
                "min_target_count": self.min_target_count,
                "target_count": self.target_count,
                "achieved": self.stats["total_processed"] >= self.min_target_count
            }
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š í†µê³„ ì €ì¥ ì™„ë£Œ: {stats_file}")
    
    def get_processing_summary(self) -> str:
        """ì²˜ë¦¬ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        korean_ratio = (self.stats["korean_instructions"] / self.stats["total_processed"] 
                       if self.stats["total_processed"] > 0 else 0)
        english_ratio = (self.stats["english_instructions"] / self.stats["total_processed"] 
                        if self.stats["total_processed"] > 0 else 0)
        
        target_achieved = "âœ… ë‹¬ì„±" if self.stats["total_processed"] >= self.min_target_count else "âŒ ë¯¸ë‹¬ì„±"
        
        summary = f"""
ğŸ“Š ë¯¸ì„¸ì¡°ì • ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½
==================================
ì´ ì²˜ë¦¬ëœ ëª…ë ¹ì–´: {self.stats['total_processed']:,}ê°œ
â”œâ”€ í•œêµ­ì–´: {self.stats['korean_instructions']:,}ê°œ ({korean_ratio:.1%})
â””â”€ ì˜ì–´: {self.stats['english_instructions']:,}ê°œ ({english_ratio:.1%})

ëª©í‘œ ë‹¬ì„± ì—¬ë¶€: {target_achieved}
â”œâ”€ ìµœì†Œ ëª©í‘œ: {self.min_target_count:,}ê°œ
â”œâ”€ í˜„ì¬ ë‹¬ì„±: {self.stats['total_processed']:,}ê°œ
â””â”€ ë‹¬ì„±ë¥ : {(self.stats['total_processed']/self.min_target_count*100):.1f}%

í•„í„°ë§ ê²°ê³¼:
â”œâ”€ í’ˆì§ˆ í•„í„°ë¡œ ì œì™¸: {self.stats['filtered_out']:,}ê°œ
â”œâ”€ ì¤‘ë³µ ì œê±°: {self.stats['duplicates_removed']:,}ê°œ
â””â”€ ë°ì´í„° ì¦ê°•: {self.stats['augmented_data']:,}ê°œ

ëª©í‘œ ì–¸ì–´ ë¹„ìœ¨:
â”œâ”€ í•œêµ­ì–´: {self.language_ratios['ko']:.1%}
â””â”€ ì˜ì–´: {self.language_ratios['en']:.1%}

ì €ì¥ ìœ„ì¹˜: {self.output_dir}
"""
        return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë¯¸ì„¸ì¡°ì • ë°ì´í„° ì „ì²˜ë¦¬")
    
    parser.add_argument(
        "--raw-data-dir",
        default="raw_datasets",
        help="ì›ì‹œ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: raw_datasets)"
    )
    parser.add_argument(
        "--output-dir",
        default="datasets",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: datasets)"
    )
    parser.add_argument(
        "--target-count",
        type=int,
        default=100000,
        help="ëª©í‘œ ë°ì´í„° ê°œìˆ˜ (ê¸°ë³¸ê°’: 100000)"
    )
    parser.add_argument(
        "--min-target",
        type=int,
        default=50000,
        help="ìµœì†Œ ëª©í‘œ ë°ì´í„° ê°œìˆ˜ (ê¸°ë³¸ê°’: 50000)"
    )
    parser.add_argument(
        "--korean-ratio",
        type=float,
        default=0.6,
        help="í•œêµ­ì–´ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.6)"
    )
    parser.add_argument(
        "--english-ratio",
        type=float,
        default=0.4,
        help="ì˜ì–´ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.4)"
    )
    parser.add_argument(
        "--no-augmentation",
        action="store_true",
        help="ë°ì´í„° ì¦ê°• ë¹„í™œì„±í™”"
    )
    parser.add_argument(
        "--korean-only",
        action="store_true",
        help="í•œêµ­ì–´ ì „ìš© ë°ì´í„°ì…‹ë§Œ ìƒì„±"
    )
    parser.add_argument(
        "--english-only",
        action="store_true",
        help="ì˜ì–´ ì „ìš© ë°ì´í„°ì…‹ë§Œ ìƒì„±"
    )
    
    args = parser.parse_args()
    
    # ì–¸ì–´ ë¹„ìœ¨ ê²€ì¦
    if abs((args.korean_ratio + args.english_ratio) - 1.0) > 0.01:
        logger.error("âŒ í•œêµ­ì–´ì™€ ì˜ì–´ ë¹„ìœ¨ì˜ í•©ì´ 1.0ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        sys.exit(1)
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    processor = FinetuningDataProcessor(
        raw_data_dir=args.raw_data_dir,
        output_dir=args.output_dir
    )
    
    # ì„¤ì • ì—…ë°ì´íŠ¸
    processor.target_count = args.target_count
    processor.min_target_count = args.min_target
    processor.language_ratios = {
        "ko": args.korean_ratio,
        "en": args.english_ratio
    }
    
    try:
        # ì›ì‹œ ëª…ë ¹ì–´ ë°ì´í„° ë¡œë“œ
        grouped_data = processor.load_instruction_data()
        
        if not grouped_data["ko"] and not grouped_data["en"]:
            logger.error("âŒ ì²˜ë¦¬í•  ëª…ë ¹ì–´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            sys.exit(1)
        
        # ë°ì´í„° ì¦ê°• (ì˜µì…˜ì— ë”°ë¼)
        if not args.no_augmentation:
            grouped_data = processor.augment_data(grouped_data)
        
        # ë°ì´í„°ì…‹ ìƒì„± ë° ì €ì¥
        if args.korean_only:
            if grouped_data["ko"]:
                logger.info("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ì „ìš© ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ìƒì„±")
                processor.save_finetuning_dataset(grouped_data["ko"], "korean")
        elif args.english_only:
            if grouped_data["en"]:
                logger.info("ğŸ‡ºğŸ‡¸ ì˜ì–´ ì „ìš© ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ìƒì„±")
                processor.save_finetuning_dataset(grouped_data["en"], "english")
        else:
            # í˜¼í•© ë°ì´í„°ì…‹ ìƒì„±
            logger.info("ğŸ”€ í˜¼í•© ë¯¸ì„¸ì¡°ì • ë°ì´í„°ì…‹ ìƒì„±")
            mixed_dataset = processor.create_mixed_finetuning_dataset(grouped_data)
            processor.save_finetuning_dataset(mixed_dataset, "mixed")
        
        # í†µê³„ ì €ì¥
        processor.save_statistics()
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print(processor.get_processing_summary())
        
        logger.info("âœ… ë¯¸ì„¸ì¡°ì • ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
Dynamic all datasets download script
JSON ì„¤ì • ê¸°ë°˜ ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
import argparse
import time
import logging
from pathlib import Path

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬
script_dir = Path(__file__).parent
sys.path.insert(0, str(script_dir))

from dataset_loader import DynamicDatasetLoader

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def main():
    parser = argparse.ArgumentParser(description="JSON ì„¤ì • ê¸°ë°˜ ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--force", action="store_true", help="ê¸°ì¡´ ë°ì´í„° ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--small", action="store_true", help="ì†ŒëŸ‰ ìƒ˜í”Œë§Œ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--output_dir", type=str, default="../../../../datasets", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--korean_only", action="store_true", help="í•œêµ­ì–´ë§Œ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--english_only", action="store_true", help="ì˜ì–´ë§Œ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--korean_config", type=str, 
                       default=str(script_dir.parent.parent / "configs" / "dataset" / "korean_datasets.json"),
                       help="í•œêµ­ì–´ ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼")
    parser.add_argument("--english_config", type=str, 
                       default=str(script_dir.parent.parent / "configs" / "dataset" / "english_datasets.json"),
                       help="ì˜ì–´ ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼")
    parser.add_argument("--info", action="store_true", help="ë°ì´í„°ì…‹ ì„¤ì • ì •ë³´ë§Œ í‘œì‹œ")
    
    args = parser.parse_args()
    
    try:
        start_time = time.time()
        
        # ì„¤ì • íŒŒì¼ ê²½ë¡œë“¤
        korean_config = Path(args.korean_config)
        english_config = Path(args.english_config)
        
        # ì •ë³´ í‘œì‹œ ëª¨ë“œ
        if args.info:
            logger.info("ğŸ“‹ ì „ì²´ ë°ì´í„°ì…‹ ì„¤ì • ì •ë³´:")
            
            # í•œêµ­ì–´ ì„¤ì • ì •ë³´
            if korean_config.exists() and not args.english_only:
                logger.info("\nğŸ‡°ğŸ‡· í•œêµ­ì–´ ë°ì´í„°ì…‹:")
                korean_loader = DynamicDatasetLoader(str(korean_config), str(script_dir.parent.parent.parent.parent / "models"))
                korean_info = korean_loader.get_dataset_info()
                for i, ds in enumerate(korean_info['enabled_datasets'], 1):
                    logger.info(f"   {i}. {ds['description']}")
            
            # ì˜ì–´ ì„¤ì • ì •ë³´
            if english_config.exists() and not args.korean_only:
                logger.info("\nğŸ‡ºğŸ‡¸ ì˜ì–´ ë°ì´í„°ì…‹:")
                english_loader = DynamicDatasetLoader(str(english_config), str(script_dir.parent.parent.parent.parent / "models"))
                english_info = english_loader.get_dataset_info()
                for i, ds in enumerate(english_info['enabled_datasets'], 1):
                    logger.info(f"   {i}. {ds['description']}")
            
            return 0
        
        logger.info("ì „ì²´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        korean_data = []
        english_data = []
        
        # í•œêµ­ì–´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        if not args.english_only and korean_config.exists():
            logger.info("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            
            korean_loader = DynamicDatasetLoader(str(korean_config), str(script_dir.parent.parent.parent.parent / "models"))
            korean_output_file = output_dir / korean_loader.global_settings.get('output_filename', 'korean_corpus.json')
            
            # ê¸°ì¡´ íŒŒì¼ ì²´í¬
            if korean_output_file.exists() and not args.force:
                logger.info(f"ê¸°ì¡´ í•œêµ­ì–´ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤: {korean_output_file}")
            else:
                if args.force and korean_output_file.exists():
                    korean_output_file.unlink()
                    logger.info("ê¸°ì¡´ í•œêµ­ì–´ ë°ì´í„° ì‚­ì œ")
                
                korean_data = korean_loader.load_all_datasets(small_mode=args.small)
                if korean_data:
                    korean_loader.save_texts(korean_data, str(korean_output_file))
        
        # ì˜ì–´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        if not args.korean_only and english_config.exists():
            logger.info("ğŸ‡ºğŸ‡¸ ì˜ì–´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            
            english_loader = DynamicDatasetLoader(str(english_config), str(script_dir.parent.parent.parent.parent / "models"))
            english_output_file = output_dir / english_loader.global_settings.get('output_filename', 'english_corpus.json')
            
            # ê¸°ì¡´ íŒŒì¼ ì²´í¬
            if english_output_file.exists() and not args.force:
                logger.info(f"ê¸°ì¡´ ì˜ì–´ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤: {english_output_file}")
            else:
                if args.force and english_output_file.exists():
                    english_output_file.unlink()
                    logger.info("ê¸°ì¡´ ì˜ì–´ ë°ì´í„° ì‚­ì œ")
                
                english_data = english_loader.load_all_datasets(small_mode=args.small)
                if english_data:
                    english_loader.save_texts(english_data, str(english_output_file))
        
        # ê²°ê³¼ ìš”ì•½
        end_time = time.time()
        total_time = end_time - start_time
        
        logger.info("=" * 60)
        logger.info("âœ… ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        
        korean_path = output_dir / "korean_corpus.json"
        english_path = output_dir / "english_corpus.json"
        
        if korean_path.exists():
            korean_size = korean_path.stat().st_size / (1024 * 1024)  # MB
            logger.info(f"ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë°ì´í„°: {korean_size:.1f} MB")
        
        if english_path.exists():
            english_size = english_path.stat().st_size / (1024 * 1024)  # MB
            logger.info(f"ğŸ‡ºğŸ‡¸ ì˜ì–´ ë°ì´í„°: {english_size:.1f} MB")
        
        total_docs = len(korean_data) + len(english_data)
        logger.info(f"ğŸ“Š ì´ ë¬¸ì„œ: {total_docs:,}ê°œ")
        
        # ì´ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
        total_size = 0
        if korean_path.exists():
            total_size += korean_path.stat().st_size
        if english_path.exists():
            total_size += english_path.stat().st_size
        
        total_size_mb = total_size / (1024 * 1024)
        logger.info(f"ğŸ’¾ ì´ í¬ê¸°: {total_size_mb:.1f} MB")
        
        logger.info("=" * 60)
        logger.info("ğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("1. python3 common/scripts/check_datasets.py  # ë°ì´í„° í™•ì¸")
        logger.info("2. ì›í•˜ëŠ” ëª¨ë¸ì—ì„œ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì‹œì‘")
        
        logger.info("\nğŸ”§ ì„¤ì • íŒŒì¼ ê´€ë¦¬:")
        logger.info(f"   â€¢ í•œêµ­ì–´: {korean_config}")
        logger.info(f"   â€¢ ì˜ì–´: {english_config}")
        logger.info("ğŸ’¡ ë°ì´í„°ì…‹ì„ ì¶”ê°€/ì œê±°í•˜ë ¤ë©´ JSON ì„¤ì • íŒŒì¼ì„ í¸ì§‘í•˜ì„¸ìš”.")
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 
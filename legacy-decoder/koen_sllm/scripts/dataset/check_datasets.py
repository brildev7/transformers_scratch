#!/usr/bin/env python3
"""
Independent dataset checker script
ë…ë¦½ì ì¸ ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ì…‹ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, Optional, List

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ë””ë ‰í† ë¦¬ 
script_dir = Path(__file__).parent


def check_dataset_file(file_path: Path, dataset_name: str) -> Optional[Dict]:
    """ë°ì´í„°ì…‹ íŒŒì¼ í™•ì¸"""
    if not file_path.exists():
        logger.warning(f"âŒ {dataset_name} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return None
    
    try:
        # íŒŒì¼ í¬ê¸°
        file_size = file_path.stat().st_size
        size_mb = file_size / (1024 * 1024)
        
        # ë‚´ìš© í™•ì¸
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            logger.error(f"âŒ {dataset_name} íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")
            return None
        
        num_docs = len(data)
        
        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
        if num_docs > 0:
            sample_lengths = []
            valid_texts = 0
            
            for i, text in enumerate(data[:min(100, num_docs)]):
                if isinstance(text, str) and text.strip():
                    sample_lengths.append(len(text))
                    valid_texts += 1
            
            if sample_lengths:
                avg_length = sum(sample_lengths) / len(sample_lengths)
                min_length = min(sample_lengths)
                max_length = max(sample_lengths)
            else:
                avg_length = min_length = max_length = 0
                
            # ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë¹„ìœ¨
            valid_ratio = valid_texts / min(100, num_docs) * 100
        else:
            avg_length = min_length = max_length = 0
            valid_ratio = 0
        
        info = {
            'file_size_mb': size_mb,
            'num_documents': num_docs,
            'avg_text_length': avg_length,
            'min_text_length': min_length,
            'max_text_length': max_length,
            'valid_text_ratio': valid_ratio,
            'sample_texts': data[:3] if num_docs > 0 else []
        }
        
        logger.info(f"âœ… {dataset_name} ë°ì´í„°ì…‹:")
        logger.info(f"   ğŸ“ íŒŒì¼ í¬ê¸°: {size_mb:.1f} MB")
        logger.info(f"   ğŸ“„ ë¬¸ì„œ ìˆ˜: {num_docs:,}ê°œ")
        logger.info(f"   ğŸ“ í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {avg_length:.0f}ì")
        logger.info(f"   ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´ ë²”ìœ„: {min_length}~{max_length}ì")
        logger.info(f"   âœ”ï¸  ìœ íš¨ í…ìŠ¤íŠ¸ ë¹„ìœ¨: {valid_ratio:.1f}%")
        
        return info
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ {dataset_name} JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None
    except Exception as e:
        logger.error(f"âŒ {dataset_name} íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return None


def show_sample_texts(korean_data: Optional[Dict], english_data: Optional[Dict], num_samples: int = 3):
    """ìƒ˜í”Œ í…ìŠ¤íŠ¸ í‘œì‹œ"""
    logger.info("ğŸ“ ìƒ˜í”Œ í…ìŠ¤íŠ¸:")
    
    if korean_data and korean_data['sample_texts']:
        logger.info("\nğŸ‡°ğŸ‡· í•œêµ­ì–´ ìƒ˜í”Œ:")
        for i, text in enumerate(korean_data['sample_texts'][:num_samples]):
            if isinstance(text, str):
                preview = text[:200] + "..." if len(text) > 200 else text
                logger.info(f"   {i+1}. {preview}")
    
    if english_data and english_data['sample_texts']:
        logger.info("\nğŸ‡ºğŸ‡¸ ì˜ì–´ ìƒ˜í”Œ:")
        for i, text in enumerate(english_data['sample_texts'][:num_samples]):
            if isinstance(text, str):
                preview = text[:200] + "..." if len(text) > 200 else text
                logger.info(f"   {i+1}. {preview}")


def check_disk_usage(data_dir: Path):
    """ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    total_size = 0
    file_count = 0
    
    if data_dir.exists():
        for file_path in data_dir.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
                file_count += 1
    
    total_mb = total_size / (1024 * 1024)
    total_gb = total_mb / 1024
    
    logger.info(f"ğŸ’¾ ë°ì´í„° ë””ë ‰í† ë¦¬ ì‚¬ìš©ëŸ‰:")
    logger.info(f"   ğŸ“ ì´ íŒŒì¼ ìˆ˜: {file_count}ê°œ")
    logger.info(f"   ğŸ“¦ ì´ í¬ê¸°: {total_mb:.1f} MB ({total_gb:.2f} GB)")


def analyze_data_quality(korean_data: Optional[Dict], english_data: Optional[Dict]) -> List[str]:
    """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
    quality_issues = []
    
    # í•œêµ­ì–´ ë°ì´í„° ì²´í¬
    if korean_data:
        if korean_data['avg_text_length'] < 50:
            quality_issues.append("âš ï¸  í•œêµ­ì–´ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (í‰ê·  50ì ë¯¸ë§Œ)")
        if korean_data['num_documents'] < 100:
            quality_issues.append("âš ï¸  í•œêµ­ì–´ ë¬¸ì„œ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (100ê°œ ë¯¸ë§Œ)")
        if korean_data['valid_text_ratio'] < 80:
            quality_issues.append(f"âš ï¸  í•œêµ­ì–´ ìœ íš¨ í…ìŠ¤íŠ¸ ë¹„ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤ ({korean_data['valid_text_ratio']:.1f}%)")
    else:
        quality_issues.append("âŒ í•œêµ­ì–´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    # ì˜ì–´ ë°ì´í„° ì²´í¬
    if english_data:
        if english_data['avg_text_length'] < 50:
            quality_issues.append("âš ï¸  ì˜ì–´ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (í‰ê·  50ì ë¯¸ë§Œ)")
        if english_data['num_documents'] < 100:
            quality_issues.append("âš ï¸  ì˜ì–´ ë¬¸ì„œ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (100ê°œ ë¯¸ë§Œ)")
        if english_data['valid_text_ratio'] < 80:
            quality_issues.append(f"âš ï¸  ì˜ì–´ ìœ íš¨ í…ìŠ¤íŠ¸ ë¹„ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤ ({english_data['valid_text_ratio']:.1f}%)")
    else:
        quality_issues.append("âŒ ì˜ì–´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    return quality_issues


def provide_recommendations(total_docs: int, quality_issues: List[str]):
    """ì‚¬ìš©ìì—ê²Œ ê¶Œì¥ì‚¬í•­ ì œê³µ"""
    logger.info("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
    
    has_critical_issues = any("âŒ" in issue for issue in quality_issues)
    
    if has_critical_issues:
        logger.info("   ğŸ”„ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”:")
        logger.info("      python3 common/scripts/download_all.py --small")
    elif total_docs < 1000:
        logger.info("   ğŸ”„ ë” ë§ì€ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤:")
        logger.info("      python3 common/scripts/download_all.py")
    elif total_docs < 10000:
        logger.info("   ğŸ“Š í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œëŠ” ì¶©ë¶„í•˜ì§€ë§Œ, ì‹¤ì œ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        logger.info("      python3 common/scripts/download_all.py")
    else:
        logger.info("   âœ… í•™ìŠµì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤")
        logger.info("   ğŸš€ ì´ì œ ì›í•˜ëŠ” ëª¨ë¸ì—ì„œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    
    logger.info("\nğŸ”§ ì¶”ê°€ ë„êµ¬:")
    logger.info("   â€¢ ì…¸ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©: ./common/scripts/download_datasets.sh")
    logger.info("   â€¢ ì˜ì–´ë§Œ: python3 common/scripts/download_english.py")
    logger.info("   â€¢ í•œêµ­ì–´ë§Œ: python3 common/scripts/download_korean.py")


def main():
    parser = argparse.ArgumentParser(description="ë…ë¦½ì ì¸ ë°ì´í„°ì…‹ í™•ì¸ ë„êµ¬")
    parser.add_argument("--data_dir", type=str, default="../../../../datasets", help="ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--show_samples", action="store_true", help="ìƒ˜í”Œ í…ìŠ¤íŠ¸ í‘œì‹œ")
    parser.add_argument("--export_stats", type=str, help="í†µê³„ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸í•œ ì¶œë ¥")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    data_dir = Path(args.data_dir)
    korean_path = data_dir / "korean_corpus.json"
    english_path = data_dir / "english_corpus.json"
    
    logger.info("=" * 60)
    logger.info("ğŸ“Š ë…ë¦½ì ì¸ ë°ì´í„°ì…‹ í˜„í™© í™•ì¸")
    logger.info(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    logger.info("=" * 60)
    
    # ë°ì´í„°ì…‹ íŒŒì¼ í™•ì¸
    korean_data = check_dataset_file(korean_path, "í•œêµ­ì–´")
    english_data = check_dataset_file(english_path, "ì˜ì–´")
    
    # ì „ì²´ í†µê³„
    total_docs = 0
    total_size = 0
    
    if korean_data:
        total_docs += korean_data['num_documents']
        total_size += korean_data['file_size_mb']
    
    if english_data:
        total_docs += english_data['num_documents']
        total_size += english_data['file_size_mb']
    
    logger.info("\nğŸ“ˆ ì „ì²´ í†µê³„:")
    logger.info(f"   ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {total_docs:,}ê°œ")
    logger.info(f"   ğŸ’¾ ì´ ë°ì´í„° í¬ê¸°: {total_size:.1f} MB")
    
    # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
    logger.info("")
    check_disk_usage(data_dir)
    
    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ í‘œì‹œ
    if args.show_samples:
        logger.info("")
        show_sample_texts(korean_data, english_data)
    
    # ë°ì´í„° í’ˆì§ˆ ì²´í¬
    logger.info("\nğŸ” ë°ì´í„° í’ˆì§ˆ ì²´í¬:")
    quality_issues = analyze_data_quality(korean_data, english_data)
    
    if quality_issues:
        for issue in quality_issues:
            logger.warning(f"   {issue}")
    else:
        logger.info("   âœ… ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤")
    
    # ê¶Œì¥ì‚¬í•­ ì œê³µ
    provide_recommendations(total_docs, quality_issues)
    
    # í†µê³„ ë‚´ë³´ë‚´ê¸°
    if args.export_stats:
        from datetime import datetime
        
        stats = {
            'korean_data': korean_data,
            'english_data': english_data,
            'total_documents': total_docs,
            'total_size_mb': total_size,
            'quality_issues': quality_issues,
            'check_timestamp': datetime.now().isoformat(),
            'data_directory': str(data_dir)
        }
        
        export_path = Path(args.export_stats)
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"\nğŸ“Š í†µê³„ê°€ {export_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    logger.info("=" * 60)
    
    # ì¢…ë£Œ ì½”ë“œ ê²°ì •
    has_critical_issues = any("âŒ" in issue for issue in quality_issues)
    return 1 if has_critical_issues else 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 
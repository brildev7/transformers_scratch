#!/usr/bin/env python3
"""
Dynamic Dataset Loader
JSON ì„¤ì • ê¸°ë°˜ ë™ì  ë°ì´í„°ì…‹ ë¡œë”
"""
import json
import logging
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Set

logger = logging.getLogger(__name__)


class DatasetConfig:
    """ë°ì´í„°ì…‹ ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self, config_dict: Dict[str, Any]):
        self.name = config_dict['name']
        self.config = config_dict.get('config')
        self.split = config_dict['split']
        self.description = config_dict['description']
        self.text_field = config_dict['text_field']
        self.limits = config_dict['limits']
        self.min_text_length = config_dict['min_text_length']
        self.max_text_length = config_dict.get('max_text_length')
        self.streaming = config_dict.get('streaming', False)
        self.enabled = config_dict.get('enabled', True)
        self.priority = config_dict.get('priority', 999)
        self.fallback = config_dict.get('fallback', {})
        self.filter = config_dict.get('filter', {})
        self.note = config_dict.get('note', '')
    
    def get_limit(self, small_mode: bool) -> int:
        """ëª¨ë“œì— ë”°ë¥¸ ë¬¸ì„œ ì œí•œ ìˆ˜ ë°˜í™˜"""
        return self.limits['small'] if small_mode else self.limits['full']
    
    def __repr__(self):
        return f"DatasetConfig(name='{self.name}', description='{self.description}')"


class DynamicDatasetLoader:
    """JSON ì„¤ì • ê¸°ë°˜ ë™ì  ë°ì´í„°ì…‹ ë¡œë”"""
    
    def __init__(self, config_file_path: str, cache_dir: str):
        self.config_file_path = Path(config_file_path)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ í•´ì‹œ ì„¸íŠ¸
        self.text_hashes: Set[str] = set()
        
        # JSON ì„¤ì • ë¡œë“œ
        self.config = self._load_config()
        self.global_settings = self.config.get('global_settings', {})
        self.datasets = self._parse_datasets()
        
        logger.info(f"ì„¤ì • íŒŒì¼ ë¡œë“œ: {self.config_file_path}")
        logger.info(f"í™œì„±í™”ëœ ë°ì´í„°ì…‹: {len(self.datasets)}ê°œ")
    
    def _load_config(self) -> Dict[str, Any]:
        """JSON ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.config_file_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except Exception as e:
            logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {self.config_file_path}")
            logger.error(f"ì˜¤ë¥˜: {e}")
            raise
    
    def _parse_datasets(self) -> List[DatasetConfig]:
        """ë°ì´í„°ì…‹ ì„¤ì • íŒŒì‹±"""
        datasets = []
        for dataset_dict in self.config.get('datasets', []):
            if dataset_dict.get('enabled', True):
                datasets.append(DatasetConfig(dataset_dict))
        
        # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        datasets.sort(key=lambda x: x.priority)
        return datasets
    
    def _get_text_hash(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ í•´ì‹œê°’ ìƒì„±"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _is_duplicate(self, text: str) -> bool:
        """ì¤‘ë³µ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸"""
        text_hash = self._get_text_hash(text)
        if text_hash in self.text_hashes:
            return True
        self.text_hashes.add(text_hash)
        return False
    
    def load_dataset_safely(self, dataset_config: DatasetConfig, small_mode: bool = False) -> List[str]:
        """ì•ˆì „í•˜ê²Œ ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            from datasets import load_dataset
        except ImportError:
            logger.error("datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install datasets")
            return []
        
        logger.info(f"{dataset_config.description} ë¡œë“œ ì¤‘...")
        texts = []
        
        try:
            # ë°ì´í„°ì…‹ ë¡œë“œ íŒŒë¼ë¯¸í„° êµ¬ì„±
            load_params = {
                'path': dataset_config.name,
                'split': dataset_config.split,
                'cache_dir': str(self.cache_dir)
            }
            
            if dataset_config.config:
                load_params['name'] = dataset_config.config
            
            if dataset_config.streaming:
                load_params['streaming'] = True
            
            # ë°ì´í„°ì…‹ ë¡œë“œ
            dataset = load_dataset(**load_params)
            
            # ì œí•œ ìˆ˜ ì„¤ì •
            limit = dataset_config.get_limit(small_mode)
            
            # ìŠ¤íŠ¸ë¦¬ë° vs ì¼ë°˜ ì²˜ë¦¬
            if dataset_config.streaming:
                texts = self._process_streaming_dataset(dataset, dataset_config, limit)
            else:
                texts = self._process_regular_dataset(dataset, dataset_config, limit)
            
            logger.info(f"{dataset_config.description}: {len(texts)}ê°œ ë¬¸ì„œ")
            return texts
            
        except Exception as e:
            logger.warning(f"{dataset_config.description} ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # fallback ì²˜ë¦¬
            fallback_action = dataset_config.fallback.get('action', 'skip')
            if fallback_action == 'skip':
                logger.info(f"{dataset_config.description} ìŠ¤í‚µë¨")
            elif fallback_action == 'continue':
                logger.info(f"{dataset_config.description} ì‹¤íŒ¨í–ˆì§€ë§Œ ê³„ì† ì§„í–‰")
            
            return []
    
    def _process_streaming_dataset(self, dataset, config: DatasetConfig, limit: int) -> List[str]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        texts = []
        for i, example in enumerate(dataset):
            if i >= limit:
                break
            
            text = self._extract_text(example, config)
            if text and self._validate_text(text, config) and not self._is_duplicate(text):
                texts.append(text)
        
        return texts
    
    def _process_regular_dataset(self, dataset, config: DatasetConfig, limit: int) -> List[str]:
        """ì¼ë°˜ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        texts = []
        
        # ë°ì´í„°ì…‹ í¬ê¸° ì œí•œ
        if len(dataset) > limit:
            dataset = dataset.select(range(limit))
        
        for example in dataset:
            text = self._extract_text(example, config)
            if text and self._validate_text(text, config) and not self._is_duplicate(text):
                texts.append(text)
        
        return texts
    
    def _extract_text(self, example: Dict, config: DatasetConfig) -> Optional[str]:
        """ì˜ˆì œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        text = example.get(config.text_field, '').strip()
        
        # ì–¸ì–´ í•„í„° ì ìš©
        if config.filter:
            lang_field = config.filter.get('language_field')
            lang_value = config.filter.get('language_value')
            if lang_field and lang_value:
                if lang_value.lower() not in example.get(lang_field, '').lower():
                    return None
        
        # ê¸¸ì´ ì œí•œ ì ìš©
        if config.max_text_length and len(text) > config.max_text_length:
            text = text[:config.max_text_length]
        
        return text
    
    def _validate_text(self, text: str, config: DatasetConfig) -> bool:
        """í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦"""
        if len(text) < config.min_text_length:
            return False
        
        return True
    
    def load_all_datasets(self, small_mode: bool = False) -> List[str]:
        """ëª¨ë“  í™œì„±í™”ëœ ë°ì´í„°ì…‹ ë¡œë“œ"""
        all_texts = []
        total_processed = 0
        duplicates_removed = 0
        
        logger.info("ğŸ”„ ì¤‘ë³µ ì œê±° ê¸°ëŠ¥ í™œì„±í™”")
        
        for dataset_config in self.datasets:
            initial_hash_count = len(self.text_hashes)
            texts = self.load_dataset_safely(dataset_config, small_mode)
            
            # ì¤‘ë³µ ì œê±° í†µê³„ ê³„ì‚°
            final_hash_count = len(self.text_hashes)
            dataset_duplicates = (initial_hash_count + len(texts)) - final_hash_count
            
            total_processed += len(texts) + dataset_duplicates
            duplicates_removed += dataset_duplicates
            
            if dataset_duplicates > 0:
                logger.info(f"   â™»ï¸  ì¤‘ë³µ ì œê±°: {dataset_duplicates}ê°œ")
            
            all_texts.extend(texts)
        
        # ìµœì¢… í†µê³„
        logger.info(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
        logger.info(f"   ì´ ì²˜ë¦¬: {total_processed:,}ê°œ ë¬¸ì„œ")
        logger.info(f"   ì¤‘ë³µ ì œê±°: {duplicates_removed:,}ê°œ ë¬¸ì„œ")
        logger.info(f"   ìµœì¢… ë°ì´í„°: {len(all_texts):,}ê°œ ë¬¸ì„œ")
        logger.info(f"   ì¤‘ë³µë¥ : {(duplicates_removed/total_processed*100):.1f}%" if total_processed > 0 else "   ì¤‘ë³µë¥ : 0.0%")
        
        return all_texts
    
    def save_texts(self, texts: List[str], output_path: Optional[str] = None) -> Path:
        """í…ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if output_path is None:
            output_path = self.global_settings.get('output_filename', 'corpus.json')
        
        output_file = Path(output_path)
        
        # ì „ì—­ ì„¤ì •ì—ì„œ JSON ì €ì¥ ì˜µì…˜ ê°€ì ¸ì˜¤ê¸°
        indent = self.global_settings.get('json_indent', 2)
        encoding = self.global_settings.get('encoding', 'utf-8')
        
        with open(output_file, 'w', encoding=encoding) as f:
            json.dump(texts, f, ensure_ascii=False, indent=indent)
        
        logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file} ({len(texts):,}ê°œ ë¬¸ì„œ)")
        return output_file
    
    def get_dataset_info(self) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ì •ë³´ ë°˜í™˜"""
        enabled_datasets = [
            {
                'name': ds.name,
                'description': ds.description,
                'priority': ds.priority,
                'enabled': ds.enabled
            }
            for ds in self.datasets
        ]
        
        return {
            'config_file': str(self.config_file_path),
            'total_datasets': len(self.datasets),
            'enabled_datasets': enabled_datasets,
            'global_settings': self.global_settings
        } 
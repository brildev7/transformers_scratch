# Configuration Files Directory

μ΄ λ””λ ‰ν† λ¦¬λ” νΈλμ¤ν¬λ¨Έ λ¨λΈ ν”„λ΅μ νΈμ λ¨λ“  μ„¤μ • νμΌλ“¤μ„ μ©λ„λ³„λ΅ μ²΄κ³„μ μΌλ΅ κ΄€λ¦¬ν•©λ‹λ‹¤.

## π“ λ””λ ‰ν† λ¦¬ κµ¬μ΅°

```
configs/
β”β”€β”€ training/           # λ¨λΈ ν›λ ¨ μ„¤μ •
β”‚   β”β”€β”€ small_model.json      # μ†ν• λ¨λΈ ν›λ ¨ μ„¤μ • (124M)
β”‚   β”β”€β”€ base_model.json       # κΈ°λ³Έ λ¨λΈ ν›λ ¨ μ„¤μ • (355M)
β”‚   β””β”€β”€ large_model.json      # λ€ν• λ¨λΈ ν›λ ¨ μ„¤μ • (1.3B)
β”β”€β”€ models/             # λ¨λΈ μ•„ν‚¤ν…μ² μ„¤μ •
β”‚   β”β”€β”€ gpt_small.json        # GPT-Small μ•„ν‚¤ν…μ²
β”‚   β”β”€β”€ gpt_base.json         # GPT-Base μ•„ν‚¤ν…μ²
β”‚   β””β”€β”€ gpt_large.json        # GPT-Large μ•„ν‚¤ν…μ²
β”β”€β”€ dataset/           # λ°μ΄ν„°μ…‹ μ„¤μ •
β”‚   β”β”€β”€ pretraining.json      # μ‚¬μ „ν›λ ¨μ© λ°μ΄ν„° μ„¤μ •
β”‚   β””β”€β”€ finetuning.json       # νμΈνλ‹μ© λ°μ΄ν„° μ„¤μ •
β””β”€β”€ README.md           # μ΄ νμΌ
```

## π― μ©λ„λ³„ λ¶„λ¥

### 1. **Training Configs** (`training/`)
λ¨λΈ ν›λ ¨μ— ν•„μ”ν• λ¨λ“  ν•μ΄νΌνλΌλ―Έν„°λ¥Ό ν¬ν•¨:
- ν•™μµλ¥ , λ°°μΉ ν¬κΈ°, μ¤μΌ€μ¤„λ§
- λ¨λΈ μ•„ν‚¤ν…μ² + ν›λ ¨ μ„¤μ • ν†µν•©
- GPU/λ¶„μ‚° ν•™μµ μ„¤μ •

### 2. **Model Architecture Configs** (`models/`)
μμ λ¨λΈ μ•„ν‚¤ν…μ² μ •μλ§ ν¬ν•¨:
- λ¨λΈ κµ¬μ΅° (layers, heads, dimensions)
- μ•„ν‚¤ν…μ² ν•μ΄νΌνλΌλ―Έν„°
- ν›λ ¨κ³Ό λ…λ¦½μ μΈ λ¨λΈ μ •μ

### 3. **Dataset Configs** (`dataset/`)
λ°μ΄ν„° μ²λ¦¬ λ° λ΅λ”© μ„¤μ •:
- λ°μ΄ν„°μ…‹ κ²½λ΅ λ° ν•μ‹
- μ „μ²λ¦¬ νμ΄ν”„λΌμΈ
- λ°μ΄ν„° λ΅λ” μ„¤μ •

## π€ μ‚¬μ© λ°©λ²•

### ν›λ ¨ μ‹μ‘
```bash
# μ†ν• λ¨λΈλ΅ λΉ λ¥Έ ν…μ¤νΈ
python train.py --config configs/training/small_model.json

# κΈ°λ³Έ λ¨λΈλ΅ μ‹¤μ  ν›λ ¨
python train.py --config configs/training/base_model.json

# λ€ν• λ¨λΈλ΅ production ν›λ ¨
python train.py --config configs/training/large_model.json
```

### λ¨λΈ μƒμ„±
```python
from transformers import AutoConfig, AutoModel

# λ¨λΈ μ•„ν‚¤ν…μ²λ§ λ΅λ“
config = AutoConfig.from_json_file("configs/models/gpt_base.json")
model = AutoModel.from_config(config)
```

### λ°μ΄ν„°μ…‹ μ„¤μ •
```python
import json

# μ‚¬μ „ν›λ ¨μ© λ°μ΄ν„° μ„¤μ • λ΅λ“
with open("configs/dataset/pretraining.json") as f:
    data_config = json.load(f)
```

## β™οΈ μ„¤μ • νμΌ μ„ νƒ κ°€μ΄λ“

### λ¨λΈ ν¬κΈ°λ³„ κ¶μ¥μ‚¬ν•­

| λ¨λΈ ν¬κΈ° | νλΌλ―Έν„° μ | Training Config | Model Config | μ©λ„ |
|----------|------------|----------------|---------------|------|
| **Small** | 124M | `small_model.json` | `gpt_small.json` | λΉ λ¥Έ μ‹¤ν—, ν…μ¤νΈ |
| **Base** | 355M | `base_model.json` | `gpt_base.json` | μΌλ°μ μΈ ν›λ ¨ |
| **Large** | 1.3B | `large_model.json` | `gpt_large.json` | Production λ¨λΈ |

### ν•λ“μ›¨μ–΄λ³„ κ¶μ¥μ‚¬ν•­

| GPU λ©”λ¨λ¦¬ | κ¶μ¥ μ„¤μ • | λ°°μΉ ν¬κΈ° | κ·Έλλ””μ–ΈνΈ λ„μ  |
|-----------|----------|----------|----------------|
| 8GB | Small | 4-8 | 4-8 |
| 16GB | Base | 2-4 | 8-16 |
| 24GB+ | Large | 1-2 | 16-32 |

## π”§ μ„¤μ • νμΌ μμ •

### μƒλ΅μ΄ λ¨λΈ ν¬κΈ° μ¶”κ°€
1. `models/` μ— μƒ μ•„ν‚¤ν…μ² νμΌ μƒμ„±
2. `training/` μ— ν•΄λ‹Ή ν›λ ¨ μ„¤μ • μƒμ„±
3. νλΌλ―Έν„° μ κ³„μ‚°ν•μ—¬ `estimated_parameters` μ—…λ°μ΄νΈ

### ν•μ΄νΌνλΌλ―Έν„° νλ‹
- **ν•™μµλ¥ **: `0.0001` β†’ `0.00005` (λ€ν• λ¨λΈμΌμλ΅ μ‘κ²)
- **λ°°μΉ ν¬κΈ°**: λ©”λ¨λ¦¬μ— λ§κ² μ΅°μ •
- **μ‹ν€€μ¤ κΈΈμ΄**: νƒμ¤ν¬μ— λ”°λΌ μ΅°μ •

### λ¶„μ‚° ν•™μµ μ„¤μ •
- `world_size`: μ‚¬μ©ν•  GPU μ
- `grad_accumulation_steps`: ν¨κ³Όμ  λ°°μΉ ν¬κΈ° μ΅°μ •
- `deepspeed_config`: DeepSpeed μ‚¬μ© μ‹ μ„¤μ • νμΌ μ§€μ •

## π“ μ„¤μ • κ²€μ¦

### ν•„μ ν•„λ“ ν™•μΈ
λ¨λ“  training configμ—λ” λ‹¤μμ΄ ν¬ν•¨λμ–΄μ•Ό ν•¨:
- λ¨λΈ μ•„ν‚¤ν…μ² (vocab_size, d_model, n_heads, n_layers)
- ν›λ ¨ μ„¤μ • (learning_rate, batch_size, max_steps)
- λ°μ΄ν„° κ²½λ΅ (train_data_path, val_data_path, tokenizer_path)

### μ„¤μ • μ ν¨μ„± κ²€μ‚¬
```bash
# μ„¤μ • νμΌ λ¬Έλ²• κ²€μ‚¬
python -m json.tool configs/training/base_model.json

# λ¨λΈ μƒμ„± ν…μ¤νΈ
python scripts/validate_config.py --config configs/training/base_model.json
```

## π”— κ΄€λ ¨ λ¬Έμ„

- [Training Guide](../docs/training.md)
- [Model Architecture](../docs/models.md)
- [Dataset Preparation](../common/scripts/dataset/README.md)
- [Distributed Training](../docs/distributed.md) 
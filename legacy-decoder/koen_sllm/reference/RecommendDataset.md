소규모 연구소를 위한 Transformer 디코더 모델 사전학습용 한국어/영어 데이터셋 분석 및 구축 전략섹션 1: 소규모 연구소를 위한 현실적인 사전학습 기준 정립본 섹션에서는 소규모 연구소 환경에서 '현실적인 규모'의 언어 모델 사전학습 프로젝트를 정의하고, 이를 바탕으로 데이터셋 선택을 위한 구체적인 의사결정 프레임워크를 수립합니다. 계산 자원, 데이터 규모, 품질 간의 균형을 분석하여 실현 가능한 목표를 설정하는 데 중점을 둡니다.1.1. 소규모 언어 모델(SLM) 패러다임의 정의최근 인공지능 분야의 거대 언어 모델(LLM) 경쟁이 심화됨에 따라, 상대적으로 적은 자원으로 특정 목적에 최적화된 소규모 언어 모델(Small Language Model, SLM)이 주목받고 있습니다.1 SLM은 일반적으로 수백만에서 수십억 개 수준의 파라미터를 가지며, 이는 수천억 개 이상의 파라미터를 가진 LLM에 비해 훨씬 작고 효율적입니다. 이러한 특성 덕분에 SLM은 제한된 메모리와 계산 능력 환경에서도 구동이 가능하여, 소규모 연구소의 현실적인 대안으로 부상하고 있습니다.1본 보고서에서 목표로 하는 모델 아키텍처는 사용자 요구사항에 명시된 바와 같이 디코더-온리(Decoder-only) Transformer 구조입니다. 이 구조는 GPT(Generative Pre-trained Transformers) 계열 모델의 핵심이며, 주어진 텍스트를 바탕으로 다음 단어를 예측하는 방식으로 학습하여 문장 생성, 대화, 요약 등 다양한 생성 과제에 탁월한 성능을 보입니다.1 BERT와 같은 인코더-온리(Encoder-only) 모델이 문맥 이해에 중점을 두는 반면, 디코더-온리 모델은 순차적 텍스트 생성에 특화되어 있어 본 프로젝트의 목적에 부합합니다.2모델을 사전학습한 후에는 성능 최적화를 위해 가지치기(pruning), 양자화(quantization), 지식 증류(knowledge distillation)와 같은 기법을 적용할 수 있습니다.1 하지만 본 보고서의 핵심 과제는 이러한 사후 최적화가 아닌, 모델을 처음부터(from scratch) 사전학습하는 단계에 필요한 데이터셋을 조사하고 구축하는 것이므로, 데이터의 양과 질, 그리고 이를 처리하는 데 필요한 자원 분석에 집중합니다.1.2. 계산 및 데이터 예산의 정량화'현실적인 규모'라는 모호한 개념을 구체적인 수치로 정의하기 위해, 실제 SLM 개발 사례를 벤치마크로 활용합니다. 이는 데이터셋의 목표 크기와 필요한 계산 자원을 추정하는 데 중요한 기준을 제공합니다.첫 번째 주요 사례는 국내 금융 분야에 특화된 KB-BERT 모델입니다.3 이 모델은 약 1억 1천만(110M) 개의 파라미터를 가지며, 사전학습을 위해 약 40GB 규모의 특화 말뭉치를 사용했습니다. 학습에는 NVIDIA V100 32GB GPU 8개를 사용하여 총 20일이 소요되었습니다. 이를 GPU 시간으로 환산하면 약 8×24×20=3,840 GPU-hours에 해당합니다. 이 수치는 소규모 연구소에서 상당한 투자로 간주될 수 있지만, 감당 가능한 프로젝트 범위를 설정하는 데 중요한 기준점이 됩니다.두 번째 참고 사례는 Project Gutenberg 데이터셋을 이용한 GPT 모델 사전학습 예제입니다.4 이 예제에 따르면, 단일 V100 GPU 환경에서 500MB 크기의 텍스트 파일 하나를 학습하는 데 약 4시간이 소요됩니다. 이를 단순 비례로 확장하면, 50GB 규모의 데이터셋을 한 번 학습(1 epoch)하는 데에는 약 4 hours×(50,000MB/500MB)=400 GPU-hours가 필요합니다.이 두 사례를 종합하면, 소규모 연구소에서 현실적으로 도전할 수 있는 사전학습 데이터 말뭉치의 규모는 50GB에서 200GB 사이로 설정하는 것이 타당합니다. 이 규모는 유의미한 성능을 가진 모델을 개발하기에 충분하면서도, 수 페타바이트(PB)에 달하는 최신 거대 모델의 데이터 요구량과는 비교할 수 없을 정도로 작아 실현 가능성이 높습니다. 이 데이터 규모는 수천 GPU-hour 수준의 계산 예산과 직접적으로 연관됩니다.1.3. 데이터 품질과 어휘집의 중요성SLM 연구 동향에 따르면, 학습 데이터의 양이 특정 임계점(예: 1조 토큰)을 넘어서면 데이터의 품질이 양보다 모델 성능에 더 큰 영향을 미칩니다.5 이 점은 자원이 제한된 소규모 연구소에 매우 중요한 시사점을 제공합니다. 무작정 거대하고 노이즈가 많은 데이터셋을 사용하기보다, 잘 정제된 고품질의 데이터셋을 선별하여 집중하는 것이 더 효율적인 전략일 수 있습니다.또한, 어휘집(vocabulary)의 크기는 모델의 메모리 사용량에 직접적인 영향을 미치는 핵심 요소입니다.5 예를 들어, Bloom-560M 모델은 25만 개가 넘는 매우 큰 어휘집을 사용하여 비슷한 크기의 다른 모델보다 더 많은 메모리를 요구합니다.5 소규모 연구소에서 사용할 GPU의 VRAM 용량을 고려할 때, 어휘집 크기를 적절히 조절하는 것이 필수적입니다. 최근 SLM들은 주로 5만 개 이상의 어휘집 크기를 가지며, 성능과 효율성 사이의 균형을 맞추기 위해 5만에서 10만 개 사이의 어휘집 크기를 목표로 하는 것이 합리적인 접근입니다.5결론적으로, 소규모 연구소의 성공적인 SLM 사전학습은 단순히 데이터를 많이 모으는 것이 아니라, 데이터 효율성과 품질 중심의 큐레이션 전략을 통해 달성될 수 있습니다. 거대 기업의 규모를 따라가는 대신, 지능적인 데이터 선택과 조합을 통해 경쟁 우위를 확보해야 합니다. 데이터셋의 크기 선택은 예상되는 학습 비용과 직결되므로, 프로젝트 시작 전에 신중한 비용-편익 분석이 선행되어야 합니다.섹션 2: 한국어 사전학습 데이터셋 심층 분석본 섹션에서는 한국어 디코더 모델 사전학습에 사용 가능한 주요 데이터셋을 심층적으로 분석합니다. 특히 데이터의 규모와 품질, 그리고 소규모 연구소의 상업적 활용 가능성에 직접적인 영향을 미치는 라이선스와 접근성 문제를 중점적으로 다룹니다.2.1. 정부 주도 말뭉치: 높은 잠재력과 높은 리스크정부 기관이 주도하여 구축한 대규모 말뭉치는 양과 질 측면에서 매우 매력적이지만, 접근 절차의 복잡성과 라이선스의 제약이라는 명확한 한계를 동시에 가지고 있습니다.2.1.1. AI-HubAI-Hub는 과학기술정보통신부와 한국지능정보사회진흥원이 주관하는 국내 최대 규모의 인공지능 학습용 데이터 플랫폼입니다.7 이곳에서는 다양한 종류의 대규모 한국어 텍스트 데이터셋을 제공합니다.주요 데이터셋 및 규모: 대표적으로 '한국어 성능이 개선된 초거대AI 언어모델 개발 및 데이터' 프로젝트는 20억 어절에 달하는 방대한 텍스트 말뭉치를 포함하며, 이는 다운로드 파일 기준으로 약 16.07GB에 해당합니다.8 이 외에도 '국가기록물 대상 초거대 AI 학습을 위한 말뭉치'(833.17MB), '초거대 AI 헬스케어 질의응답 데이터'(5.51GB) 등 다양한 도메인의 데이터가 존재합니다.8접근 방법: 데이터 다운로드를 위해서는 AI-Hub 웹사이트에 가입 후, 데이터별로 사용 목적을 명시하여 다운로드 신청을 해야 합니다. 이 과정에서 내국인 본인 확인 절차가 요구되며, 일부 민감 데이터는 인터넷과 분리된 '안심존' 환경에서만 접근이 허용될 수 있습니다.7 또한, 일부 사용자는 전용 다운로드 프로그램의 불안정성(예: 이어받기 기능 부재)을 지적하기도 해, 대용량 데이터 확보에 어려움이 따를 수 있습니다.12라이선스: AI-Hub 데이터의 이용 정책은 소규모 연구소에 가장 큰 제약이 될 수 있습니다.11 일반적인 정책에 따르면, 제공된 데이터를 활용한 연구 및 개발은 가능하지만, 이를 통해 개발된 모델이나 서비스를 판매하는 등 상업적으로 이용하려면 데이터를 구축한 원 수행기관과 별도의 협의가 필수적입니다.11 또한, 생성된 2차 저작물에는 데이터 출처를 명시해야 하며, 제3자에게 데이터를 재배포하는 것은 금지됩니다. 국외 서버를 이용하거나 해외 기관과 협력하는 경우에도 별도 합의가 필요할 수 있어 13, 향후 연구 결과의 활용 및 확장에 상당한 법적 불확실성을 야기합니다.2.1.2. 국립국어원 모두의 말뭉치국립국어원에서 운영하는 '모두의 말뭉치'는 고품질의 정제된 한국어 자료를 제공하는 중요한 자원입니다.14주요 데이터셋 및 규모: '모두의 말뭉치: 웹' 데이터는 210만 개 이상의 웹 텍스트 예문을 포함하고 있으며, 정제된 문어체 텍스트로 구성되어 있습니다.16접근 방법: AI-Hub와 유사하게, 웹사이트 회원가입 후 필요한 말뭉치를 개별적으로 신청하고 승인을 받아야 다운로드할 수 있습니다.17 이러한 접근 제한 때문에, Korpora와 같은 파이썬 라이브러리에서도 직접 다운로드 기능은 제공하지 않고, 사용자가 수동으로 다운로드한 파일을 불러오는 기능만 지원합니다.16라이선스: 각 말뭉치의 라이선스가 개별적으로 적용되므로, 사용자는 활용하려는 모든 데이터에 대해 라이선스 조건을 일일이 확인해야 하는 행정적 부담이 있습니다.18 이는 신속한 연구 개발을 저해하는 요인이 될 수 있습니다.이처럼 정부 주도 말뭉치는 그 규모와 품질 면에서 우수하지만, 복잡한 접근 절차와 상업적 활용을 제약하는 라이선스는 민첩성과 미래 확장성이 중요한 소규모 연구소에 큰 부담으로 작용합니다. 특히 상업화 계획이 조금이라도 있는 경우, 라이선스 협의 과정에서 발생하는 시간적, 법적 비용은 예측하기 어렵습니다.2.2. 공개 및 허용적 라이선스 말뭉치: 낮은 리스크와 높은 유연성정부 주도 데이터의 대안으로, 접근이 용이하고 라이선스가 명확한 오픈소스 데이터셋을 활용하는 전략이 있습니다. 비록 규모는 작을 수 있으나, 법적 리스크 없이 자유롭게 연구하고 결과를 활용할 수 있다는 큰 장점을 가집니다.한국어 위키백과 덤프 (Korean Wikipedia Dumps):설명: 위키백과는 잘 정제되고 구조화된 고품질 텍스트의 대표적인 원천입니다. Hugging Face Hub에 공개된 lcw99/wikipedia-korean-20221001 20 및 최신 버전인 lcw99/wikipedia-korean-20240501 21는 이러한 위키백과 덤프를 사전 처리하여 사용하기 쉽게 만든 데이터셋입니다.규모: 2022년 버전은 약 60만 개의 문서를 포함하며 20, Korpora 라이브러리를 통해 접근 가능한 kowikitext 데이터셋의 경우 훈련용 데이터가 약 1.7GB에 달합니다.22 전체 규모는 정부 데이터에 비해 작지만, 텍스트의 품질이 매우 높아 SLM 학습에 효과적입니다.접근 방법: Hugging Face의 datasets 라이브러리나 Korpora 패키지를 통해 단 몇 줄의 코드로 손쉽게 다운로드하고 불러올 수 있습니다.22 이는 정부 포털의 수동 신청 절차와 비교할 때 압도적인 편의성을 제공합니다.라이선스: lcw99/wikipedia-korean-20221001 데이터셋은 상업적 이용이 자유로운 Apache 2.0 라이선스를 따릅니다.24kowikitext는 CC-BY-SA 3.0 라이선스 하에 배포되어, 동일한 라이선스를 유지하는 조건 하에 상업적 활용이 가능합니다.23 두 라이선스 모두 소규모 연구소가 법적 부담 없이 모델을 개발하고 활용하기에 매우 적합합니다.기타 Hugging Face 데이터셋:Hugging Face Hub에는 wicho/kor_sae 25나 wicho/kor_3i4k 26와 같이 특정 과제(예: 의도 분류)를 위해 구축된 소규모 한국어 데이터셋들이 다수 존재합니다. 이들은 크기가 작아(수만 건 수준) 직접적인 사전학습용으로는 부적합하지만, 대부분 CC BY-SA 4.0과 같은 허용적 라이선스를 채택하고 있어 한국어 오픈 데이터 생태계의 가능성을 보여줍니다.결론적으로, 한국어 데이터셋을 선택하는 것은 **'정부 주도 경로'**와 '오픈소스 경로' 사이의 전략적 결정입니다. 전자는 데이터의 양적 우위를 제공하지만 접근성과 라이선스에 큰 제약이 따르는 반면, 후자는 데이터 규모는 작지만 법적 안정성과 개발 유연성을 보장합니다. 미래의 상업화 가능성을 열어두고 신속한 프로토타이핑을 중시하는 소규모 연구소에게는, 법적/행정적 리스크가 큰 정부 주도 경로보다 다소 규모가 작더라도 오픈소스 경로를 택하는 것이 훨씬 더 현명하고 전략적인 선택일 수 있습니다.표 1: 한국어 사전학습 데이터셋 비교 분석데이터 소스예상 규모 (GB)내용 요약라이선스접근 방법주요 고려사항AI-Hub (초거대 AI 모델 데이터)~16 GB뉴스, 블로그, 서적 등 다양한 출처의 20억 어절 범용 텍스트상업적 이용 시 원 수행기관과 별도 협의 필요회원가입 후 신청/승인 절차 필요. 전용 다운로더 사용.장점: 국내 최대 규모, 높은 품질. 단점: 복잡한 접근 절차, 상업적 활용에 대한 심각한 법적 불확실성.국립국어원 모두의 말뭉치 (웹)미상 (수백만 문장)웹사이트에서 수집한 고품질 문어체 텍스트개별 말뭉치 라이선스 확인 필요회원가입 후 신청/승인 절차 필요. 수동 다운로드.장점: 국립 기관이 정제한 고품질 데이터. 단점: 접근 불편, 라이선스 개별 확인 필요.한국어 위키백과 (lcw99/kowikitext)~2 GB백과사전 형식의 잘 정제되고 구조화된 텍스트Apache 2.0 / CC-BY-SA 3.0 (상업적 이용 가능)Hugging Face datasets 또는 Korpora 라이브러리로 즉시 다운로드장점: 매우 높은 접근성, 명확하고 허용적인 라이선스, 고품질 텍스트. 단점: 데이터 규모가 상대적으로 작음.섹션 3: 영어 사전학습 데이터셋 심층 분석영어 텍스트 데이터는 그 양이 방대하여 부족함의 문제가 아닌, 큐레이션과 리스크 관리의 문제가 핵심입니다. 소규모 연구소는 방대한 자료 속에서 계산 예산에 부합하고, 데이터 품질이 높으며, 법적 문제가 없는 데이터셋을 지능적으로 선별하는 전략이 필요합니다.3.1. 웹 기반의 기초 데이터셋웹 크롤링 기반의 대규모 데이터셋은 범용 언어 모델 학습의 기초를 형성합니다. 이 중에서도 잘 정제된 데이터셋을 선택하는 것이 중요합니다.C4 (Colossal Cleaned Crawled Corpus):설명: Google이 T5 모델 학습을 위해 구축한 데이터셋으로, Common Crawl의 방대한 웹 크롤링 데이터를 정제한 것입니다.27 SLM 사전학습의 표준적인 베이스라인으로 널리 사용됩니다.규모: 전체 영어 버전(en)은 305GB에 달하지만 28, 소규모 프로젝트에 더 적합한 서브셋이 존재합니다. 특히 뉴스 기사와 유사한 스타일로 필터링된 realnewslike 버전은 15GB로, 고품질의 데이터를 부담 없는 크기로 사용할 수 있는 훌륭한 선택지입니다.29접근 방법: Hugging Face Hub의 allenai/c4 저장소를 통해 datasets 라이브러리, Dask, 또는 Git LFS로 쉽게 접근할 수 있습니다.29라이선스: ODC-BY 라이선스를 따르며, 이는 Common Crawl의 이용 약관을 준수하는 조건 하에 연구 및 상업적 개발에 자유롭게 사용할 수 있는 허용적인 라이선스입니다.30OpenWebText:설명: OpenAI가 GPT-2 학습에 사용한 WebText 데이터셋을 오픈소스로 재현한 프로젝트입니다. Reddit에서 높은 추천수(karma)를 받은 게시물의 링크를 기반으로 데이터를 수집하여, 일반적인 웹 크롤링 데이터보다 평균적인 품질이 높습니다.31규모: 처리된 텍스트 데이터의 총량은 약 38GB ~ 42GB이며, 다운로드 및 생성에 필요한 총 디스크 공간은 약 55GB입니다.31 이는 소규모 연구소의 목표 범위(50-200GB)에 잘 부합하는 크기입니다.접근 방법: Hugging Face Hub의 Skylion007/openwebtext 저장소를 통해 이용 가능합니다.31라이선스: 데이터셋의 패키징은 CC0 ("No Rights Reserved") 라이선스 하에 배포됩니다. 이는 저작권에 대한 제약이 거의 없는 매우 허용적인 라이선스로, 법적 리스크를 최소화하고자 하는 프로젝트에 이상적입니다.313.2. 정제된 고품질 말뭉치특정 목적을 위해 전문가 집단이 직접 큐레이션한 데이터셋은 모델에 특정 능력(예: 긴 문맥 이해, 대화체 생성)을 부여하는 데 효과적입니다.The Pile:설명: EleutherAI가 큐레이션한 825GiB 규모의 방대한 데이터셋으로, 22개의 각기 다른 고품질 데이터 소스를 조합하여 구성되었습니다.32 The Pile의 핵심 가치는 학술 논문, 소스 코드, 서적, 대화 기록 등 다양한 양식의 텍스트를 포함하여 모델의 범용 지식과 추론 능력을 극대화하는 데 있습니다.전략적 활용: 소규모 연구소는 The Pile 전체를 사용하는 것이 아니라, 이를 **고품질 데이터 소스의 '라이브러리'**로 간주하고 필요한 구성 요소만 선별하여 사용하는 '뷔페식' 접근법을 취해야 합니다.추천 구성 요소: 다음은 The Pile의 구성 요소 중 품질이 높고 법적 리스크가 낮은 것들입니다.32OpenWebText2: 67.40 GB (고품질 웹 텍스트)arXiv: 60.36 GB (학술 논문, 연구 및 기술 지식)PubMed Central: 96.93 GB (생물의학 분야 학술 자료)Wikipedia (en): 6.85 GB (백과사전적 지식)Stack Exchange: 34.57 GB (Q&A 형식의 전문 지식)Gutenberg (PG-19): 11.68 GB (저작권 만료 고서)중요 경고: The Pile의 구성 요소 중 **Books3 (108.40 GB)**는 불법 복제 도서 사이트인 Bibliotik에서 수집된 것으로, 저작권 침해 문제가 있으며 실제로 DMCA 삭제 요청의 대상이 되었습니다.32심각한 법적 분쟁을 피하기 위해 이 데이터셋은 반드시 제외해야 합니다.라이선스: The Pile 자체의 라이선스도 있지만, 실질적으로 중요한 것은 포함된 각 구성 요소의 개별 라이선스입니다. 사용자는 선택한 각 데이터셋의 라이선스 정책을 반드시 준수해야 합니다.BookCorpus:설명: 약 11,000권의 미출간 소설로 구성된 데이터셋으로, 자연스러운 서사 구조와 대화체를 학습시키는 데 매우 효과적입니다.34규모: 약 10억 단어 규모이며, 다운로드 파일 크기는 약 4.6GB ~ 4.8GB 수준입니다.35내용: 로맨스, 모험 등 16개의 다양한 장르를 포함하여 모델이 긴 호흡의 일관된 스토리를 생성하는 능력을 기르는 데 도움을 줍니다.34접근 방법: Hugging Face Hub의 Yuti/bookcorpus, rojagtap/bookcorpus 등 여러 커뮤니티 버전을 통해 접근할 수 있습니다.35라이선스: 원본 데이터셋의 저작권 상태가 다소 불분명하지만, 연구 커뮤니티에서 널리 사용되어 왔습니다. The Pile의 구성 요소인 BookCorpus2를 사용하는 것이 하나의 대안이 될 수 있습니다.32영어 데이터셋을 구성하는 작업은 부족한 자원을 찾는 것이 아니라, 넘쳐나는 선택지 속에서 최적의 조합을 찾아내는 과정입니다. The Pile과 같은 거대 말뭉치의 등장은 데이터 수집의 패러다임을 '지능적인 데이터 선별'로 바꾸었습니다. 따라서 연구소의 역할은 큐레이터가 되어, C4의 realnewslike 서브셋이나 BookCorpus와 같은 깨끗한 데이터 소스와, The Pile에서 법적 문제가 없는 고품질 구성 요소(arXiv, Wikipedia 등)를 조합하는 것입니다. 이러한 '알라카르트(a la carte)' 방식은 데이터 예산 내에서 품질과 다양성을 극대화하고 법적 리스크를 최소화하는 최적의 전략입니다.표 2: 영어 사전학습 데이터셋 비교 분석데이터 소스추천 규모 (GB)내용 요약라이선스접근 방법주요 고려사항C4 (realnewslike 서브셋)15 GBCommon Crawl에서 뉴스 기사 스타일로 정제된 고품질 웹 텍스트ODC-BY (상업적 이용 가능)Hugging Face datasets 라이브러리장점: 부담 없는 크기, 높은 품질, 명확한 라이선스. 단점: 도메인이 뉴스로 다소 한정적.OpenWebText~40 GBReddit 추천 기반으로 수집된 고품질 웹 텍스트CC0 (Public Domain에 가까움)Hugging Face datasets 라이브러리장점: 매우 허용적인 라이선스, 일반 웹 크롤링보다 높은 품질. 단점: Reddit 사용자층의 편향이 존재할 수 있음.The Pile (구성 요소 선별)50-150 GB학술, 서적, Q&A 등 다양한 도메인의 데이터 라이브러리구성 요소별 라이선스 상이 (CC, Apache 등)개별 구성 요소 소싱 또는 전체 다운로드 후 선별장점: 최고의 데이터 다양성 확보 가능. 단점: Books3 등 저작권 문제 데이터 제외 필수, 큐레이션 노력 필요.BookCorpus~5 GB미출간 소설 모음. 서사 구조 및 대화체 학습에 유용원본 라이선스 불분명 (연구용으로 널리 사용)Hugging Face datasets 라이브러리장점: 모델의 스토리텔링 능력 향상에 기여. 단점: 라이선스의 법적 명확성이 다소 낮음.섹션 4: 전략적 말뭉치 구축 및 최종 권고안본 섹션에서는 앞선 분석을 종합하여, 소규모 연구소가 즉시 활용할 수 있는 구체적인 이중 언어(한국어/영어) 말뭉치 구축 레시피를 제안하고, 최종적인 전략적 권고안을 제시합니다.4.1. 제안 이중 언어 말뭉치 레시피데이터셋 선택은 단순히 기술적 결정을 넘어, 연구소의 장기적인 목표(상업화 여부, 연구 집중 분야 등)를 반영하는 전략적 결정입니다. 아래 표는 두 가지 상이한 목표에 맞춘 구체적인 말뭉치 구축 예시입니다.표 3: 소규모 연구소를 위한 이중 언어 말뭉치 구축 레시피레시피 1: "상용화 대비 및 민첩성 중심"레시피 2: "연구 성능 극대화"총 목표 규모~75 GB~150 GB구성 요소한국어 (2-3 GB):
- lcw99/wikipedia-korean (~2GB)

영어 (~72 GB):
- C4/realnewslike (15GB)
- Skylion007/openwebtext (~40GB)
- BookCorpus (~5GB)
- The Pile/Gutenberg (PG-19) (~12GB)한국어 (~18 GB):
- lcw99/wikipedia-korean (~2GB)
- AI-Hub/초거대AI 데이터 (~16GB)

영어 (~132 GB):
- The Pile/OpenWebText2 (67GB)
- The Pile/arXiv (60GB)
- BookCorpus (~5GB)라이선스 프로파일완전 허용적:
Apache 2.0, ODC-BY, CC0, Public Domain 등. 상업적 이용 및 수정에 제약 없음.혼합형 (제한적):
허용적 라이선스와 연구용으로 제한된 AI-Hub 데이터 혼합. 결과 모델의 상업적 활용에 제약 발생.핵심 전략법적 리스크 최소화 및 미래 상업화 옵션 확보. 민첩한 연구 개발 및 자유로운 결과물 활용에 초점.데이터 규모와 다양성을 최대화하여 벤치마크 성능을 극대화. 순수 학술 연구 목적에 적합.이 레시피들은 단순한 제안이며, 연구소의 특정 목표에 따라 The Pile의 다른 구성 요소(예: Stack Exchange, PubMed Central)를 추가하거나 데이터 비율을 조정하여 맞춤형으로 구성할 수 있습니다. 중요한 것은 각 구성 요소의 라이선스를 명확히 인지하고, 그에 따른 제약을 감수할 것인지 전략적으로 판단하는 것입니다.4.2. 필수 전처리 및 어휘집 고려사항최종 말뭉치를 구성한 후에는 모델 학습의 효율성과 성능을 극대화하기 위한 몇 가지 필수적인 전처리 과정이 필요합니다.중복 제거 (Deduplication): 선택한 데이터셋 내부 및 데이터셋 간에 존재하는 중복 문서를 철저히 제거해야 합니다. 이는 한정된 데이터로 최대의 학습 효과를 얻기 위한 필수 과정입니다. OpenWebText 구축에 사용된 LSH(Local-Sensitivity Hashing) 기법은 효과적인 중복 제거의 좋은 예시입니다.31품질 필터링 (Quality Filtering): C4 데이터셋의 정제 과정에서 영감을 받아, 의미 있는 내용이 거의 없는 저품질 문서를 걸러내는 휴리스틱을 적용해야 합니다.30 예를 들어, 문장의 수가 지나치게 적거나, 특정 단어가 비정상적으로 많이 반복되거나, HTML 태그 등 비텍스트 요소가 많은 문서는 제거하는 것이 좋습니다.통합 이중 언어 어휘집 구축: 한국어와 영어 데이터가 혼합된 최종 말뭉치의 대표적인 샘플을 추출하여, 이를 기반으로 **단일 토크나이저(tokenizer)**를 학습시켜야 합니다. Google의 SentencePiece(BPE 알고리즘)가 널리 사용되는 효과적인 도구입니다. 어휘집 크기는 앞서 분석한 바와 같이, 언어적 표현력과 GPU 메모리 효율성 사이의 균형을 고려하여 5만에서 10만 개 사이로 설정하는 것을 권장합니다.54.3. 최종 권고안 및 리스크 평가주요 권고 사항: 미래의 유연성과 확장성이 핵심 자산인 소규모 연구소에는 레시피 1: "상용화 대비 및 민첩성 중심" 말뭉치 구성을 강력히 권고합니다.근거: AI-Hub와 같은 대규모 정부 데이터를 사용하여 얻을 수 있는 약간의 성능 향상은, 그로 인해 발생하는 막대한 법적/행정적 리스크와 미래의 상업화 기회 상실을 감수할 만큼 크지 않을 가능성이 높습니다. 데이터 품질이 양보다 중요하다는 최근 연구 동향을 고려할 때 5, Apache 2.0, CC0 등 허용적 라이선스를 가진 오픈소스 데이터셋의 규모와 품질은 경쟁력 있는 SLM을 훈련하기에 충분합니다.리스크 관리: 허용적인 라이선스를 사용하더라도, 프로젝트에 사용된 모든 데이터의 출처, 버전, 라이선스 정보를 체계적으로 기록하고 관리하는 것이 매우 중요합니다. 이는 연구의 재현성을 보장하고, 향후 투자 유치나 기술 이전 시 필요한 실사(due diligence)에 대비하는 핵심적인 과정입니다. The Pile이나 Korpora의 개별 구성 요소 라이선스를 반드시 확인해야 한다는 점을 다시 한번 강조합니다.18궁극적으로 소규모 연구소에 가장 귀중한 자산은 벤치마크 점수의 미미한 상승이 아니라, 법적 제약 없이 자유롭게 혁신하고 그 결과를 상업적으로 활용할 수 있는 자유입니다. 데이터셋 선택은 이러한 장기적이고 전략적인 우선순위를 반영해야 합니다.